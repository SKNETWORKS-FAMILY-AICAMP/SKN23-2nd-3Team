{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "device: mps\n",
                        "Load parquet files\n",
                        "rows: 813540 train/val/test: 574092 137615 101833\n",
                        "n_features: 14\n",
                        "tuning start\n",
                        "[lr=0.0003 wd=0 bs=256] epoch 1/30 train_loss=0.25078 val_loss=0.22558 val_pr_auc=0.91385 val_recall_at_10pct=0.11190\n",
                        "[lr=0.0003 wd=0 bs=256] epoch 2/30 train_loss=0.24858 val_loss=0.22514 val_pr_auc=0.91400 val_recall_at_10pct=0.11197\n",
                        "[lr=0.0003 wd=0 bs=256] epoch 3/30 train_loss=0.24824 val_loss=0.22539 val_pr_auc=0.91399 val_recall_at_10pct=0.11194\n",
                        "[lr=0.0003 wd=0 bs=256] epoch 4/30 train_loss=0.24810 val_loss=0.22513 val_pr_auc=0.91433 val_recall_at_10pct=0.11201\n",
                        "[lr=0.0003 wd=0 bs=256] epoch 5/30 train_loss=0.24796 val_loss=0.22620 val_pr_auc=0.91419 val_recall_at_10pct=0.11199\n",
                        "[lr=0.0003 wd=0 bs=256] epoch 6/30 train_loss=0.24782 val_loss=0.22493 val_pr_auc=0.91444 val_recall_at_10pct=0.11209\n",
                        "[lr=0.0003 wd=0 bs=256] epoch 7/30 train_loss=0.24779 val_loss=0.22452 val_pr_auc=0.91449 val_recall_at_10pct=0.11212\n",
                        "[lr=0.0003 wd=0 bs=256] epoch 8/30 train_loss=0.24774 val_loss=0.22520 val_pr_auc=0.91461 val_recall_at_10pct=0.11213\n",
                        "[lr=0.0003 wd=0 bs=256] epoch 9/30 train_loss=0.24758 val_loss=0.22468 val_pr_auc=0.91467 val_recall_at_10pct=0.11215\n",
                        "[lr=0.0003 wd=0 bs=256] epoch 10/30 train_loss=0.24760 val_loss=0.22576 val_pr_auc=0.91469 val_recall_at_10pct=0.11215\n",
                        "[lr=0.0003 wd=0 bs=256] epoch 11/30 train_loss=0.24756 val_loss=0.22511 val_pr_auc=0.91475 val_recall_at_10pct=0.11214\n",
                        "[lr=0.0003 wd=0 bs=256] epoch 12/30 train_loss=0.24744 val_loss=0.22504 val_pr_auc=0.91457 val_recall_at_10pct=0.11215\n",
                        "[lr=0.0003 wd=0 bs=256] epoch 13/30 train_loss=0.24746 val_loss=0.22463 val_pr_auc=0.91514 val_recall_at_10pct=0.11229\n",
                        "[lr=0.0003 wd=0 bs=256] epoch 14/30 train_loss=0.24745 val_loss=0.22544 val_pr_auc=0.91496 val_recall_at_10pct=0.11216\n",
                        "[lr=0.0003 wd=0 bs=256] epoch 15/30 train_loss=0.24732 val_loss=0.22492 val_pr_auc=0.91520 val_recall_at_10pct=0.11232\n",
                        "[lr=0.0003 wd=0 bs=256] epoch 16/30 train_loss=0.24743 val_loss=0.22447 val_pr_auc=0.91515 val_recall_at_10pct=0.11227\n",
                        "[lr=0.0003 wd=0 bs=256] epoch 17/30 train_loss=0.24734 val_loss=0.22518 val_pr_auc=0.91485 val_recall_at_10pct=0.11223\n",
                        "[lr=0.0003 wd=0 bs=256] epoch 18/30 train_loss=0.24726 val_loss=0.22412 val_pr_auc=0.91502 val_recall_at_10pct=0.11231\n",
                        "[lr=0.0003 wd=0 bs=256] epoch 19/30 train_loss=0.24730 val_loss=0.22531 val_pr_auc=0.91538 val_recall_at_10pct=0.11220\n",
                        "[lr=0.0003 wd=0 bs=256] epoch 20/30 train_loss=0.24731 val_loss=0.22502 val_pr_auc=0.91512 val_recall_at_10pct=0.11224\n",
                        "[lr=0.0003 wd=0 bs=256] epoch 21/30 train_loss=0.24729 val_loss=0.22509 val_pr_auc=0.91502 val_recall_at_10pct=0.11220\n",
                        "[lr=0.0003 wd=0 bs=256] epoch 22/30 train_loss=0.24731 val_loss=0.22469 val_pr_auc=0.91522 val_recall_at_10pct=0.11228\n",
                        "[lr=0.0003 wd=0 bs=256] epoch 23/30 train_loss=0.24721 val_loss=0.22514 val_pr_auc=0.91514 val_recall_at_10pct=0.11221\n",
                        "[lr=0.0003 wd=0 bs=256] epoch 24/30 train_loss=0.24723 val_loss=0.22493 val_pr_auc=0.91534 val_recall_at_10pct=0.11230\n",
                        "[lr=0.0003 wd=0 bs=256] epoch 25/30 train_loss=0.24718 val_loss=0.22449 val_pr_auc=0.91545 val_recall_at_10pct=0.11226\n",
                        "[lr=0.0003 wd=0 bs=256] epoch 26/30 train_loss=0.24723 val_loss=0.22612 val_pr_auc=0.91516 val_recall_at_10pct=0.11234\n",
                        "[lr=0.0003 wd=0 bs=256] epoch 27/30 train_loss=0.24715 val_loss=0.22519 val_pr_auc=0.91504 val_recall_at_10pct=0.11220\n",
                        "[lr=0.0003 wd=0 bs=256] epoch 28/30 train_loss=0.24714 val_loss=0.22519 val_pr_auc=0.91506 val_recall_at_10pct=0.11224\n",
                        "[lr=0.0003 wd=0 bs=256] epoch 29/30 train_loss=0.24715 val_loss=0.22497 val_pr_auc=0.91528 val_recall_at_10pct=0.11224\n",
                        "[lr=0.0003 wd=0 bs=256] epoch 30/30 train_loss=0.24711 val_loss=0.22475 val_pr_auc=0.91520 val_recall_at_10pct=0.11225\n",
                        "[lr=0.0003 wd=0 bs=512] epoch 1/30 train_loss=0.25098 val_loss=0.22525 val_pr_auc=0.91370 val_recall_at_10pct=0.11192\n",
                        "[lr=0.0003 wd=0 bs=512] epoch 2/30 train_loss=0.24873 val_loss=0.22547 val_pr_auc=0.91375 val_recall_at_10pct=0.11192\n",
                        "[lr=0.0003 wd=0 bs=512] epoch 3/30 train_loss=0.24842 val_loss=0.22511 val_pr_auc=0.91426 val_recall_at_10pct=0.11189\n",
                        "[lr=0.0003 wd=0 bs=512] epoch 4/30 train_loss=0.24824 val_loss=0.22540 val_pr_auc=0.91412 val_recall_at_10pct=0.11197\n",
                        "[lr=0.0003 wd=0 bs=512] epoch 5/30 train_loss=0.24797 val_loss=0.22529 val_pr_auc=0.91436 val_recall_at_10pct=0.11190\n",
                        "[lr=0.0003 wd=0 bs=512] epoch 6/30 train_loss=0.24786 val_loss=0.22507 val_pr_auc=0.91426 val_recall_at_10pct=0.11203\n",
                        "[lr=0.0003 wd=0 bs=512] epoch 7/30 train_loss=0.24779 val_loss=0.22454 val_pr_auc=0.91463 val_recall_at_10pct=0.11212\n",
                        "[lr=0.0003 wd=0 bs=512] epoch 8/30 train_loss=0.24777 val_loss=0.22495 val_pr_auc=0.91431 val_recall_at_10pct=0.11204\n",
                        "[lr=0.0003 wd=0 bs=512] epoch 9/30 train_loss=0.24763 val_loss=0.22496 val_pr_auc=0.91459 val_recall_at_10pct=0.11213\n",
                        "[lr=0.0003 wd=0 bs=512] epoch 10/30 train_loss=0.24768 val_loss=0.22553 val_pr_auc=0.91453 val_recall_at_10pct=0.11210\n",
                        "[lr=0.0003 wd=0 bs=512] epoch 11/30 train_loss=0.24752 val_loss=0.22484 val_pr_auc=0.91480 val_recall_at_10pct=0.11215\n",
                        "[lr=0.0003 wd=0 bs=512] epoch 12/30 train_loss=0.24744 val_loss=0.22420 val_pr_auc=0.91470 val_recall_at_10pct=0.11212\n",
                        "[lr=0.0003 wd=0 bs=512] epoch 13/30 train_loss=0.24747 val_loss=0.22455 val_pr_auc=0.91492 val_recall_at_10pct=0.11214\n",
                        "[lr=0.0003 wd=0 bs=512] epoch 14/30 train_loss=0.24747 val_loss=0.22426 val_pr_auc=0.91482 val_recall_at_10pct=0.11209\n",
                        "[lr=0.0003 wd=0 bs=512] epoch 15/30 train_loss=0.24743 val_loss=0.22470 val_pr_auc=0.91489 val_recall_at_10pct=0.11220\n",
                        "[lr=0.0003 wd=0 bs=512] epoch 16/30 train_loss=0.24743 val_loss=0.22463 val_pr_auc=0.91486 val_recall_at_10pct=0.11214\n",
                        "[lr=0.0003 wd=0 bs=512] epoch 17/30 train_loss=0.24728 val_loss=0.22559 val_pr_auc=0.91467 val_recall_at_10pct=0.11215\n",
                        "[lr=0.0003 wd=0 bs=512] epoch 18/30 train_loss=0.24729 val_loss=0.22460 val_pr_auc=0.91479 val_recall_at_10pct=0.11228\n",
                        "[lr=0.0003 wd=0 bs=512] epoch 19/30 train_loss=0.24731 val_loss=0.22469 val_pr_auc=0.91495 val_recall_at_10pct=0.11226\n",
                        "[lr=0.0003 wd=0 bs=512] epoch 20/30 train_loss=0.24732 val_loss=0.22396 val_pr_auc=0.91480 val_recall_at_10pct=0.11210\n",
                        "[lr=0.0003 wd=0 bs=512] epoch 21/30 train_loss=0.24725 val_loss=0.22458 val_pr_auc=0.91521 val_recall_at_10pct=0.11226\n",
                        "[lr=0.0003 wd=0 bs=512] epoch 22/30 train_loss=0.24723 val_loss=0.22516 val_pr_auc=0.91506 val_recall_at_10pct=0.11225\n",
                        "[lr=0.0003 wd=0 bs=512] epoch 23/30 train_loss=0.24733 val_loss=0.22464 val_pr_auc=0.91502 val_recall_at_10pct=0.11221\n",
                        "[lr=0.0003 wd=0 bs=512] epoch 24/30 train_loss=0.24722 val_loss=0.22492 val_pr_auc=0.91511 val_recall_at_10pct=0.11226\n",
                        "[lr=0.0003 wd=0 bs=512] epoch 25/30 train_loss=0.24721 val_loss=0.22465 val_pr_auc=0.91518 val_recall_at_10pct=0.11236\n",
                        "[lr=0.0003 wd=0 bs=512] epoch 26/30 train_loss=0.24722 val_loss=0.22494 val_pr_auc=0.91501 val_recall_at_10pct=0.11220\n",
                        "[lr=0.0003 wd=0 bs=512] epoch 27/30 train_loss=0.24720 val_loss=0.22456 val_pr_auc=0.91508 val_recall_at_10pct=0.11217\n",
                        "[lr=0.0003 wd=0 bs=512] epoch 28/30 train_loss=0.24713 val_loss=0.22442 val_pr_auc=0.91529 val_recall_at_10pct=0.11233\n",
                        "[lr=0.0003 wd=0 bs=512] epoch 29/30 train_loss=0.24714 val_loss=0.22457 val_pr_auc=0.91502 val_recall_at_10pct=0.11220\n",
                        "[lr=0.0003 wd=0 bs=512] epoch 30/30 train_loss=0.24720 val_loss=0.22482 val_pr_auc=0.91531 val_recall_at_10pct=0.11230\n",
                        "[lr=0.0003 wd=1e-05 bs=256] epoch 1/30 train_loss=0.25052 val_loss=0.22496 val_pr_auc=0.91341 val_recall_at_10pct=0.11189\n",
                        "[lr=0.0003 wd=1e-05 bs=256] epoch 2/30 train_loss=0.24869 val_loss=0.22528 val_pr_auc=0.91386 val_recall_at_10pct=0.11195\n",
                        "[lr=0.0003 wd=1e-05 bs=256] epoch 3/30 train_loss=0.24834 val_loss=0.22477 val_pr_auc=0.91402 val_recall_at_10pct=0.11192\n",
                        "[lr=0.0003 wd=1e-05 bs=256] epoch 4/30 train_loss=0.24817 val_loss=0.22472 val_pr_auc=0.91418 val_recall_at_10pct=0.11198\n",
                        "[lr=0.0003 wd=1e-05 bs=256] epoch 5/30 train_loss=0.24798 val_loss=0.22469 val_pr_auc=0.91447 val_recall_at_10pct=0.11209\n",
                        "[lr=0.0003 wd=1e-05 bs=256] epoch 6/30 train_loss=0.24784 val_loss=0.22480 val_pr_auc=0.91429 val_recall_at_10pct=0.11207\n",
                        "[lr=0.0003 wd=1e-05 bs=256] epoch 7/30 train_loss=0.24775 val_loss=0.22488 val_pr_auc=0.91443 val_recall_at_10pct=0.11197\n",
                        "[lr=0.0003 wd=1e-05 bs=256] epoch 8/30 train_loss=0.24770 val_loss=0.22457 val_pr_auc=0.91450 val_recall_at_10pct=0.11213\n",
                        "[lr=0.0003 wd=1e-05 bs=256] epoch 9/30 train_loss=0.24764 val_loss=0.22451 val_pr_auc=0.91463 val_recall_at_10pct=0.11211\n",
                        "[lr=0.0003 wd=1e-05 bs=256] epoch 10/30 train_loss=0.24753 val_loss=0.22501 val_pr_auc=0.91454 val_recall_at_10pct=0.11217\n",
                        "[lr=0.0003 wd=1e-05 bs=256] epoch 11/30 train_loss=0.24756 val_loss=0.22553 val_pr_auc=0.91451 val_recall_at_10pct=0.11214\n",
                        "[lr=0.0003 wd=1e-05 bs=256] epoch 12/30 train_loss=0.24758 val_loss=0.22454 val_pr_auc=0.91452 val_recall_at_10pct=0.11216\n",
                        "[lr=0.0003 wd=1e-05 bs=256] epoch 13/30 train_loss=0.24749 val_loss=0.22477 val_pr_auc=0.91481 val_recall_at_10pct=0.11215\n",
                        "[lr=0.0003 wd=1e-05 bs=256] epoch 14/30 train_loss=0.24744 val_loss=0.22431 val_pr_auc=0.91474 val_recall_at_10pct=0.11220\n",
                        "[lr=0.0003 wd=1e-05 bs=256] epoch 15/30 train_loss=0.24746 val_loss=0.22493 val_pr_auc=0.91484 val_recall_at_10pct=0.11210\n",
                        "[lr=0.0003 wd=1e-05 bs=256] epoch 16/30 train_loss=0.24740 val_loss=0.22435 val_pr_auc=0.91511 val_recall_at_10pct=0.11230\n",
                        "[lr=0.0003 wd=1e-05 bs=256] epoch 17/30 train_loss=0.24731 val_loss=0.22525 val_pr_auc=0.91470 val_recall_at_10pct=0.11222\n",
                        "[lr=0.0003 wd=1e-05 bs=256] epoch 18/30 train_loss=0.24745 val_loss=0.22496 val_pr_auc=0.91468 val_recall_at_10pct=0.11215\n",
                        "[lr=0.0003 wd=1e-05 bs=256] epoch 19/30 train_loss=0.24733 val_loss=0.22457 val_pr_auc=0.91507 val_recall_at_10pct=0.11233\n",
                        "[lr=0.0003 wd=1e-05 bs=256] epoch 20/30 train_loss=0.24736 val_loss=0.22469 val_pr_auc=0.91500 val_recall_at_10pct=0.11226\n",
                        "[lr=0.0003 wd=1e-05 bs=256] epoch 21/30 train_loss=0.24730 val_loss=0.22429 val_pr_auc=0.91504 val_recall_at_10pct=0.11227\n",
                        "[lr=0.0003 wd=1e-05 bs=256] epoch 22/30 train_loss=0.24732 val_loss=0.22481 val_pr_auc=0.91497 val_recall_at_10pct=0.11224\n",
                        "[lr=0.0003 wd=1e-05 bs=256] epoch 23/30 train_loss=0.24730 val_loss=0.22476 val_pr_auc=0.91507 val_recall_at_10pct=0.11227\n",
                        "[lr=0.0003 wd=1e-05 bs=256] epoch 24/30 train_loss=0.24721 val_loss=0.22521 val_pr_auc=0.91518 val_recall_at_10pct=0.11234\n",
                        "[lr=0.0003 wd=1e-05 bs=256] epoch 25/30 train_loss=0.24725 val_loss=0.22558 val_pr_auc=0.91523 val_recall_at_10pct=0.11233\n",
                        "[lr=0.0003 wd=1e-05 bs=256] epoch 26/30 train_loss=0.24727 val_loss=0.22576 val_pr_auc=0.91512 val_recall_at_10pct=0.11227\n",
                        "[lr=0.0003 wd=1e-05 bs=256] epoch 27/30 train_loss=0.24725 val_loss=0.22415 val_pr_auc=0.91541 val_recall_at_10pct=0.11235\n",
                        "[lr=0.0003 wd=1e-05 bs=256] epoch 28/30 train_loss=0.24722 val_loss=0.22540 val_pr_auc=0.91491 val_recall_at_10pct=0.11228\n",
                        "[lr=0.0003 wd=1e-05 bs=256] epoch 29/30 train_loss=0.24723 val_loss=0.22641 val_pr_auc=0.91531 val_recall_at_10pct=0.11221\n",
                        "[lr=0.0003 wd=1e-05 bs=256] epoch 30/30 train_loss=0.24721 val_loss=0.22494 val_pr_auc=0.91506 val_recall_at_10pct=0.11231\n",
                        "[lr=0.0003 wd=1e-05 bs=512] epoch 1/30 train_loss=0.25127 val_loss=0.22558 val_pr_auc=0.91329 val_recall_at_10pct=0.11191\n",
                        "[lr=0.0003 wd=1e-05 bs=512] epoch 2/30 train_loss=0.24880 val_loss=0.22550 val_pr_auc=0.91373 val_recall_at_10pct=0.11189\n",
                        "[lr=0.0003 wd=1e-05 bs=512] epoch 3/30 train_loss=0.24842 val_loss=0.22528 val_pr_auc=0.91368 val_recall_at_10pct=0.11192\n",
                        "[lr=0.0003 wd=1e-05 bs=512] epoch 4/30 train_loss=0.24816 val_loss=0.22453 val_pr_auc=0.91400 val_recall_at_10pct=0.11190\n",
                        "[lr=0.0003 wd=1e-05 bs=512] epoch 5/30 train_loss=0.24795 val_loss=0.22486 val_pr_auc=0.91422 val_recall_at_10pct=0.11200\n",
                        "[lr=0.0003 wd=1e-05 bs=512] epoch 6/30 train_loss=0.24795 val_loss=0.22514 val_pr_auc=0.91440 val_recall_at_10pct=0.11201\n",
                        "[lr=0.0003 wd=1e-05 bs=512] epoch 7/30 train_loss=0.24775 val_loss=0.22504 val_pr_auc=0.91439 val_recall_at_10pct=0.11207\n",
                        "[lr=0.0003 wd=1e-05 bs=512] epoch 8/30 train_loss=0.24768 val_loss=0.22469 val_pr_auc=0.91428 val_recall_at_10pct=0.11216\n",
                        "[lr=0.0003 wd=1e-05 bs=512] epoch 9/30 train_loss=0.24771 val_loss=0.22562 val_pr_auc=0.91449 val_recall_at_10pct=0.11210\n",
                        "[lr=0.0003 wd=1e-05 bs=512] epoch 10/30 train_loss=0.24763 val_loss=0.22451 val_pr_auc=0.91445 val_recall_at_10pct=0.11210\n",
                        "[lr=0.0003 wd=1e-05 bs=512] epoch 11/30 train_loss=0.24758 val_loss=0.22605 val_pr_auc=0.91465 val_recall_at_10pct=0.11219\n",
                        "[lr=0.0003 wd=1e-05 bs=512] epoch 12/30 train_loss=0.24752 val_loss=0.22481 val_pr_auc=0.91453 val_recall_at_10pct=0.11211\n",
                        "[lr=0.0003 wd=1e-05 bs=512] epoch 13/30 train_loss=0.24749 val_loss=0.22456 val_pr_auc=0.91456 val_recall_at_10pct=0.11215\n",
                        "[lr=0.0003 wd=1e-05 bs=512] epoch 14/30 train_loss=0.24744 val_loss=0.22510 val_pr_auc=0.91449 val_recall_at_10pct=0.11221\n",
                        "[lr=0.0003 wd=1e-05 bs=512] epoch 15/30 train_loss=0.24742 val_loss=0.22463 val_pr_auc=0.91476 val_recall_at_10pct=0.11213\n",
                        "[lr=0.0003 wd=1e-05 bs=512] epoch 16/30 train_loss=0.24737 val_loss=0.22496 val_pr_auc=0.91477 val_recall_at_10pct=0.11224\n",
                        "[lr=0.0003 wd=1e-05 bs=512] epoch 17/30 train_loss=0.24738 val_loss=0.22478 val_pr_auc=0.91472 val_recall_at_10pct=0.11224\n",
                        "[lr=0.0003 wd=1e-05 bs=512] epoch 18/30 train_loss=0.24731 val_loss=0.22441 val_pr_auc=0.91502 val_recall_at_10pct=0.11218\n",
                        "[lr=0.0003 wd=1e-05 bs=512] epoch 19/30 train_loss=0.24741 val_loss=0.22537 val_pr_auc=0.91497 val_recall_at_10pct=0.11220\n",
                        "[lr=0.0003 wd=1e-05 bs=512] epoch 20/30 train_loss=0.24724 val_loss=0.22473 val_pr_auc=0.91490 val_recall_at_10pct=0.11225\n",
                        "[lr=0.0003 wd=1e-05 bs=512] epoch 21/30 train_loss=0.24731 val_loss=0.22425 val_pr_auc=0.91501 val_recall_at_10pct=0.11220\n",
                        "[lr=0.0003 wd=1e-05 bs=512] epoch 22/30 train_loss=0.24727 val_loss=0.22483 val_pr_auc=0.91490 val_recall_at_10pct=0.11226\n",
                        "[lr=0.0003 wd=1e-05 bs=512] epoch 23/30 train_loss=0.24724 val_loss=0.22477 val_pr_auc=0.91512 val_recall_at_10pct=0.11235\n",
                        "[lr=0.0003 wd=1e-05 bs=512] epoch 24/30 train_loss=0.24727 val_loss=0.22442 val_pr_auc=0.91492 val_recall_at_10pct=0.11224\n",
                        "[lr=0.0003 wd=1e-05 bs=512] epoch 25/30 train_loss=0.24717 val_loss=0.22452 val_pr_auc=0.91524 val_recall_at_10pct=0.11228\n",
                        "[lr=0.0003 wd=1e-05 bs=512] epoch 26/30 train_loss=0.24728 val_loss=0.22499 val_pr_auc=0.91504 val_recall_at_10pct=0.11222\n",
                        "[lr=0.0003 wd=1e-05 bs=512] epoch 27/30 train_loss=0.24716 val_loss=0.22493 val_pr_auc=0.91508 val_recall_at_10pct=0.11222\n",
                        "[lr=0.0003 wd=1e-05 bs=512] epoch 28/30 train_loss=0.24715 val_loss=0.22450 val_pr_auc=0.91520 val_recall_at_10pct=0.11233\n",
                        "[lr=0.0003 wd=1e-05 bs=512] epoch 29/30 train_loss=0.24707 val_loss=0.22468 val_pr_auc=0.91515 val_recall_at_10pct=0.11229\n",
                        "[lr=0.0003 wd=1e-05 bs=512] epoch 30/30 train_loss=0.24709 val_loss=0.22442 val_pr_auc=0.91536 val_recall_at_10pct=0.11226\n",
                        "[lr=0.0003 wd=0.0001 bs=256] epoch 1/30 train_loss=0.25047 val_loss=0.22595 val_pr_auc=0.91360 val_recall_at_10pct=0.11188\n",
                        "[lr=0.0003 wd=0.0001 bs=256] epoch 2/30 train_loss=0.24860 val_loss=0.22573 val_pr_auc=0.91412 val_recall_at_10pct=0.11194\n",
                        "[lr=0.0003 wd=0.0001 bs=256] epoch 3/30 train_loss=0.24827 val_loss=0.22582 val_pr_auc=0.91378 val_recall_at_10pct=0.11192\n",
                        "[lr=0.0003 wd=0.0001 bs=256] epoch 4/30 train_loss=0.24806 val_loss=0.22525 val_pr_auc=0.91388 val_recall_at_10pct=0.11200\n",
                        "[lr=0.0003 wd=0.0001 bs=256] epoch 5/30 train_loss=0.24795 val_loss=0.22475 val_pr_auc=0.91456 val_recall_at_10pct=0.11216\n",
                        "[lr=0.0003 wd=0.0001 bs=256] epoch 6/30 train_loss=0.24775 val_loss=0.22498 val_pr_auc=0.91405 val_recall_at_10pct=0.11207\n",
                        "[lr=0.0003 wd=0.0001 bs=256] epoch 7/30 train_loss=0.24777 val_loss=0.22472 val_pr_auc=0.91464 val_recall_at_10pct=0.11215\n",
                        "[lr=0.0003 wd=0.0001 bs=256] epoch 8/30 train_loss=0.24769 val_loss=0.22491 val_pr_auc=0.91457 val_recall_at_10pct=0.11207\n",
                        "[lr=0.0003 wd=0.0001 bs=256] epoch 9/30 train_loss=0.24760 val_loss=0.22522 val_pr_auc=0.91443 val_recall_at_10pct=0.11211\n",
                        "[lr=0.0003 wd=0.0001 bs=256] epoch 10/30 train_loss=0.24763 val_loss=0.22523 val_pr_auc=0.91433 val_recall_at_10pct=0.11218\n",
                        "[lr=0.0003 wd=0.0001 bs=256] epoch 11/30 train_loss=0.24761 val_loss=0.22475 val_pr_auc=0.91487 val_recall_at_10pct=0.11220\n",
                        "[lr=0.0003 wd=0.0001 bs=256] epoch 12/30 train_loss=0.24762 val_loss=0.22511 val_pr_auc=0.91478 val_recall_at_10pct=0.11222\n",
                        "[lr=0.0003 wd=0.0001 bs=256] epoch 13/30 train_loss=0.24753 val_loss=0.22580 val_pr_auc=0.91491 val_recall_at_10pct=0.11217\n",
                        "[lr=0.0003 wd=0.0001 bs=256] epoch 14/30 train_loss=0.24750 val_loss=0.22523 val_pr_auc=0.91497 val_recall_at_10pct=0.11206\n",
                        "[lr=0.0003 wd=0.0001 bs=256] epoch 15/30 train_loss=0.24745 val_loss=0.22538 val_pr_auc=0.91451 val_recall_at_10pct=0.11225\n",
                        "[lr=0.0003 wd=0.0001 bs=256] epoch 16/30 train_loss=0.24745 val_loss=0.22538 val_pr_auc=0.91485 val_recall_at_10pct=0.11222\n",
                        "[lr=0.0003 wd=0.0001 bs=256] epoch 17/30 train_loss=0.24744 val_loss=0.22499 val_pr_auc=0.91521 val_recall_at_10pct=0.11225\n",
                        "[lr=0.0003 wd=0.0001 bs=256] epoch 18/30 train_loss=0.24753 val_loss=0.22530 val_pr_auc=0.91503 val_recall_at_10pct=0.11232\n",
                        "[lr=0.0003 wd=0.0001 bs=256] epoch 19/30 train_loss=0.24748 val_loss=0.22491 val_pr_auc=0.91493 val_recall_at_10pct=0.11227\n",
                        "[lr=0.0003 wd=0.0001 bs=256] epoch 20/30 train_loss=0.24743 val_loss=0.22443 val_pr_auc=0.91538 val_recall_at_10pct=0.11227\n",
                        "[lr=0.0003 wd=0.0001 bs=256] epoch 21/30 train_loss=0.24740 val_loss=0.22521 val_pr_auc=0.91500 val_recall_at_10pct=0.11214\n",
                        "[lr=0.0003 wd=0.0001 bs=256] epoch 22/30 train_loss=0.24741 val_loss=0.22502 val_pr_auc=0.91496 val_recall_at_10pct=0.11213\n",
                        "[lr=0.0003 wd=0.0001 bs=256] epoch 23/30 train_loss=0.24747 val_loss=0.22551 val_pr_auc=0.91472 val_recall_at_10pct=0.11212\n",
                        "[lr=0.0003 wd=0.0001 bs=256] epoch 24/30 train_loss=0.24745 val_loss=0.22531 val_pr_auc=0.91501 val_recall_at_10pct=0.11228\n",
                        "[lr=0.0003 wd=0.0001 bs=256] epoch 25/30 train_loss=0.24744 val_loss=0.22492 val_pr_auc=0.91520 val_recall_at_10pct=0.11238\n",
                        "[lr=0.0003 wd=0.0001 bs=256] epoch 26/30 train_loss=0.24741 val_loss=0.22533 val_pr_auc=0.91494 val_recall_at_10pct=0.11219\n",
                        "[lr=0.0003 wd=0.0001 bs=256] epoch 27/30 train_loss=0.24742 val_loss=0.22626 val_pr_auc=0.91483 val_recall_at_10pct=0.11226\n",
                        "[lr=0.0003 wd=0.0001 bs=256] epoch 28/30 train_loss=0.24740 val_loss=0.22507 val_pr_auc=0.91477 val_recall_at_10pct=0.11222\n",
                        "[lr=0.0003 wd=0.0001 bs=256] epoch 29/30 train_loss=0.24734 val_loss=0.22536 val_pr_auc=0.91479 val_recall_at_10pct=0.11214\n",
                        "[lr=0.0003 wd=0.0001 bs=256] epoch 30/30 train_loss=0.24737 val_loss=0.22481 val_pr_auc=0.91504 val_recall_at_10pct=0.11208\n",
                        "[lr=0.0003 wd=0.0001 bs=512] epoch 1/30 train_loss=0.25076 val_loss=0.22492 val_pr_auc=0.91321 val_recall_at_10pct=0.11189\n",
                        "[lr=0.0003 wd=0.0001 bs=512] epoch 2/30 train_loss=0.24868 val_loss=0.22546 val_pr_auc=0.91379 val_recall_at_10pct=0.11191\n",
                        "[lr=0.0003 wd=0.0001 bs=512] epoch 3/30 train_loss=0.24831 val_loss=0.22551 val_pr_auc=0.91402 val_recall_at_10pct=0.11198\n",
                        "[lr=0.0003 wd=0.0001 bs=512] epoch 4/30 train_loss=0.24814 val_loss=0.22511 val_pr_auc=0.91388 val_recall_at_10pct=0.11193\n",
                        "[lr=0.0003 wd=0.0001 bs=512] epoch 5/30 train_loss=0.24796 val_loss=0.22497 val_pr_auc=0.91388 val_recall_at_10pct=0.11200\n",
                        "[lr=0.0003 wd=0.0001 bs=512] epoch 6/30 train_loss=0.24785 val_loss=0.22469 val_pr_auc=0.91399 val_recall_at_10pct=0.11201\n",
                        "[lr=0.0003 wd=0.0001 bs=512] epoch 7/30 train_loss=0.24782 val_loss=0.22446 val_pr_auc=0.91411 val_recall_at_10pct=0.11209\n",
                        "[lr=0.0003 wd=0.0001 bs=512] epoch 8/30 train_loss=0.24779 val_loss=0.22506 val_pr_auc=0.91458 val_recall_at_10pct=0.11206\n",
                        "[lr=0.0003 wd=0.0001 bs=512] epoch 9/30 train_loss=0.24771 val_loss=0.22505 val_pr_auc=0.91435 val_recall_at_10pct=0.11213\n",
                        "[lr=0.0003 wd=0.0001 bs=512] epoch 10/30 train_loss=0.24755 val_loss=0.22494 val_pr_auc=0.91439 val_recall_at_10pct=0.11209\n",
                        "[lr=0.0003 wd=0.0001 bs=512] epoch 11/30 train_loss=0.24754 val_loss=0.22483 val_pr_auc=0.91469 val_recall_at_10pct=0.11220\n",
                        "[lr=0.0003 wd=0.0001 bs=512] epoch 12/30 train_loss=0.24746 val_loss=0.22522 val_pr_auc=0.91479 val_recall_at_10pct=0.11220\n",
                        "[lr=0.0003 wd=0.0001 bs=512] epoch 13/30 train_loss=0.24739 val_loss=0.22498 val_pr_auc=0.91462 val_recall_at_10pct=0.11214\n",
                        "[lr=0.0003 wd=0.0001 bs=512] epoch 14/30 train_loss=0.24744 val_loss=0.22480 val_pr_auc=0.91468 val_recall_at_10pct=0.11217\n",
                        "[lr=0.0003 wd=0.0001 bs=512] epoch 15/30 train_loss=0.24735 val_loss=0.22490 val_pr_auc=0.91487 val_recall_at_10pct=0.11213\n",
                        "[lr=0.0003 wd=0.0001 bs=512] epoch 16/30 train_loss=0.24742 val_loss=0.22487 val_pr_auc=0.91497 val_recall_at_10pct=0.11234\n",
                        "[lr=0.0003 wd=0.0001 bs=512] epoch 17/30 train_loss=0.24734 val_loss=0.22464 val_pr_auc=0.91501 val_recall_at_10pct=0.11227\n",
                        "[lr=0.0003 wd=0.0001 bs=512] epoch 18/30 train_loss=0.24736 val_loss=0.22477 val_pr_auc=0.91491 val_recall_at_10pct=0.11233\n",
                        "[lr=0.0003 wd=0.0001 bs=512] epoch 19/30 train_loss=0.24727 val_loss=0.22478 val_pr_auc=0.91522 val_recall_at_10pct=0.11237\n",
                        "[lr=0.0003 wd=0.0001 bs=512] epoch 20/30 train_loss=0.24733 val_loss=0.22503 val_pr_auc=0.91496 val_recall_at_10pct=0.11226\n",
                        "[lr=0.0003 wd=0.0001 bs=512] epoch 21/30 train_loss=0.24737 val_loss=0.22495 val_pr_auc=0.91508 val_recall_at_10pct=0.11231\n",
                        "[lr=0.0003 wd=0.0001 bs=512] epoch 22/30 train_loss=0.24729 val_loss=0.22542 val_pr_auc=0.91515 val_recall_at_10pct=0.11235\n",
                        "[lr=0.0003 wd=0.0001 bs=512] epoch 23/30 train_loss=0.24728 val_loss=0.22506 val_pr_auc=0.91518 val_recall_at_10pct=0.11237\n",
                        "[lr=0.0003 wd=0.0001 bs=512] epoch 24/30 train_loss=0.24723 val_loss=0.22454 val_pr_auc=0.91479 val_recall_at_10pct=0.11227\n",
                        "[lr=0.0003 wd=0.0001 bs=512] epoch 25/30 train_loss=0.24734 val_loss=0.22468 val_pr_auc=0.91519 val_recall_at_10pct=0.11225\n",
                        "[lr=0.0003 wd=0.0001 bs=512] epoch 26/30 train_loss=0.24729 val_loss=0.22447 val_pr_auc=0.91519 val_recall_at_10pct=0.11222\n",
                        "[lr=0.0003 wd=0.0001 bs=512] epoch 27/30 train_loss=0.24729 val_loss=0.22557 val_pr_auc=0.91509 val_recall_at_10pct=0.11221\n",
                        "[lr=0.0003 wd=0.0001 bs=512] epoch 28/30 train_loss=0.24728 val_loss=0.22427 val_pr_auc=0.91521 val_recall_at_10pct=0.11239\n",
                        "[lr=0.0003 wd=0.0001 bs=512] epoch 29/30 train_loss=0.24719 val_loss=0.22812 val_pr_auc=0.91490 val_recall_at_10pct=0.11224\n",
                        "[lr=0.0003 wd=0.0001 bs=512] epoch 30/30 train_loss=0.24727 val_loss=0.22505 val_pr_auc=0.91506 val_recall_at_10pct=0.11220\n",
                        "[lr=0.001 wd=0 bs=256] epoch 1/30 train_loss=0.24936 val_loss=0.22562 val_pr_auc=0.91307 val_recall_at_10pct=0.11202\n",
                        "[lr=0.001 wd=0 bs=256] epoch 2/30 train_loss=0.24828 val_loss=0.22650 val_pr_auc=0.91373 val_recall_at_10pct=0.11199\n",
                        "[lr=0.001 wd=0 bs=256] epoch 3/30 train_loss=0.24801 val_loss=0.22587 val_pr_auc=0.91349 val_recall_at_10pct=0.11208\n",
                        "[lr=0.001 wd=0 bs=256] epoch 4/30 train_loss=0.24788 val_loss=0.22577 val_pr_auc=0.91434 val_recall_at_10pct=0.11214\n",
                        "[lr=0.001 wd=0 bs=256] epoch 5/30 train_loss=0.24780 val_loss=0.22473 val_pr_auc=0.91454 val_recall_at_10pct=0.11220\n",
                        "[lr=0.001 wd=0 bs=256] epoch 6/30 train_loss=0.24775 val_loss=0.22499 val_pr_auc=0.91463 val_recall_at_10pct=0.11219\n",
                        "[lr=0.001 wd=0 bs=256] epoch 7/30 train_loss=0.24764 val_loss=0.22542 val_pr_auc=0.91443 val_recall_at_10pct=0.11213\n",
                        "[lr=0.001 wd=0 bs=256] epoch 8/30 train_loss=0.24755 val_loss=0.22472 val_pr_auc=0.91483 val_recall_at_10pct=0.11234\n",
                        "[lr=0.001 wd=0 bs=256] epoch 9/30 train_loss=0.24753 val_loss=0.22431 val_pr_auc=0.91458 val_recall_at_10pct=0.11214\n",
                        "[lr=0.001 wd=0 bs=256] epoch 10/30 train_loss=0.24748 val_loss=0.22466 val_pr_auc=0.91484 val_recall_at_10pct=0.11228\n",
                        "[lr=0.001 wd=0 bs=256] epoch 11/30 train_loss=0.24750 val_loss=0.22449 val_pr_auc=0.91521 val_recall_at_10pct=0.11228\n",
                        "[lr=0.001 wd=0 bs=256] epoch 12/30 train_loss=0.24742 val_loss=0.22536 val_pr_auc=0.91493 val_recall_at_10pct=0.11217\n",
                        "[lr=0.001 wd=0 bs=256] epoch 13/30 train_loss=0.24745 val_loss=0.22498 val_pr_auc=0.91490 val_recall_at_10pct=0.11222\n",
                        "[lr=0.001 wd=0 bs=256] epoch 14/30 train_loss=0.24735 val_loss=0.22522 val_pr_auc=0.91491 val_recall_at_10pct=0.11235\n",
                        "[lr=0.001 wd=0 bs=256] epoch 15/30 train_loss=0.24732 val_loss=0.22638 val_pr_auc=0.91521 val_recall_at_10pct=0.11223\n",
                        "[lr=0.001 wd=0 bs=256] epoch 16/30 train_loss=0.24735 val_loss=0.22490 val_pr_auc=0.91525 val_recall_at_10pct=0.11237\n",
                        "[lr=0.001 wd=0 bs=256] epoch 17/30 train_loss=0.24722 val_loss=0.22458 val_pr_auc=0.91523 val_recall_at_10pct=0.11239\n",
                        "[lr=0.001 wd=0 bs=256] epoch 18/30 train_loss=0.24732 val_loss=0.22423 val_pr_auc=0.91496 val_recall_at_10pct=0.11226\n",
                        "[lr=0.001 wd=0 bs=256] epoch 19/30 train_loss=0.24725 val_loss=0.22432 val_pr_auc=0.91529 val_recall_at_10pct=0.11230\n",
                        "[lr=0.001 wd=0 bs=256] epoch 20/30 train_loss=0.24725 val_loss=0.22495 val_pr_auc=0.91518 val_recall_at_10pct=0.11221\n",
                        "[lr=0.001 wd=0 bs=256] epoch 21/30 train_loss=0.24720 val_loss=0.22465 val_pr_auc=0.91500 val_recall_at_10pct=0.11237\n",
                        "[lr=0.001 wd=0 bs=256] epoch 22/30 train_loss=0.24725 val_loss=0.22621 val_pr_auc=0.91539 val_recall_at_10pct=0.11228\n",
                        "[lr=0.001 wd=0 bs=256] epoch 23/30 train_loss=0.24718 val_loss=0.22399 val_pr_auc=0.91504 val_recall_at_10pct=0.11224\n",
                        "[lr=0.001 wd=0 bs=256] epoch 24/30 train_loss=0.24728 val_loss=0.22499 val_pr_auc=0.91459 val_recall_at_10pct=0.11222\n",
                        "[lr=0.001 wd=0 bs=256] epoch 25/30 train_loss=0.24715 val_loss=0.22458 val_pr_auc=0.91529 val_recall_at_10pct=0.11231\n",
                        "[lr=0.001 wd=0 bs=256] epoch 26/30 train_loss=0.24716 val_loss=0.22488 val_pr_auc=0.91530 val_recall_at_10pct=0.11230\n",
                        "[lr=0.001 wd=0 bs=256] epoch 27/30 train_loss=0.24713 val_loss=0.22473 val_pr_auc=0.91490 val_recall_at_10pct=0.11227\n",
                        "[lr=0.001 wd=0 bs=256] epoch 28/30 train_loss=0.24715 val_loss=0.22483 val_pr_auc=0.91502 val_recall_at_10pct=0.11238\n",
                        "[lr=0.001 wd=0 bs=256] epoch 29/30 train_loss=0.24712 val_loss=0.22426 val_pr_auc=0.91541 val_recall_at_10pct=0.11241\n",
                        "[lr=0.001 wd=0 bs=256] epoch 30/30 train_loss=0.24711 val_loss=0.22445 val_pr_auc=0.91565 val_recall_at_10pct=0.11232\n",
                        "[lr=0.001 wd=0 bs=512] epoch 1/30 train_loss=0.24962 val_loss=0.22592 val_pr_auc=0.91360 val_recall_at_10pct=0.11192\n",
                        "[lr=0.001 wd=0 bs=512] epoch 2/30 train_loss=0.24823 val_loss=0.22722 val_pr_auc=0.91408 val_recall_at_10pct=0.11201\n",
                        "[lr=0.001 wd=0 bs=512] epoch 3/30 train_loss=0.24805 val_loss=0.22461 val_pr_auc=0.91436 val_recall_at_10pct=0.11196\n",
                        "[lr=0.001 wd=0 bs=512] epoch 4/30 train_loss=0.24784 val_loss=0.22519 val_pr_auc=0.91425 val_recall_at_10pct=0.11208\n",
                        "[lr=0.001 wd=0 bs=512] epoch 5/30 train_loss=0.24761 val_loss=0.22464 val_pr_auc=0.91442 val_recall_at_10pct=0.11210\n",
                        "[lr=0.001 wd=0 bs=512] epoch 6/30 train_loss=0.24766 val_loss=0.22436 val_pr_auc=0.91447 val_recall_at_10pct=0.11208\n",
                        "[lr=0.001 wd=0 bs=512] epoch 7/30 train_loss=0.24752 val_loss=0.22499 val_pr_auc=0.91473 val_recall_at_10pct=0.11212\n",
                        "[lr=0.001 wd=0 bs=512] epoch 8/30 train_loss=0.24747 val_loss=0.22575 val_pr_auc=0.91473 val_recall_at_10pct=0.11213\n",
                        "[lr=0.001 wd=0 bs=512] epoch 9/30 train_loss=0.24741 val_loss=0.22484 val_pr_auc=0.91482 val_recall_at_10pct=0.11218\n",
                        "[lr=0.001 wd=0 bs=512] epoch 10/30 train_loss=0.24736 val_loss=0.22629 val_pr_auc=0.91488 val_recall_at_10pct=0.11227\n",
                        "[lr=0.001 wd=0 bs=512] epoch 11/30 train_loss=0.24734 val_loss=0.22453 val_pr_auc=0.91500 val_recall_at_10pct=0.11213\n",
                        "[lr=0.001 wd=0 bs=512] epoch 12/30 train_loss=0.24742 val_loss=0.22515 val_pr_auc=0.91498 val_recall_at_10pct=0.11221\n",
                        "[lr=0.001 wd=0 bs=512] epoch 13/30 train_loss=0.24731 val_loss=0.22499 val_pr_auc=0.91512 val_recall_at_10pct=0.11223\n",
                        "[lr=0.001 wd=0 bs=512] epoch 14/30 train_loss=0.24722 val_loss=0.22727 val_pr_auc=0.91503 val_recall_at_10pct=0.11214\n",
                        "[lr=0.001 wd=0 bs=512] epoch 15/30 train_loss=0.24723 val_loss=0.22529 val_pr_auc=0.91507 val_recall_at_10pct=0.11227\n",
                        "[lr=0.001 wd=0 bs=512] epoch 16/30 train_loss=0.24716 val_loss=0.22422 val_pr_auc=0.91535 val_recall_at_10pct=0.11229\n",
                        "[lr=0.001 wd=0 bs=512] epoch 17/30 train_loss=0.24721 val_loss=0.22479 val_pr_auc=0.91513 val_recall_at_10pct=0.11227\n",
                        "[lr=0.001 wd=0 bs=512] epoch 18/30 train_loss=0.24720 val_loss=0.22471 val_pr_auc=0.91523 val_recall_at_10pct=0.11227\n",
                        "[lr=0.001 wd=0 bs=512] epoch 19/30 train_loss=0.24719 val_loss=0.22479 val_pr_auc=0.91511 val_recall_at_10pct=0.11223\n",
                        "[lr=0.001 wd=0 bs=512] epoch 20/30 train_loss=0.24706 val_loss=0.22515 val_pr_auc=0.91524 val_recall_at_10pct=0.11227\n",
                        "[lr=0.001 wd=0 bs=512] epoch 21/30 train_loss=0.24719 val_loss=0.22475 val_pr_auc=0.91520 val_recall_at_10pct=0.11230\n",
                        "[lr=0.001 wd=0 bs=512] epoch 22/30 train_loss=0.24709 val_loss=0.22447 val_pr_auc=0.91521 val_recall_at_10pct=0.11231\n",
                        "[lr=0.001 wd=0 bs=512] epoch 23/30 train_loss=0.24706 val_loss=0.22498 val_pr_auc=0.91542 val_recall_at_10pct=0.11238\n",
                        "[lr=0.001 wd=0 bs=512] epoch 24/30 train_loss=0.24700 val_loss=0.22521 val_pr_auc=0.91513 val_recall_at_10pct=0.11223\n",
                        "[lr=0.001 wd=0 bs=512] epoch 25/30 train_loss=0.24704 val_loss=0.22502 val_pr_auc=0.91512 val_recall_at_10pct=0.11227\n",
                        "[lr=0.001 wd=0 bs=512] epoch 26/30 train_loss=0.24699 val_loss=0.22539 val_pr_auc=0.91526 val_recall_at_10pct=0.11231\n",
                        "[lr=0.001 wd=0 bs=512] epoch 27/30 train_loss=0.24710 val_loss=0.22426 val_pr_auc=0.91530 val_recall_at_10pct=0.11233\n",
                        "[lr=0.001 wd=0 bs=512] epoch 28/30 train_loss=0.24705 val_loss=0.22517 val_pr_auc=0.91527 val_recall_at_10pct=0.11233\n",
                        "[lr=0.001 wd=0 bs=512] epoch 29/30 train_loss=0.24691 val_loss=0.22507 val_pr_auc=0.91492 val_recall_at_10pct=0.11227\n",
                        "[lr=0.001 wd=0 bs=512] epoch 30/30 train_loss=0.24701 val_loss=0.22528 val_pr_auc=0.91530 val_recall_at_10pct=0.11227\n",
                        "[lr=0.001 wd=1e-05 bs=256] epoch 1/30 train_loss=0.24946 val_loss=0.22602 val_pr_auc=0.91395 val_recall_at_10pct=0.11197\n",
                        "[lr=0.001 wd=1e-05 bs=256] epoch 2/30 train_loss=0.24836 val_loss=0.22438 val_pr_auc=0.91380 val_recall_at_10pct=0.11198\n",
                        "[lr=0.001 wd=1e-05 bs=256] epoch 3/30 train_loss=0.24801 val_loss=0.22511 val_pr_auc=0.91424 val_recall_at_10pct=0.11200\n",
                        "[lr=0.001 wd=1e-05 bs=256] epoch 4/30 train_loss=0.24793 val_loss=0.22515 val_pr_auc=0.91451 val_recall_at_10pct=0.11208\n",
                        "[lr=0.001 wd=1e-05 bs=256] epoch 5/30 train_loss=0.24781 val_loss=0.22590 val_pr_auc=0.91453 val_recall_at_10pct=0.11204\n",
                        "[lr=0.001 wd=1e-05 bs=256] epoch 6/30 train_loss=0.24782 val_loss=0.22491 val_pr_auc=0.91441 val_recall_at_10pct=0.11219\n",
                        "[lr=0.001 wd=1e-05 bs=256] epoch 7/30 train_loss=0.24772 val_loss=0.22488 val_pr_auc=0.91465 val_recall_at_10pct=0.11215\n",
                        "[lr=0.001 wd=1e-05 bs=256] epoch 8/30 train_loss=0.24770 val_loss=0.22455 val_pr_auc=0.91538 val_recall_at_10pct=0.11225\n",
                        "[lr=0.001 wd=1e-05 bs=256] epoch 9/30 train_loss=0.24764 val_loss=0.22549 val_pr_auc=0.91485 val_recall_at_10pct=0.11219\n",
                        "[lr=0.001 wd=1e-05 bs=256] epoch 10/30 train_loss=0.24756 val_loss=0.22576 val_pr_auc=0.91480 val_recall_at_10pct=0.11212\n",
                        "[lr=0.001 wd=1e-05 bs=256] epoch 11/30 train_loss=0.24761 val_loss=0.22499 val_pr_auc=0.91501 val_recall_at_10pct=0.11232\n",
                        "[lr=0.001 wd=1e-05 bs=256] epoch 12/30 train_loss=0.24758 val_loss=0.22520 val_pr_auc=0.91481 val_recall_at_10pct=0.11223\n",
                        "[lr=0.001 wd=1e-05 bs=256] epoch 13/30 train_loss=0.24757 val_loss=0.22488 val_pr_auc=0.91505 val_recall_at_10pct=0.11227\n",
                        "[lr=0.001 wd=1e-05 bs=256] epoch 14/30 train_loss=0.24749 val_loss=0.22606 val_pr_auc=0.91497 val_recall_at_10pct=0.11225\n",
                        "[lr=0.001 wd=1e-05 bs=256] epoch 15/30 train_loss=0.24751 val_loss=0.22478 val_pr_auc=0.91542 val_recall_at_10pct=0.11235\n",
                        "[lr=0.001 wd=1e-05 bs=256] epoch 16/30 train_loss=0.24758 val_loss=0.22460 val_pr_auc=0.91519 val_recall_at_10pct=0.11227\n",
                        "[lr=0.001 wd=1e-05 bs=256] epoch 17/30 train_loss=0.24754 val_loss=0.22514 val_pr_auc=0.91503 val_recall_at_10pct=0.11221\n",
                        "[lr=0.001 wd=1e-05 bs=256] epoch 18/30 train_loss=0.24749 val_loss=0.22593 val_pr_auc=0.91491 val_recall_at_10pct=0.11215\n",
                        "[lr=0.001 wd=1e-05 bs=256] epoch 19/30 train_loss=0.24748 val_loss=0.22535 val_pr_auc=0.91452 val_recall_at_10pct=0.11214\n",
                        "[lr=0.001 wd=1e-05 bs=256] epoch 20/30 train_loss=0.24750 val_loss=0.22581 val_pr_auc=0.91466 val_recall_at_10pct=0.11217\n",
                        "[lr=0.001 wd=1e-05 bs=256] epoch 21/30 train_loss=0.24751 val_loss=0.22481 val_pr_auc=0.91501 val_recall_at_10pct=0.11225\n",
                        "[lr=0.001 wd=1e-05 bs=256] epoch 22/30 train_loss=0.24753 val_loss=0.22573 val_pr_auc=0.91506 val_recall_at_10pct=0.11227\n",
                        "[lr=0.001 wd=1e-05 bs=256] epoch 23/30 train_loss=0.24746 val_loss=0.22526 val_pr_auc=0.91497 val_recall_at_10pct=0.11224\n",
                        "[lr=0.001 wd=1e-05 bs=256] epoch 24/30 train_loss=0.24747 val_loss=0.22432 val_pr_auc=0.91504 val_recall_at_10pct=0.11225\n",
                        "[lr=0.001 wd=1e-05 bs=256] epoch 25/30 train_loss=0.24749 val_loss=0.22518 val_pr_auc=0.91512 val_recall_at_10pct=0.11227\n",
                        "[lr=0.001 wd=1e-05 bs=256] epoch 26/30 train_loss=0.24751 val_loss=0.22488 val_pr_auc=0.91515 val_recall_at_10pct=0.11232\n",
                        "[lr=0.001 wd=1e-05 bs=256] epoch 27/30 train_loss=0.24751 val_loss=0.22416 val_pr_auc=0.91560 val_recall_at_10pct=0.11245\n",
                        "[lr=0.001 wd=1e-05 bs=256] epoch 28/30 train_loss=0.24741 val_loss=0.22502 val_pr_auc=0.91533 val_recall_at_10pct=0.11227\n",
                        "[lr=0.001 wd=1e-05 bs=256] epoch 29/30 train_loss=0.24748 val_loss=0.22548 val_pr_auc=0.91537 val_recall_at_10pct=0.11230\n",
                        "[lr=0.001 wd=1e-05 bs=256] epoch 30/30 train_loss=0.24754 val_loss=0.22515 val_pr_auc=0.91491 val_recall_at_10pct=0.11219\n",
                        "[lr=0.001 wd=1e-05 bs=512] epoch 1/30 train_loss=0.24974 val_loss=0.22449 val_pr_auc=0.91418 val_recall_at_10pct=0.11191\n",
                        "[lr=0.001 wd=1e-05 bs=512] epoch 2/30 train_loss=0.24837 val_loss=0.22564 val_pr_auc=0.91420 val_recall_at_10pct=0.11200\n",
                        "[lr=0.001 wd=1e-05 bs=512] epoch 3/30 train_loss=0.24797 val_loss=0.22525 val_pr_auc=0.91435 val_recall_at_10pct=0.11200\n",
                        "[lr=0.001 wd=1e-05 bs=512] epoch 4/30 train_loss=0.24789 val_loss=0.22503 val_pr_auc=0.91452 val_recall_at_10pct=0.11208\n",
                        "[lr=0.001 wd=1e-05 bs=512] epoch 5/30 train_loss=0.24778 val_loss=0.22514 val_pr_auc=0.91406 val_recall_at_10pct=0.11222\n",
                        "[lr=0.001 wd=1e-05 bs=512] epoch 6/30 train_loss=0.24764 val_loss=0.22522 val_pr_auc=0.91462 val_recall_at_10pct=0.11213\n",
                        "[lr=0.001 wd=1e-05 bs=512] epoch 7/30 train_loss=0.24758 val_loss=0.22580 val_pr_auc=0.91457 val_recall_at_10pct=0.11216\n",
                        "[lr=0.001 wd=1e-05 bs=512] epoch 8/30 train_loss=0.24756 val_loss=0.22448 val_pr_auc=0.91425 val_recall_at_10pct=0.11214\n",
                        "[lr=0.001 wd=1e-05 bs=512] epoch 9/30 train_loss=0.24751 val_loss=0.22622 val_pr_auc=0.91456 val_recall_at_10pct=0.11218\n",
                        "[lr=0.001 wd=1e-05 bs=512] epoch 10/30 train_loss=0.24743 val_loss=0.22568 val_pr_auc=0.91487 val_recall_at_10pct=0.11213\n",
                        "[lr=0.001 wd=1e-05 bs=512] epoch 11/30 train_loss=0.24746 val_loss=0.22515 val_pr_auc=0.91491 val_recall_at_10pct=0.11212\n",
                        "[lr=0.001 wd=1e-05 bs=512] epoch 12/30 train_loss=0.24744 val_loss=0.22400 val_pr_auc=0.91496 val_recall_at_10pct=0.11229\n",
                        "[lr=0.001 wd=1e-05 bs=512] epoch 13/30 train_loss=0.24744 val_loss=0.22511 val_pr_auc=0.91511 val_recall_at_10pct=0.11234\n",
                        "[lr=0.001 wd=1e-05 bs=512] epoch 14/30 train_loss=0.24733 val_loss=0.22499 val_pr_auc=0.91537 val_recall_at_10pct=0.11220\n",
                        "[lr=0.001 wd=1e-05 bs=512] epoch 15/30 train_loss=0.24738 val_loss=0.22484 val_pr_auc=0.91505 val_recall_at_10pct=0.11215\n",
                        "[lr=0.001 wd=1e-05 bs=512] epoch 16/30 train_loss=0.24731 val_loss=0.22487 val_pr_auc=0.91512 val_recall_at_10pct=0.11221\n",
                        "[lr=0.001 wd=1e-05 bs=512] epoch 17/30 train_loss=0.24736 val_loss=0.22512 val_pr_auc=0.91469 val_recall_at_10pct=0.11228\n",
                        "[lr=0.001 wd=1e-05 bs=512] epoch 18/30 train_loss=0.24735 val_loss=0.22468 val_pr_auc=0.91469 val_recall_at_10pct=0.11223\n",
                        "[lr=0.001 wd=1e-05 bs=512] epoch 19/30 train_loss=0.24734 val_loss=0.22447 val_pr_auc=0.91490 val_recall_at_10pct=0.11238\n",
                        "[lr=0.001 wd=1e-05 bs=512] epoch 20/30 train_loss=0.24727 val_loss=0.22426 val_pr_auc=0.91529 val_recall_at_10pct=0.11232\n",
                        "[lr=0.001 wd=1e-05 bs=512] epoch 21/30 train_loss=0.24728 val_loss=0.22505 val_pr_auc=0.91547 val_recall_at_10pct=0.11238\n",
                        "[lr=0.001 wd=1e-05 bs=512] epoch 22/30 train_loss=0.24729 val_loss=0.22442 val_pr_auc=0.91494 val_recall_at_10pct=0.11223\n",
                        "[lr=0.001 wd=1e-05 bs=512] epoch 23/30 train_loss=0.24730 val_loss=0.22577 val_pr_auc=0.91519 val_recall_at_10pct=0.11233\n",
                        "[lr=0.001 wd=1e-05 bs=512] epoch 24/30 train_loss=0.24721 val_loss=0.22529 val_pr_auc=0.91478 val_recall_at_10pct=0.11225\n",
                        "[lr=0.001 wd=1e-05 bs=512] epoch 25/30 train_loss=0.24732 val_loss=0.22473 val_pr_auc=0.91523 val_recall_at_10pct=0.11232\n",
                        "[lr=0.001 wd=1e-05 bs=512] epoch 26/30 train_loss=0.24723 val_loss=0.22530 val_pr_auc=0.91505 val_recall_at_10pct=0.11227\n",
                        "[lr=0.001 wd=1e-05 bs=512] epoch 27/30 train_loss=0.24718 val_loss=0.22457 val_pr_auc=0.91531 val_recall_at_10pct=0.11231\n",
                        "[lr=0.001 wd=1e-05 bs=512] epoch 28/30 train_loss=0.24723 val_loss=0.22430 val_pr_auc=0.91531 val_recall_at_10pct=0.11223\n",
                        "[lr=0.001 wd=1e-05 bs=512] epoch 29/30 train_loss=0.24729 val_loss=0.22503 val_pr_auc=0.91542 val_recall_at_10pct=0.11222\n",
                        "[lr=0.001 wd=1e-05 bs=512] epoch 30/30 train_loss=0.24729 val_loss=0.22494 val_pr_auc=0.91516 val_recall_at_10pct=0.11224\n",
                        "[lr=0.001 wd=0.0001 bs=256] epoch 1/30 train_loss=0.24940 val_loss=0.22527 val_pr_auc=0.91373 val_recall_at_10pct=0.11198\n",
                        "[lr=0.001 wd=0.0001 bs=256] epoch 2/30 train_loss=0.24825 val_loss=0.22475 val_pr_auc=0.91425 val_recall_at_10pct=0.11195\n",
                        "[lr=0.001 wd=0.0001 bs=256] epoch 3/30 train_loss=0.24820 val_loss=0.22557 val_pr_auc=0.91420 val_recall_at_10pct=0.11220\n",
                        "[lr=0.001 wd=0.0001 bs=256] epoch 4/30 train_loss=0.24817 val_loss=0.22584 val_pr_auc=0.91442 val_recall_at_10pct=0.11198\n",
                        "[lr=0.001 wd=0.0001 bs=256] epoch 5/30 train_loss=0.24803 val_loss=0.22546 val_pr_auc=0.91414 val_recall_at_10pct=0.11211\n",
                        "[lr=0.001 wd=0.0001 bs=256] epoch 6/30 train_loss=0.24804 val_loss=0.22617 val_pr_auc=0.91444 val_recall_at_10pct=0.11220\n",
                        "[lr=0.001 wd=0.0001 bs=256] epoch 7/30 train_loss=0.24809 val_loss=0.22526 val_pr_auc=0.91499 val_recall_at_10pct=0.11219\n",
                        "[lr=0.001 wd=0.0001 bs=256] epoch 8/30 train_loss=0.24807 val_loss=0.22593 val_pr_auc=0.91413 val_recall_at_10pct=0.11213\n",
                        "[lr=0.001 wd=0.0001 bs=256] epoch 9/30 train_loss=0.24807 val_loss=0.22522 val_pr_auc=0.91435 val_recall_at_10pct=0.11209\n",
                        "[lr=0.001 wd=0.0001 bs=256] epoch 10/30 train_loss=0.24804 val_loss=0.22620 val_pr_auc=0.91375 val_recall_at_10pct=0.11201\n",
                        "[lr=0.001 wd=0.0001 bs=256] epoch 11/30 train_loss=0.24811 val_loss=0.22650 val_pr_auc=0.91473 val_recall_at_10pct=0.11215\n",
                        "[lr=0.001 wd=0.0001 bs=256] epoch 12/30 train_loss=0.24806 val_loss=0.22831 val_pr_auc=0.91406 val_recall_at_10pct=0.11208\n",
                        "[lr=0.001 wd=0.0001 bs=256] epoch 13/30 train_loss=0.24798 val_loss=0.22574 val_pr_auc=0.91437 val_recall_at_10pct=0.11205\n",
                        "[lr=0.001 wd=0.0001 bs=256] epoch 14/30 train_loss=0.24813 val_loss=0.22477 val_pr_auc=0.91365 val_recall_at_10pct=0.11210\n",
                        "[lr=0.001 wd=0.0001 bs=256] epoch 15/30 train_loss=0.24813 val_loss=0.22595 val_pr_auc=0.91424 val_recall_at_10pct=0.11212\n",
                        "[lr=0.001 wd=0.0001 bs=256] epoch 16/30 train_loss=0.24812 val_loss=0.22641 val_pr_auc=0.91412 val_recall_at_10pct=0.11211\n",
                        "[lr=0.001 wd=0.0001 bs=256] epoch 17/30 train_loss=0.24807 val_loss=0.22619 val_pr_auc=0.91388 val_recall_at_10pct=0.11210\n",
                        "[lr=0.001 wd=0.0001 bs=256] epoch 18/30 train_loss=0.24804 val_loss=0.22592 val_pr_auc=0.91379 val_recall_at_10pct=0.11218\n",
                        "[lr=0.001 wd=0.0001 bs=256] epoch 19/30 train_loss=0.24804 val_loss=0.22462 val_pr_auc=0.91435 val_recall_at_10pct=0.11212\n",
                        "[lr=0.001 wd=0.0001 bs=256] epoch 20/30 train_loss=0.24806 val_loss=0.22572 val_pr_auc=0.91399 val_recall_at_10pct=0.11211\n",
                        "[lr=0.001 wd=0.0001 bs=256] epoch 21/30 train_loss=0.24805 val_loss=0.22491 val_pr_auc=0.91453 val_recall_at_10pct=0.11204\n",
                        "[lr=0.001 wd=0.0001 bs=256] epoch 22/30 train_loss=0.24805 val_loss=0.22646 val_pr_auc=0.91406 val_recall_at_10pct=0.11207\n",
                        "[lr=0.001 wd=0.0001 bs=256] epoch 23/30 train_loss=0.24813 val_loss=0.22608 val_pr_auc=0.91482 val_recall_at_10pct=0.11210\n",
                        "[lr=0.001 wd=0.0001 bs=256] epoch 24/30 train_loss=0.24815 val_loss=0.22553 val_pr_auc=0.91440 val_recall_at_10pct=0.11224\n",
                        "[lr=0.001 wd=0.0001 bs=256] epoch 25/30 train_loss=0.24807 val_loss=0.22559 val_pr_auc=0.91435 val_recall_at_10pct=0.11212\n",
                        "[lr=0.001 wd=0.0001 bs=256] epoch 26/30 train_loss=0.24811 val_loss=0.22646 val_pr_auc=0.91430 val_recall_at_10pct=0.11209\n",
                        "[lr=0.001 wd=0.0001 bs=256] epoch 27/30 train_loss=0.24804 val_loss=0.22524 val_pr_auc=0.91415 val_recall_at_10pct=0.11210\n",
                        "[lr=0.001 wd=0.0001 bs=256] epoch 28/30 train_loss=0.24812 val_loss=0.22604 val_pr_auc=0.91447 val_recall_at_10pct=0.11215\n",
                        "[lr=0.001 wd=0.0001 bs=256] epoch 29/30 train_loss=0.24808 val_loss=0.22559 val_pr_auc=0.91395 val_recall_at_10pct=0.11213\n",
                        "[lr=0.001 wd=0.0001 bs=256] epoch 30/30 train_loss=0.24802 val_loss=0.22516 val_pr_auc=0.91436 val_recall_at_10pct=0.11222\n",
                        "[lr=0.001 wd=0.0001 bs=512] epoch 1/30 train_loss=0.24945 val_loss=0.22544 val_pr_auc=0.91316 val_recall_at_10pct=0.11189\n",
                        "[lr=0.001 wd=0.0001 bs=512] epoch 2/30 train_loss=0.24826 val_loss=0.22554 val_pr_auc=0.91438 val_recall_at_10pct=0.11199\n",
                        "[lr=0.001 wd=0.0001 bs=512] epoch 3/30 train_loss=0.24802 val_loss=0.22512 val_pr_auc=0.91391 val_recall_at_10pct=0.11202\n",
                        "[lr=0.001 wd=0.0001 bs=512] epoch 4/30 train_loss=0.24793 val_loss=0.22469 val_pr_auc=0.91483 val_recall_at_10pct=0.11214\n",
                        "[lr=0.001 wd=0.0001 bs=512] epoch 5/30 train_loss=0.24793 val_loss=0.22570 val_pr_auc=0.91409 val_recall_at_10pct=0.11191\n",
                        "[lr=0.001 wd=0.0001 bs=512] epoch 6/30 train_loss=0.24781 val_loss=0.22478 val_pr_auc=0.91455 val_recall_at_10pct=0.11213\n",
                        "[lr=0.001 wd=0.0001 bs=512] epoch 7/30 train_loss=0.24778 val_loss=0.22468 val_pr_auc=0.91410 val_recall_at_10pct=0.11212\n",
                        "[lr=0.001 wd=0.0001 bs=512] epoch 8/30 train_loss=0.24777 val_loss=0.22444 val_pr_auc=0.91437 val_recall_at_10pct=0.11201\n",
                        "[lr=0.001 wd=0.0001 bs=512] epoch 9/30 train_loss=0.24779 val_loss=0.22550 val_pr_auc=0.91442 val_recall_at_10pct=0.11208\n",
                        "[lr=0.001 wd=0.0001 bs=512] epoch 10/30 train_loss=0.24781 val_loss=0.22523 val_pr_auc=0.91464 val_recall_at_10pct=0.11216\n",
                        "[lr=0.001 wd=0.0001 bs=512] epoch 11/30 train_loss=0.24776 val_loss=0.22497 val_pr_auc=0.91428 val_recall_at_10pct=0.11214\n",
                        "[lr=0.001 wd=0.0001 bs=512] epoch 12/30 train_loss=0.24777 val_loss=0.22592 val_pr_auc=0.91435 val_recall_at_10pct=0.11215\n",
                        "[lr=0.001 wd=0.0001 bs=512] epoch 13/30 train_loss=0.24772 val_loss=0.22462 val_pr_auc=0.91419 val_recall_at_10pct=0.11217\n",
                        "[lr=0.001 wd=0.0001 bs=512] epoch 14/30 train_loss=0.24777 val_loss=0.22521 val_pr_auc=0.91510 val_recall_at_10pct=0.11216\n",
                        "[lr=0.001 wd=0.0001 bs=512] epoch 15/30 train_loss=0.24779 val_loss=0.22539 val_pr_auc=0.91445 val_recall_at_10pct=0.11213\n",
                        "[lr=0.001 wd=0.0001 bs=512] epoch 16/30 train_loss=0.24768 val_loss=0.22371 val_pr_auc=0.91453 val_recall_at_10pct=0.11214\n",
                        "[lr=0.001 wd=0.0001 bs=512] epoch 17/30 train_loss=0.24772 val_loss=0.22567 val_pr_auc=0.91462 val_recall_at_10pct=0.11216\n",
                        "[lr=0.001 wd=0.0001 bs=512] epoch 18/30 train_loss=0.24780 val_loss=0.22407 val_pr_auc=0.91487 val_recall_at_10pct=0.11221\n",
                        "[lr=0.001 wd=0.0001 bs=512] epoch 19/30 train_loss=0.24778 val_loss=0.22560 val_pr_auc=0.91460 val_recall_at_10pct=0.11217\n",
                        "[lr=0.001 wd=0.0001 bs=512] epoch 20/30 train_loss=0.24777 val_loss=0.22610 val_pr_auc=0.91441 val_recall_at_10pct=0.11220\n",
                        "[lr=0.001 wd=0.0001 bs=512] epoch 21/30 train_loss=0.24777 val_loss=0.22457 val_pr_auc=0.91423 val_recall_at_10pct=0.11210\n",
                        "[lr=0.001 wd=0.0001 bs=512] epoch 22/30 train_loss=0.24776 val_loss=0.22443 val_pr_auc=0.91483 val_recall_at_10pct=0.11219\n",
                        "[lr=0.001 wd=0.0001 bs=512] epoch 23/30 train_loss=0.24777 val_loss=0.22450 val_pr_auc=0.91440 val_recall_at_10pct=0.11214\n",
                        "[lr=0.001 wd=0.0001 bs=512] epoch 24/30 train_loss=0.24774 val_loss=0.22503 val_pr_auc=0.91380 val_recall_at_10pct=0.11221\n",
                        "[lr=0.001 wd=0.0001 bs=512] epoch 25/30 train_loss=0.24771 val_loss=0.22457 val_pr_auc=0.91454 val_recall_at_10pct=0.11211\n",
                        "[lr=0.001 wd=0.0001 bs=512] epoch 26/30 train_loss=0.24775 val_loss=0.22517 val_pr_auc=0.91434 val_recall_at_10pct=0.11213\n",
                        "[lr=0.001 wd=0.0001 bs=512] epoch 27/30 train_loss=0.24776 val_loss=0.22491 val_pr_auc=0.91476 val_recall_at_10pct=0.11214\n",
                        "[lr=0.001 wd=0.0001 bs=512] epoch 28/30 train_loss=0.24777 val_loss=0.22522 val_pr_auc=0.91419 val_recall_at_10pct=0.11211\n",
                        "[lr=0.001 wd=0.0001 bs=512] epoch 29/30 train_loss=0.24776 val_loss=0.22532 val_pr_auc=0.91511 val_recall_at_10pct=0.11210\n",
                        "[lr=0.001 wd=0.0001 bs=512] epoch 30/30 train_loss=0.24768 val_loss=0.22502 val_pr_auc=0.91434 val_recall_at_10pct=0.11208\n",
                        "[lr=0.002 wd=0 bs=256] epoch 1/30 train_loss=0.24925 val_loss=0.22482 val_pr_auc=0.91424 val_recall_at_10pct=0.11200\n",
                        "[lr=0.002 wd=0 bs=256] epoch 2/30 train_loss=0.24827 val_loss=0.22531 val_pr_auc=0.91462 val_recall_at_10pct=0.11217\n",
                        "[lr=0.002 wd=0 bs=256] epoch 3/30 train_loss=0.24800 val_loss=0.22614 val_pr_auc=0.91445 val_recall_at_10pct=0.11221\n",
                        "[lr=0.002 wd=0 bs=256] epoch 4/30 train_loss=0.24788 val_loss=0.22556 val_pr_auc=0.91405 val_recall_at_10pct=0.11203\n",
                        "[lr=0.002 wd=0 bs=256] epoch 5/30 train_loss=0.24782 val_loss=0.22511 val_pr_auc=0.91457 val_recall_at_10pct=0.11214\n",
                        "[lr=0.002 wd=0 bs=256] epoch 6/30 train_loss=0.24765 val_loss=0.22608 val_pr_auc=0.91501 val_recall_at_10pct=0.11223\n",
                        "[lr=0.002 wd=0 bs=256] epoch 7/30 train_loss=0.24765 val_loss=0.22518 val_pr_auc=0.91519 val_recall_at_10pct=0.11241\n",
                        "[lr=0.002 wd=0 bs=256] epoch 8/30 train_loss=0.24758 val_loss=0.22465 val_pr_auc=0.91530 val_recall_at_10pct=0.11220\n",
                        "[lr=0.002 wd=0 bs=256] epoch 9/30 train_loss=0.24751 val_loss=0.22578 val_pr_auc=0.91488 val_recall_at_10pct=0.11224\n",
                        "[lr=0.002 wd=0 bs=256] epoch 10/30 train_loss=0.24748 val_loss=0.22586 val_pr_auc=0.91535 val_recall_at_10pct=0.11222\n",
                        "[lr=0.002 wd=0 bs=256] epoch 11/30 train_loss=0.24752 val_loss=0.22526 val_pr_auc=0.91559 val_recall_at_10pct=0.11239\n",
                        "[lr=0.002 wd=0 bs=256] epoch 12/30 train_loss=0.24748 val_loss=0.22428 val_pr_auc=0.91492 val_recall_at_10pct=0.11216\n",
                        "[lr=0.002 wd=0 bs=256] epoch 13/30 train_loss=0.24735 val_loss=0.22578 val_pr_auc=0.91489 val_recall_at_10pct=0.11220\n",
                        "[lr=0.002 wd=0 bs=256] epoch 14/30 train_loss=0.24741 val_loss=0.22451 val_pr_auc=0.91533 val_recall_at_10pct=0.11232\n",
                        "[lr=0.002 wd=0 bs=256] epoch 15/30 train_loss=0.24737 val_loss=0.22443 val_pr_auc=0.91502 val_recall_at_10pct=0.11223\n",
                        "[lr=0.002 wd=0 bs=256] epoch 16/30 train_loss=0.24740 val_loss=0.22456 val_pr_auc=0.91543 val_recall_at_10pct=0.11234\n",
                        "[lr=0.002 wd=0 bs=256] epoch 17/30 train_loss=0.24730 val_loss=0.22514 val_pr_auc=0.91546 val_recall_at_10pct=0.11241\n",
                        "[lr=0.002 wd=0 bs=256] epoch 18/30 train_loss=0.24730 val_loss=0.22498 val_pr_auc=0.91544 val_recall_at_10pct=0.11234\n",
                        "[lr=0.002 wd=0 bs=256] epoch 19/30 train_loss=0.24726 val_loss=0.22475 val_pr_auc=0.91520 val_recall_at_10pct=0.11227\n",
                        "[lr=0.002 wd=0 bs=256] epoch 20/30 train_loss=0.24728 val_loss=0.22467 val_pr_auc=0.91548 val_recall_at_10pct=0.11235\n",
                        "[lr=0.002 wd=0 bs=256] epoch 21/30 train_loss=0.24724 val_loss=0.22503 val_pr_auc=0.91528 val_recall_at_10pct=0.11230\n",
                        "[lr=0.002 wd=0 bs=256] epoch 22/30 train_loss=0.24728 val_loss=0.22419 val_pr_auc=0.91548 val_recall_at_10pct=0.11234\n",
                        "[lr=0.002 wd=0 bs=256] epoch 23/30 train_loss=0.24723 val_loss=0.22361 val_pr_auc=0.91556 val_recall_at_10pct=0.11231\n",
                        "[lr=0.002 wd=0 bs=256] epoch 24/30 train_loss=0.24728 val_loss=0.22520 val_pr_auc=0.91537 val_recall_at_10pct=0.11233\n",
                        "[lr=0.002 wd=0 bs=256] epoch 25/30 train_loss=0.24722 val_loss=0.22466 val_pr_auc=0.91514 val_recall_at_10pct=0.11230\n",
                        "[lr=0.002 wd=0 bs=256] epoch 26/30 train_loss=0.24726 val_loss=0.22586 val_pr_auc=0.91537 val_recall_at_10pct=0.11239\n",
                        "[lr=0.002 wd=0 bs=256] epoch 27/30 train_loss=0.24717 val_loss=0.22429 val_pr_auc=0.91542 val_recall_at_10pct=0.11229\n",
                        "[lr=0.002 wd=0 bs=256] epoch 28/30 train_loss=0.24717 val_loss=0.22520 val_pr_auc=0.91534 val_recall_at_10pct=0.11230\n",
                        "[lr=0.002 wd=0 bs=256] epoch 29/30 train_loss=0.24717 val_loss=0.22368 val_pr_auc=0.91548 val_recall_at_10pct=0.11226\n",
                        "[lr=0.002 wd=0 bs=256] epoch 30/30 train_loss=0.24719 val_loss=0.22497 val_pr_auc=0.91570 val_recall_at_10pct=0.11233\n",
                        "[lr=0.002 wd=0 bs=512] epoch 1/30 train_loss=0.24925 val_loss=0.22481 val_pr_auc=0.91323 val_recall_at_10pct=0.11195\n",
                        "[lr=0.002 wd=0 bs=512] epoch 2/30 train_loss=0.24808 val_loss=0.22496 val_pr_auc=0.91399 val_recall_at_10pct=0.11188\n",
                        "[lr=0.002 wd=0 bs=512] epoch 3/30 train_loss=0.24793 val_loss=0.22644 val_pr_auc=0.91437 val_recall_at_10pct=0.11207\n",
                        "[lr=0.002 wd=0 bs=512] epoch 4/30 train_loss=0.24780 val_loss=0.22457 val_pr_auc=0.91451 val_recall_at_10pct=0.11224\n",
                        "[lr=0.002 wd=0 bs=512] epoch 5/30 train_loss=0.24765 val_loss=0.22434 val_pr_auc=0.91461 val_recall_at_10pct=0.11216\n",
                        "[lr=0.002 wd=0 bs=512] epoch 6/30 train_loss=0.24757 val_loss=0.22464 val_pr_auc=0.91379 val_recall_at_10pct=0.11220\n",
                        "[lr=0.002 wd=0 bs=512] epoch 7/30 train_loss=0.24757 val_loss=0.22486 val_pr_auc=0.91471 val_recall_at_10pct=0.11221\n",
                        "[lr=0.002 wd=0 bs=512] epoch 8/30 train_loss=0.24742 val_loss=0.22473 val_pr_auc=0.91459 val_recall_at_10pct=0.11212\n",
                        "[lr=0.002 wd=0 bs=512] epoch 9/30 train_loss=0.24744 val_loss=0.22550 val_pr_auc=0.91497 val_recall_at_10pct=0.11222\n",
                        "[lr=0.002 wd=0 bs=512] epoch 10/30 train_loss=0.24735 val_loss=0.22413 val_pr_auc=0.91475 val_recall_at_10pct=0.11227\n",
                        "[lr=0.002 wd=0 bs=512] epoch 11/30 train_loss=0.24738 val_loss=0.22476 val_pr_auc=0.91459 val_recall_at_10pct=0.11214\n",
                        "[lr=0.002 wd=0 bs=512] epoch 12/30 train_loss=0.24732 val_loss=0.22432 val_pr_auc=0.91530 val_recall_at_10pct=0.11228\n",
                        "[lr=0.002 wd=0 bs=512] epoch 13/30 train_loss=0.24728 val_loss=0.22442 val_pr_auc=0.91509 val_recall_at_10pct=0.11234\n",
                        "[lr=0.002 wd=0 bs=512] epoch 14/30 train_loss=0.24725 val_loss=0.22394 val_pr_auc=0.91496 val_recall_at_10pct=0.11231\n",
                        "[lr=0.002 wd=0 bs=512] epoch 15/30 train_loss=0.24729 val_loss=0.22444 val_pr_auc=0.91521 val_recall_at_10pct=0.11233\n",
                        "[lr=0.002 wd=0 bs=512] epoch 16/30 train_loss=0.24717 val_loss=0.22483 val_pr_auc=0.91542 val_recall_at_10pct=0.11223\n",
                        "[lr=0.002 wd=0 bs=512] epoch 17/30 train_loss=0.24719 val_loss=0.22476 val_pr_auc=0.91532 val_recall_at_10pct=0.11231\n",
                        "[lr=0.002 wd=0 bs=512] epoch 18/30 train_loss=0.24715 val_loss=0.22466 val_pr_auc=0.91514 val_recall_at_10pct=0.11229\n",
                        "[lr=0.002 wd=0 bs=512] epoch 19/30 train_loss=0.24720 val_loss=0.22386 val_pr_auc=0.91520 val_recall_at_10pct=0.11237\n",
                        "[lr=0.002 wd=0 bs=512] epoch 20/30 train_loss=0.24706 val_loss=0.22470 val_pr_auc=0.91513 val_recall_at_10pct=0.11228\n",
                        "[lr=0.002 wd=0 bs=512] epoch 21/30 train_loss=0.24714 val_loss=0.22473 val_pr_auc=0.91526 val_recall_at_10pct=0.11227\n",
                        "[lr=0.002 wd=0 bs=512] epoch 22/30 train_loss=0.24713 val_loss=0.22450 val_pr_auc=0.91541 val_recall_at_10pct=0.11232\n",
                        "[lr=0.002 wd=0 bs=512] epoch 23/30 train_loss=0.24710 val_loss=0.22441 val_pr_auc=0.91534 val_recall_at_10pct=0.11222\n",
                        "[lr=0.002 wd=0 bs=512] epoch 24/30 train_loss=0.24711 val_loss=0.22507 val_pr_auc=0.91561 val_recall_at_10pct=0.11230\n",
                        "[lr=0.002 wd=0 bs=512] epoch 25/30 train_loss=0.24709 val_loss=0.22609 val_pr_auc=0.91520 val_recall_at_10pct=0.11229\n",
                        "[lr=0.002 wd=0 bs=512] epoch 26/30 train_loss=0.24695 val_loss=0.22554 val_pr_auc=0.91542 val_recall_at_10pct=0.11233\n",
                        "[lr=0.002 wd=0 bs=512] epoch 27/30 train_loss=0.24713 val_loss=0.22357 val_pr_auc=0.91520 val_recall_at_10pct=0.11225\n",
                        "[lr=0.002 wd=0 bs=512] epoch 28/30 train_loss=0.24701 val_loss=0.22408 val_pr_auc=0.91517 val_recall_at_10pct=0.11227\n",
                        "[lr=0.002 wd=0 bs=512] epoch 29/30 train_loss=0.24708 val_loss=0.22444 val_pr_auc=0.91555 val_recall_at_10pct=0.11235\n",
                        "[lr=0.002 wd=0 bs=512] epoch 30/30 train_loss=0.24695 val_loss=0.22617 val_pr_auc=0.91492 val_recall_at_10pct=0.11216\n",
                        "[lr=0.002 wd=1e-05 bs=256] epoch 1/30 train_loss=0.24911 val_loss=0.22544 val_pr_auc=0.91391 val_recall_at_10pct=0.11204\n",
                        "[lr=0.002 wd=1e-05 bs=256] epoch 2/30 train_loss=0.24833 val_loss=0.22412 val_pr_auc=0.91446 val_recall_at_10pct=0.11221\n",
                        "[lr=0.002 wd=1e-05 bs=256] epoch 3/30 train_loss=0.24806 val_loss=0.22582 val_pr_auc=0.91416 val_recall_at_10pct=0.11219\n",
                        "[lr=0.002 wd=1e-05 bs=256] epoch 4/30 train_loss=0.24796 val_loss=0.22541 val_pr_auc=0.91434 val_recall_at_10pct=0.11214\n",
                        "[lr=0.002 wd=1e-05 bs=256] epoch 5/30 train_loss=0.24794 val_loss=0.22505 val_pr_auc=0.91486 val_recall_at_10pct=0.11222\n",
                        "[lr=0.002 wd=1e-05 bs=256] epoch 6/30 train_loss=0.24793 val_loss=0.22490 val_pr_auc=0.91447 val_recall_at_10pct=0.11214\n",
                        "[lr=0.002 wd=1e-05 bs=256] epoch 7/30 train_loss=0.24794 val_loss=0.22639 val_pr_auc=0.91469 val_recall_at_10pct=0.11214\n",
                        "[lr=0.002 wd=1e-05 bs=256] epoch 8/30 train_loss=0.24789 val_loss=0.22539 val_pr_auc=0.91477 val_recall_at_10pct=0.11224\n",
                        "[lr=0.002 wd=1e-05 bs=256] epoch 9/30 train_loss=0.24786 val_loss=0.22464 val_pr_auc=0.91422 val_recall_at_10pct=0.11214\n",
                        "[lr=0.002 wd=1e-05 bs=256] epoch 10/30 train_loss=0.24783 val_loss=0.22604 val_pr_auc=0.91428 val_recall_at_10pct=0.11214\n",
                        "[lr=0.002 wd=1e-05 bs=256] epoch 11/30 train_loss=0.24786 val_loss=0.22420 val_pr_auc=0.91486 val_recall_at_10pct=0.11227\n",
                        "[lr=0.002 wd=1e-05 bs=256] epoch 12/30 train_loss=0.24787 val_loss=0.22550 val_pr_auc=0.91485 val_recall_at_10pct=0.11223\n",
                        "[lr=0.002 wd=1e-05 bs=256] epoch 13/30 train_loss=0.24782 val_loss=0.22535 val_pr_auc=0.91421 val_recall_at_10pct=0.11202\n",
                        "[lr=0.002 wd=1e-05 bs=256] epoch 14/30 train_loss=0.24782 val_loss=0.22526 val_pr_auc=0.91366 val_recall_at_10pct=0.11214\n",
                        "[lr=0.002 wd=1e-05 bs=256] epoch 15/30 train_loss=0.24791 val_loss=0.22497 val_pr_auc=0.91483 val_recall_at_10pct=0.11219\n",
                        "[lr=0.002 wd=1e-05 bs=256] epoch 16/30 train_loss=0.24779 val_loss=0.22401 val_pr_auc=0.91501 val_recall_at_10pct=0.11220\n",
                        "[lr=0.002 wd=1e-05 bs=256] epoch 17/30 train_loss=0.24797 val_loss=0.22574 val_pr_auc=0.91473 val_recall_at_10pct=0.11214\n",
                        "[lr=0.002 wd=1e-05 bs=256] epoch 18/30 train_loss=0.24786 val_loss=0.22544 val_pr_auc=0.91499 val_recall_at_10pct=0.11222\n",
                        "[lr=0.002 wd=1e-05 bs=256] epoch 19/30 train_loss=0.24793 val_loss=0.22466 val_pr_auc=0.91437 val_recall_at_10pct=0.11211\n",
                        "[lr=0.002 wd=1e-05 bs=256] epoch 20/30 train_loss=0.24787 val_loss=0.22500 val_pr_auc=0.91444 val_recall_at_10pct=0.11208\n",
                        "[lr=0.002 wd=1e-05 bs=256] epoch 21/30 train_loss=0.24793 val_loss=0.22505 val_pr_auc=0.91312 val_recall_at_10pct=0.11204\n",
                        "[lr=0.002 wd=1e-05 bs=256] epoch 22/30 train_loss=0.24793 val_loss=0.22419 val_pr_auc=0.91495 val_recall_at_10pct=0.11213\n",
                        "[lr=0.002 wd=1e-05 bs=256] epoch 23/30 train_loss=0.24789 val_loss=0.22512 val_pr_auc=0.91458 val_recall_at_10pct=0.11218\n",
                        "[lr=0.002 wd=1e-05 bs=256] epoch 24/30 train_loss=0.24784 val_loss=0.22470 val_pr_auc=0.91459 val_recall_at_10pct=0.11228\n",
                        "[lr=0.002 wd=1e-05 bs=256] epoch 25/30 train_loss=0.24788 val_loss=0.22612 val_pr_auc=0.91445 val_recall_at_10pct=0.11210\n",
                        "[lr=0.002 wd=1e-05 bs=256] epoch 26/30 train_loss=0.24789 val_loss=0.22739 val_pr_auc=0.91495 val_recall_at_10pct=0.11214\n",
                        "[lr=0.002 wd=1e-05 bs=256] epoch 27/30 train_loss=0.24788 val_loss=0.22516 val_pr_auc=0.91519 val_recall_at_10pct=0.11215\n",
                        "[lr=0.002 wd=1e-05 bs=256] epoch 28/30 train_loss=0.24785 val_loss=0.22457 val_pr_auc=0.91427 val_recall_at_10pct=0.11211\n",
                        "[lr=0.002 wd=1e-05 bs=256] epoch 29/30 train_loss=0.24778 val_loss=0.22561 val_pr_auc=0.91394 val_recall_at_10pct=0.11211\n",
                        "[lr=0.002 wd=1e-05 bs=256] epoch 30/30 train_loss=0.24779 val_loss=0.22572 val_pr_auc=0.91504 val_recall_at_10pct=0.11227\n",
                        "[lr=0.002 wd=1e-05 bs=512] epoch 1/30 train_loss=0.24923 val_loss=0.22572 val_pr_auc=0.91397 val_recall_at_10pct=0.11201\n",
                        "[lr=0.002 wd=1e-05 bs=512] epoch 2/30 train_loss=0.24814 val_loss=0.22556 val_pr_auc=0.91424 val_recall_at_10pct=0.11211\n",
                        "[lr=0.002 wd=1e-05 bs=512] epoch 3/30 train_loss=0.24791 val_loss=0.22608 val_pr_auc=0.91451 val_recall_at_10pct=0.11208\n",
                        "[lr=0.002 wd=1e-05 bs=512] epoch 4/30 train_loss=0.24784 val_loss=0.22466 val_pr_auc=0.91445 val_recall_at_10pct=0.11219\n",
                        "[lr=0.002 wd=1e-05 bs=512] epoch 5/30 train_loss=0.24773 val_loss=0.22431 val_pr_auc=0.91455 val_recall_at_10pct=0.11213\n",
                        "[lr=0.002 wd=1e-05 bs=512] epoch 6/30 train_loss=0.24772 val_loss=0.22521 val_pr_auc=0.91440 val_recall_at_10pct=0.11217\n",
                        "[lr=0.002 wd=1e-05 bs=512] epoch 7/30 train_loss=0.24766 val_loss=0.22420 val_pr_auc=0.91457 val_recall_at_10pct=0.11210\n",
                        "[lr=0.002 wd=1e-05 bs=512] epoch 8/30 train_loss=0.24766 val_loss=0.22523 val_pr_auc=0.91457 val_recall_at_10pct=0.11214\n",
                        "[lr=0.002 wd=1e-05 bs=512] epoch 9/30 train_loss=0.24761 val_loss=0.22513 val_pr_auc=0.91458 val_recall_at_10pct=0.11221\n",
                        "[lr=0.002 wd=1e-05 bs=512] epoch 10/30 train_loss=0.24769 val_loss=0.22491 val_pr_auc=0.91365 val_recall_at_10pct=0.11220\n",
                        "[lr=0.002 wd=1e-05 bs=512] epoch 11/30 train_loss=0.24760 val_loss=0.22571 val_pr_auc=0.91438 val_recall_at_10pct=0.11212\n",
                        "[lr=0.002 wd=1e-05 bs=512] epoch 12/30 train_loss=0.24760 val_loss=0.22450 val_pr_auc=0.91498 val_recall_at_10pct=0.11223\n",
                        "[lr=0.002 wd=1e-05 bs=512] epoch 13/30 train_loss=0.24756 val_loss=0.22524 val_pr_auc=0.91480 val_recall_at_10pct=0.11219\n",
                        "[lr=0.002 wd=1e-05 bs=512] epoch 14/30 train_loss=0.24759 val_loss=0.22541 val_pr_auc=0.91416 val_recall_at_10pct=0.11241\n",
                        "[lr=0.002 wd=1e-05 bs=512] epoch 15/30 train_loss=0.24759 val_loss=0.22643 val_pr_auc=0.91495 val_recall_at_10pct=0.11223\n",
                        "[lr=0.002 wd=1e-05 bs=512] epoch 16/30 train_loss=0.24758 val_loss=0.22421 val_pr_auc=0.91519 val_recall_at_10pct=0.11221\n",
                        "[lr=0.002 wd=1e-05 bs=512] epoch 17/30 train_loss=0.24760 val_loss=0.22496 val_pr_auc=0.91509 val_recall_at_10pct=0.11239\n",
                        "[lr=0.002 wd=1e-05 bs=512] epoch 18/30 train_loss=0.24757 val_loss=0.22487 val_pr_auc=0.91469 val_recall_at_10pct=0.11223\n",
                        "[lr=0.002 wd=1e-05 bs=512] epoch 19/30 train_loss=0.24751 val_loss=0.22528 val_pr_auc=0.91443 val_recall_at_10pct=0.11214\n",
                        "[lr=0.002 wd=1e-05 bs=512] epoch 20/30 train_loss=0.24766 val_loss=0.22492 val_pr_auc=0.91522 val_recall_at_10pct=0.11233\n",
                        "[lr=0.002 wd=1e-05 bs=512] epoch 21/30 train_loss=0.24758 val_loss=0.22551 val_pr_auc=0.91455 val_recall_at_10pct=0.11220\n",
                        "[lr=0.002 wd=1e-05 bs=512] epoch 22/30 train_loss=0.24760 val_loss=0.22558 val_pr_auc=0.91517 val_recall_at_10pct=0.11227\n",
                        "[lr=0.002 wd=1e-05 bs=512] epoch 23/30 train_loss=0.24751 val_loss=0.22484 val_pr_auc=0.91484 val_recall_at_10pct=0.11227\n",
                        "[lr=0.002 wd=1e-05 bs=512] epoch 24/30 train_loss=0.24752 val_loss=0.22475 val_pr_auc=0.91437 val_recall_at_10pct=0.11208\n",
                        "[lr=0.002 wd=1e-05 bs=512] epoch 25/30 train_loss=0.24758 val_loss=0.22434 val_pr_auc=0.91465 val_recall_at_10pct=0.11213\n",
                        "[lr=0.002 wd=1e-05 bs=512] epoch 26/30 train_loss=0.24752 val_loss=0.22537 val_pr_auc=0.91475 val_recall_at_10pct=0.11210\n",
                        "[lr=0.002 wd=1e-05 bs=512] epoch 27/30 train_loss=0.24749 val_loss=0.22521 val_pr_auc=0.91491 val_recall_at_10pct=0.11224\n",
                        "[lr=0.002 wd=1e-05 bs=512] epoch 28/30 train_loss=0.24766 val_loss=0.22555 val_pr_auc=0.91525 val_recall_at_10pct=0.11220\n",
                        "[lr=0.002 wd=1e-05 bs=512] epoch 29/30 train_loss=0.24759 val_loss=0.22438 val_pr_auc=0.91500 val_recall_at_10pct=0.11218\n",
                        "[lr=0.002 wd=1e-05 bs=512] epoch 30/30 train_loss=0.24755 val_loss=0.22563 val_pr_auc=0.91501 val_recall_at_10pct=0.11213\n",
                        "[lr=0.002 wd=0.0001 bs=256] epoch 1/30 train_loss=0.24928 val_loss=0.22647 val_pr_auc=0.91373 val_recall_at_10pct=0.11191\n",
                        "[lr=0.002 wd=0.0001 bs=256] epoch 2/30 train_loss=0.24859 val_loss=0.22512 val_pr_auc=0.91414 val_recall_at_10pct=0.11216\n",
                        "[lr=0.002 wd=0.0001 bs=256] epoch 3/30 train_loss=0.24868 val_loss=0.22457 val_pr_auc=0.91392 val_recall_at_10pct=0.11182\n",
                        "[lr=0.002 wd=0.0001 bs=256] epoch 4/30 train_loss=0.24853 val_loss=0.22481 val_pr_auc=0.91314 val_recall_at_10pct=0.11193\n",
                        "[lr=0.002 wd=0.0001 bs=256] epoch 5/30 train_loss=0.24859 val_loss=0.22542 val_pr_auc=0.91389 val_recall_at_10pct=0.11220\n",
                        "[lr=0.002 wd=0.0001 bs=256] epoch 6/30 train_loss=0.24853 val_loss=0.22563 val_pr_auc=0.91317 val_recall_at_10pct=0.11206\n",
                        "[lr=0.002 wd=0.0001 bs=256] epoch 7/30 train_loss=0.24847 val_loss=0.22521 val_pr_auc=0.91362 val_recall_at_10pct=0.11208\n",
                        "[lr=0.002 wd=0.0001 bs=256] epoch 8/30 train_loss=0.24852 val_loss=0.22637 val_pr_auc=0.91391 val_recall_at_10pct=0.11214\n",
                        "[lr=0.002 wd=0.0001 bs=256] epoch 9/30 train_loss=0.24855 val_loss=0.22589 val_pr_auc=0.91406 val_recall_at_10pct=0.11205\n",
                        "[lr=0.002 wd=0.0001 bs=256] epoch 10/30 train_loss=0.24856 val_loss=0.22702 val_pr_auc=0.91383 val_recall_at_10pct=0.11201\n",
                        "[lr=0.002 wd=0.0001 bs=256] epoch 11/30 train_loss=0.24857 val_loss=0.22663 val_pr_auc=0.91298 val_recall_at_10pct=0.11206\n",
                        "[lr=0.002 wd=0.0001 bs=256] epoch 12/30 train_loss=0.24852 val_loss=0.22641 val_pr_auc=0.91348 val_recall_at_10pct=0.11193\n",
                        "[lr=0.002 wd=0.0001 bs=256] epoch 13/30 train_loss=0.24855 val_loss=0.22529 val_pr_auc=0.91372 val_recall_at_10pct=0.11189\n",
                        "[lr=0.002 wd=0.0001 bs=256] epoch 14/30 train_loss=0.24847 val_loss=0.22530 val_pr_auc=0.91389 val_recall_at_10pct=0.11223\n",
                        "[lr=0.002 wd=0.0001 bs=256] epoch 15/30 train_loss=0.24850 val_loss=0.22632 val_pr_auc=0.91358 val_recall_at_10pct=0.11197\n",
                        "[lr=0.002 wd=0.0001 bs=256] epoch 16/30 train_loss=0.24854 val_loss=0.22548 val_pr_auc=0.91386 val_recall_at_10pct=0.11204\n",
                        "[lr=0.002 wd=0.0001 bs=256] epoch 17/30 train_loss=0.24863 val_loss=0.22498 val_pr_auc=0.91368 val_recall_at_10pct=0.11200\n",
                        "[lr=0.002 wd=0.0001 bs=256] epoch 18/30 train_loss=0.24854 val_loss=0.22578 val_pr_auc=0.91403 val_recall_at_10pct=0.11186\n",
                        "[lr=0.002 wd=0.0001 bs=256] epoch 19/30 train_loss=0.24851 val_loss=0.22453 val_pr_auc=0.91485 val_recall_at_10pct=0.11216\n",
                        "[lr=0.002 wd=0.0001 bs=256] epoch 20/30 train_loss=0.24856 val_loss=0.22580 val_pr_auc=0.91441 val_recall_at_10pct=0.11207\n",
                        "[lr=0.002 wd=0.0001 bs=256] epoch 21/30 train_loss=0.24848 val_loss=0.22459 val_pr_auc=0.91333 val_recall_at_10pct=0.11195\n",
                        "[lr=0.002 wd=0.0001 bs=256] epoch 22/30 train_loss=0.24859 val_loss=0.22481 val_pr_auc=0.91420 val_recall_at_10pct=0.11208\n",
                        "[lr=0.002 wd=0.0001 bs=256] epoch 23/30 train_loss=0.24853 val_loss=0.22623 val_pr_auc=0.91296 val_recall_at_10pct=0.11208\n",
                        "[lr=0.002 wd=0.0001 bs=256] epoch 24/30 train_loss=0.24867 val_loss=0.22533 val_pr_auc=0.91417 val_recall_at_10pct=0.11202\n",
                        "[lr=0.002 wd=0.0001 bs=256] epoch 25/30 train_loss=0.24862 val_loss=0.22490 val_pr_auc=0.91421 val_recall_at_10pct=0.11183\n",
                        "[lr=0.002 wd=0.0001 bs=256] epoch 26/30 train_loss=0.24859 val_loss=0.22485 val_pr_auc=0.91412 val_recall_at_10pct=0.11205\n",
                        "[lr=0.002 wd=0.0001 bs=256] epoch 27/30 train_loss=0.24861 val_loss=0.22609 val_pr_auc=0.91375 val_recall_at_10pct=0.11197\n",
                        "[lr=0.002 wd=0.0001 bs=256] epoch 28/30 train_loss=0.24852 val_loss=0.22554 val_pr_auc=0.91406 val_recall_at_10pct=0.11212\n",
                        "[lr=0.002 wd=0.0001 bs=256] epoch 29/30 train_loss=0.24862 val_loss=0.22452 val_pr_auc=0.91305 val_recall_at_10pct=0.11183\n",
                        "[lr=0.002 wd=0.0001 bs=256] epoch 30/30 train_loss=0.24858 val_loss=0.22605 val_pr_auc=0.91288 val_recall_at_10pct=0.11195\n",
                        "[lr=0.002 wd=0.0001 bs=512] epoch 1/30 train_loss=0.24918 val_loss=0.22560 val_pr_auc=0.91434 val_recall_at_10pct=0.11213\n",
                        "[lr=0.002 wd=0.0001 bs=512] epoch 2/30 train_loss=0.24835 val_loss=0.22597 val_pr_auc=0.91393 val_recall_at_10pct=0.11187\n",
                        "[lr=0.002 wd=0.0001 bs=512] epoch 3/30 train_loss=0.24826 val_loss=0.22533 val_pr_auc=0.91377 val_recall_at_10pct=0.11208\n",
                        "[lr=0.002 wd=0.0001 bs=512] epoch 4/30 train_loss=0.24818 val_loss=0.22625 val_pr_auc=0.91461 val_recall_at_10pct=0.11208\n",
                        "[lr=0.002 wd=0.0001 bs=512] epoch 5/30 train_loss=0.24825 val_loss=0.22537 val_pr_auc=0.91431 val_recall_at_10pct=0.11201\n",
                        "[lr=0.002 wd=0.0001 bs=512] epoch 6/30 train_loss=0.24823 val_loss=0.22508 val_pr_auc=0.91385 val_recall_at_10pct=0.11219\n",
                        "[lr=0.002 wd=0.0001 bs=512] epoch 7/30 train_loss=0.24819 val_loss=0.22656 val_pr_auc=0.91392 val_recall_at_10pct=0.11200\n",
                        "[lr=0.002 wd=0.0001 bs=512] epoch 8/30 train_loss=0.24822 val_loss=0.22515 val_pr_auc=0.91453 val_recall_at_10pct=0.11215\n",
                        "[lr=0.002 wd=0.0001 bs=512] epoch 9/30 train_loss=0.24822 val_loss=0.22650 val_pr_auc=0.91455 val_recall_at_10pct=0.11211\n",
                        "[lr=0.002 wd=0.0001 bs=512] epoch 10/30 train_loss=0.24823 val_loss=0.22540 val_pr_auc=0.91416 val_recall_at_10pct=0.11210\n",
                        "[lr=0.002 wd=0.0001 bs=512] epoch 11/30 train_loss=0.24819 val_loss=0.22613 val_pr_auc=0.91388 val_recall_at_10pct=0.11202\n",
                        "[lr=0.002 wd=0.0001 bs=512] epoch 12/30 train_loss=0.24822 val_loss=0.22565 val_pr_auc=0.91424 val_recall_at_10pct=0.11208\n",
                        "[lr=0.002 wd=0.0001 bs=512] epoch 13/30 train_loss=0.24820 val_loss=0.22466 val_pr_auc=0.91445 val_recall_at_10pct=0.11209\n",
                        "[lr=0.002 wd=0.0001 bs=512] epoch 14/30 train_loss=0.24814 val_loss=0.22567 val_pr_auc=0.91352 val_recall_at_10pct=0.11206\n",
                        "[lr=0.002 wd=0.0001 bs=512] epoch 15/30 train_loss=0.24823 val_loss=0.22596 val_pr_auc=0.91421 val_recall_at_10pct=0.11182\n",
                        "[lr=0.002 wd=0.0001 bs=512] epoch 16/30 train_loss=0.24817 val_loss=0.22553 val_pr_auc=0.91454 val_recall_at_10pct=0.11204\n",
                        "[lr=0.002 wd=0.0001 bs=512] epoch 17/30 train_loss=0.24817 val_loss=0.22532 val_pr_auc=0.91404 val_recall_at_10pct=0.11211\n",
                        "[lr=0.002 wd=0.0001 bs=512] epoch 18/30 train_loss=0.24815 val_loss=0.22579 val_pr_auc=0.91437 val_recall_at_10pct=0.11196\n",
                        "[lr=0.002 wd=0.0001 bs=512] epoch 19/30 train_loss=0.24820 val_loss=0.22602 val_pr_auc=0.91302 val_recall_at_10pct=0.11208\n",
                        "[lr=0.002 wd=0.0001 bs=512] epoch 20/30 train_loss=0.24814 val_loss=0.22539 val_pr_auc=0.91370 val_recall_at_10pct=0.11198\n",
                        "[lr=0.002 wd=0.0001 bs=512] epoch 21/30 train_loss=0.24817 val_loss=0.22634 val_pr_auc=0.91461 val_recall_at_10pct=0.11204\n",
                        "[lr=0.002 wd=0.0001 bs=512] epoch 22/30 train_loss=0.24822 val_loss=0.22520 val_pr_auc=0.91374 val_recall_at_10pct=0.11210\n",
                        "[lr=0.002 wd=0.0001 bs=512] epoch 23/30 train_loss=0.24826 val_loss=0.22610 val_pr_auc=0.91351 val_recall_at_10pct=0.11207\n",
                        "[lr=0.002 wd=0.0001 bs=512] epoch 24/30 train_loss=0.24830 val_loss=0.22492 val_pr_auc=0.91399 val_recall_at_10pct=0.11214\n",
                        "[lr=0.002 wd=0.0001 bs=512] epoch 25/30 train_loss=0.24823 val_loss=0.22565 val_pr_auc=0.91443 val_recall_at_10pct=0.11211\n",
                        "[lr=0.002 wd=0.0001 bs=512] epoch 26/30 train_loss=0.24819 val_loss=0.22676 val_pr_auc=0.91404 val_recall_at_10pct=0.11200\n",
                        "[lr=0.002 wd=0.0001 bs=512] epoch 27/30 train_loss=0.24823 val_loss=0.22569 val_pr_auc=0.91441 val_recall_at_10pct=0.11208\n",
                        "[lr=0.002 wd=0.0001 bs=512] epoch 28/30 train_loss=0.24821 val_loss=0.22367 val_pr_auc=0.91425 val_recall_at_10pct=0.11203\n",
                        "[lr=0.002 wd=0.0001 bs=512] epoch 29/30 train_loss=0.24828 val_loss=0.22635 val_pr_auc=0.91407 val_recall_at_10pct=0.11214\n",
                        "[lr=0.002 wd=0.0001 bs=512] epoch 30/30 train_loss=0.24833 val_loss=0.22521 val_pr_auc=0.91390 val_recall_at_10pct=0.11203\n",
                        "best_score: (0.1124464705069761, 0.9156030108927137)\n",
                        "best_hp: {'lr': 0.001, 'weight_decay': 1e-05, 'batch_size': 256}\n",
                        "best_epoch: 27\n",
                        "retrain epoch 1/27 loss=0.23936\n",
                        "retrain epoch 2/27 loss=0.23829\n",
                        "retrain epoch 3/27 loss=0.23810\n",
                        "retrain epoch 4/27 loss=0.23787\n",
                        "retrain epoch 5/27 loss=0.23783\n",
                        "retrain epoch 6/27 loss=0.23777\n",
                        "retrain epoch 7/27 loss=0.23772\n",
                        "retrain epoch 8/27 loss=0.23773\n",
                        "retrain epoch 9/27 loss=0.23761\n",
                        "retrain epoch 10/27 loss=0.23760\n",
                        "retrain epoch 11/27 loss=0.23764\n",
                        "retrain epoch 12/27 loss=0.23765\n",
                        "retrain epoch 13/27 loss=0.23765\n",
                        "retrain epoch 14/27 loss=0.23752\n",
                        "retrain epoch 15/27 loss=0.23766\n",
                        "retrain epoch 16/27 loss=0.23757\n",
                        "retrain epoch 17/27 loss=0.23756\n",
                        "retrain epoch 18/27 loss=0.23760\n",
                        "retrain epoch 19/27 loss=0.23762\n",
                        "retrain epoch 20/27 loss=0.23756\n",
                        "retrain epoch 21/27 loss=0.23758\n",
                        "retrain epoch 22/27 loss=0.23759\n",
                        "retrain epoch 23/27 loss=0.23756\n",
                        "retrain epoch 24/27 loss=0.23757\n",
                        "retrain epoch 25/27 loss=0.23755\n",
                        "retrain epoch 26/27 loss=0.23756\n",
                        "retrain epoch 27/27 loss=0.23755\n",
                        "test_eval_start\n",
                        "test_pr_auc: 0.9330922581436691\n",
                        "saved keys: ['model', 'scaler', 'config', 'eval_dir', 'version_dir', 'figure_pr_curve', 'figure_confusion_matrix_top5', 'figure_confusion_matrix_top10', 'figure_confusion_matrix_top15', 'figure_confusion_matrix_top30']\n",
                        "model -> /Users/jy/project_2nd/SKN23-2nd-3Team/models/dl/mlp_enhance/tuned/model.pt\n",
                        "scaler -> /Users/jy/project_2nd/SKN23-2nd-3Team/models/preprocessing/mlp_enhance/tuned/scaler.pkl\n",
                        "config -> /Users/jy/project_2nd/SKN23-2nd-3Team/models/configs/mlp_enhance/tuned/config.json\n",
                        "eval_dir -> /Users/jy/project_2nd/SKN23-2nd-3Team/models/eval/dlmlp_enhance\n",
                        "version_dir -> tuned\n",
                        "figure_pr_curve -> /Users/jy/project_2nd/SKN23-2nd-3Team/assets/training/mlp_enhance/tuned/pr_curve.png\n",
                        "figure_confusion_matrix_top5 -> /Users/jy/project_2nd/SKN23-2nd-3Team/assets/training/mlp_enhance/tuned/confusion_matrix_top5.png\n",
                        "figure_confusion_matrix_top10 -> /Users/jy/project_2nd/SKN23-2nd-3Team/assets/training/mlp_enhance/tuned/confusion_matrix_top10.png\n",
                        "figure_confusion_matrix_top15 -> /Users/jy/project_2nd/SKN23-2nd-3Team/assets/training/mlp_enhance/tuned/confusion_matrix_top15.png\n",
                        "figure_confusion_matrix_top30 -> /Users/jy/project_2nd/SKN23-2nd-3Team/assets/training/mlp_enhance/tuned/confusion_matrix_top30.png\n",
                        "Done\n"
                    ]
                }
            ],
            "source": [
                "from __future__ import annotations\n",
                "\n",
                "import sys\n",
                "from pathlib import Path\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader, TensorDataset\n",
                "\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.metrics import precision_recall_curve, confusion_matrix, average_precision_score\n",
                "\n",
                "\n",
                "here = Path.cwd().resolve()\n",
                "project_root = None\n",
                "for p in [here] + list(here.parents):\n",
                "    if (p / \"app\").exists() and (p / \"models\").exists() and (p / \"data\").exists():\n",
                "        project_root = p\n",
                "        break\n",
                "if project_root is None:\n",
                "    raise FileNotFoundError(f\"   .  ={here}\")\n",
                "if str(project_root) not in sys.path:\n",
                "    sys.path.insert(0, str(project_root))\n",
                "\n",
                "from app.utils.paths import DEFAULT_PATHS as P, ensure_runtime_dirs\n",
                "from app.utils.metrics import evaluate_churn_metrics\n",
                "from app.utils.save import save_model_and_artifacts\n",
                "from models.model_definitions import MLP_enhance\n",
                "\n",
                "try:\n",
                "    from app.utils.plotting import configure_matplotlib_korean\n",
                "    configure_matplotlib_korean()\n",
                "except Exception:\n",
                "    pass\n",
                "\n",
                "ensure_runtime_dirs()\n",
                "\n",
                "\n",
                "SEED = 42\n",
                "\n",
                "REPORT_K_LIST = (5, 10, 15, 30)\n",
                "BEST_K_PCT = 10\n",
                "\n",
                "LR_LIST = [3e-4, 1e-3, 2e-3]\n",
                "WD_LIST = [0.0, 1e-5, 1e-4]\n",
                "BATCH_LIST = [256, 512]\n",
                "\n",
                "MAX_EPOCHS = 30\n",
                "USE_POS_WEIGHT = True\n",
                "RETRAIN_ON_TRAIN_VAL = True\n",
                "SAVE_VERSION = \"tuned\"\n",
                "\n",
                "\n",
                "def seed_everything(seed: int = 42) -> None:\n",
                "    np.random.seed(seed)\n",
                "    torch.manual_seed(seed)\n",
                "    if torch.cuda.is_available():\n",
                "        torch.cuda.manual_seed_all(seed)\n",
                "\n",
                "\n",
                "def get_device() -> str:\n",
                "    if torch.cuda.is_available():\n",
                "        return \"cuda\"\n",
                "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
                "        return \"mps\"\n",
                "    return \"cpu\"\n",
                "\n",
                "\n",
                "def plot_confusion_matrix_figure(y_true, y_pred, title: str, labels=(\"non_m2\", \"m2\")):\n",
                "    y_true_arr = np.asarray(y_true, dtype=int).reshape(-1)\n",
                "    y_pred_arr = np.asarray(y_pred, dtype=int).reshape(-1)\n",
                "    cm = confusion_matrix(y_true_arr, y_pred_arr)\n",
                "\n",
                "    fig, ax = plt.subplots(figsize=(6, 5))\n",
                "    im = ax.imshow(cm, interpolation=\"nearest\", aspect=\"equal\", cmap=\"Blues\")\n",
                "    fig.colorbar(im, ax=ax)\n",
                "\n",
                "    ax.set_title(title)\n",
                "    ax.set_xlabel(\"Predicted\")\n",
                "    ax.set_ylabel(\"Actual\")\n",
                "    ax.set_xticks([0, 1])\n",
                "    ax.set_yticks([0, 1])\n",
                "    ax.set_xticklabels(list(labels))\n",
                "    ax.set_yticklabels(list(labels))\n",
                "\n",
                "    thresh = float(cm.max()) / 2.0 if cm.size else 0.0\n",
                "    for i in range(cm.shape[0]):\n",
                "        for j in range(cm.shape[1]):\n",
                "            ax.text(\n",
                "                j, i, str(cm[i, j]),\n",
                "                ha=\"center\", va=\"center\",\n",
                "                color=\"white\" if float(cm[i, j]) > thresh else \"black\",\n",
                "                fontsize=12,\n",
                "            )\n",
                "\n",
                "    ax.set_xlim(-0.5, cm.shape[1] - 0.5)\n",
                "    ax.set_ylim(cm.shape[0] - 0.5, -0.5)\n",
                "    fig.tight_layout()\n",
                "    return fig\n",
                "\n",
                "\n",
                "def topk_threshold(y_prob: np.ndarray, k_pct: int) -> float:\n",
                "    prob = np.asarray(y_prob, dtype=float).reshape(-1)\n",
                "    order = np.argsort(-prob)\n",
                "    n_sel = int(np.floor(len(prob) * (float(k_pct) / 100.0)))\n",
                "    n_sel = max(n_sel, 1)\n",
                "    return float(prob[order[n_sel - 1]])\n",
                "\n",
                "\n",
                "def plot_confusion_topk(y_true, y_prob, k_pct: int, labels=(\"non_m2\", \"m2\")):\n",
                "    thr = topk_threshold(np.asarray(y_prob, dtype=float), int(k_pct))\n",
                "    y_pred = (np.asarray(y_prob, dtype=float) >= thr).astype(int)\n",
                "    return plot_confusion_matrix_figure(\n",
                "        y_true, y_pred,\n",
                "        title=f\"Confusion Matrix (Top {int(k_pct)}%, thr={thr:.5f})\",\n",
                "        labels=labels,\n",
                "    )\n",
                "\n",
                "\n",
                "def recall_at_topk(y_true: np.ndarray, y_prob: np.ndarray, k_pct: int) -> float:\n",
                "    y_true = np.asarray(y_true).astype(int).reshape(-1)\n",
                "    y_prob = np.asarray(y_prob).astype(float).reshape(-1)\n",
                "    order = np.argsort(-y_prob)\n",
                "    n = max(int(np.floor(len(y_true) * (k_pct / 100.0))), 1)\n",
                "    top_idx = order[:n]\n",
                "    return float(y_true[top_idx].sum() / max(y_true.sum(), 1))\n",
                "\n",
                "\n",
                "@torch.no_grad()\n",
                "def predict_probs(model: nn.Module, loader: DataLoader, device: str):\n",
                "    model.eval()\n",
                "    probs, trues = [], []\n",
                "    for xb, yb in loader:\n",
                "        xb = xb.to(device)\n",
                "        logits = model(xb).view(-1)\n",
                "        prob = torch.sigmoid(logits).detach().cpu().numpy().reshape(-1)\n",
                "        probs.append(prob)\n",
                "        trues.append(yb.detach().cpu().numpy().reshape(-1))\n",
                "    y_true = np.concatenate(trues).astype(int).reshape(-1)\n",
                "    y_prob = np.concatenate(probs).astype(float).reshape(-1)\n",
                "    return y_true, y_prob\n",
                "\n",
                "\n",
                "def make_criterion(y_bin: np.ndarray, device: str):\n",
                "    if not USE_POS_WEIGHT:\n",
                "        return nn.BCEWithLogitsLoss()\n",
                "\n",
                "    pos = int((y_bin == 1).sum())\n",
                "    neg = int((y_bin == 0).sum())\n",
                "    pw = torch.tensor([neg / max(pos, 1)], dtype=torch.float32).to(device)\n",
                "    return nn.BCEWithLogitsLoss(pos_weight=pw)\n",
                "\n",
                "\n",
                "def load_merged_dataset():\n",
                "    print(\"Load parquet files\")\n",
                "\n",
                "    anchors = pd.read_parquet(P.parquet_path(\"anchors\"))\n",
                "    features = pd.read_parquet(P.parquet_path(\"features_ml_clean\"))\n",
                "    labels = pd.read_parquet(P.parquet_path(\"labels\"))\n",
                "\n",
                "    for df in (anchors, features, labels):\n",
                "        df[\"user_id\"] = df[\"user_id\"].astype(str)\n",
                "\n",
                "    if \"split\" in anchors.columns:\n",
                "        anchors = anchors.drop(columns=[\"split\"])\n",
                "\n",
                "    need_cols = [c for c in [\"user_id\", \"anchor_time\", \"label\", \"split\"] if c in labels.columns]\n",
                "    if \"split\" not in need_cols:\n",
                "        raise KeyError(f\"labels.parquet split  . labels columns head: {list(labels.columns)[:50]}\")\n",
                "\n",
                "    data = anchors.merge(features, on=[\"user_id\", \"anchor_time\"], how=\"inner\")\n",
                "    data = data.merge(labels[need_cols], on=[\"user_id\", \"anchor_time\"], how=\"inner\")\n",
                "\n",
                "    data[\"target\"] = data[\"label\"].astype(str).eq(\"m2\").astype(int)\n",
                "    split_col = data[\"split\"].astype(str)\n",
                "\n",
                "    feature_cols = [c for c in features.columns if c not in (\"user_id\", \"anchor_time\")]\n",
                "    X_all = data.loc[:, feature_cols].fillna(0.0)\n",
                "    y_all = data[\"target\"].to_numpy(dtype=int)\n",
                "\n",
                "    idx_train = split_col.eq(\"train\").to_numpy()\n",
                "    idx_val = split_col.eq(\"val\").to_numpy()\n",
                "    idx_test = split_col.eq(\"test\").to_numpy()\n",
                "\n",
                "    if int(idx_train.sum()) == 0 or int(idx_val.sum()) == 0 or int(idx_test.sum()) == 0:\n",
                "        raise ValueError(\n",
                "            f\"split  . train/val/test={int(idx_train.sum())}/{int(idx_val.sum())}/{int(idx_test.sum())}\"\n",
                "        )\n",
                "\n",
                "    X_train = X_all.loc[idx_train].to_numpy(dtype=float)\n",
                "    y_train = y_all[idx_train]\n",
                "    X_val = X_all.loc[idx_val].to_numpy(dtype=float)\n",
                "    y_val = y_all[idx_val]\n",
                "    X_test = X_all.loc[idx_test].to_numpy(dtype=float)\n",
                "    y_test = y_all[idx_test]\n",
                "\n",
                "    print(\"rows:\", len(data), \"train/val/test:\", int(idx_train.sum()), int(idx_val.sum()), int(idx_test.sum()))\n",
                "    print(\"n_features:\", int(X_train.shape[1]))\n",
                "\n",
                "    return X_train, y_train, X_val, y_val, X_test, y_test, feature_cols\n",
                "\n",
                "\n",
                "def train_one_config(\n",
                "    X_train: np.ndarray, y_train: np.ndarray,\n",
                "    X_val: np.ndarray, y_val: np.ndarray,\n",
                "    lr: float, weight_decay: float, batch_size: int,\n",
                "    device: str,\n",
                "):\n",
                "    scaler = StandardScaler()\n",
                "    X_train_s = scaler.fit_transform(X_train)\n",
                "    X_val_s = scaler.transform(X_val)\n",
                "\n",
                "    train_ds = TensorDataset(torch.tensor(X_train_s, dtype=torch.float32),\n",
                "                             torch.tensor(y_train, dtype=torch.float32))\n",
                "    val_ds = TensorDataset(torch.tensor(X_val_s, dtype=torch.float32),\n",
                "                           torch.tensor(y_val, dtype=torch.float32))\n",
                "\n",
                "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
                "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
                "\n",
                "    input_dim = int(X_train_s.shape[1])\n",
                "    model = MLP_enhance(input_dim=input_dim).to(device)\n",
                "\n",
                "    criterion = make_criterion(y_train, device)\n",
                "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
                "\n",
                "    def run_epoch(loader, train_mode: bool) -> float:\n",
                "        model.train() if train_mode else model.eval()\n",
                "        loss_sum, n_sum = 0.0, 0\n",
                "        with torch.set_grad_enabled(train_mode):\n",
                "            for xb, yb in loader:\n",
                "                xb = xb.to(device)\n",
                "                yb = yb.to(device).view(-1)\n",
                "\n",
                "                if train_mode:\n",
                "                    optimizer.zero_grad()\n",
                "\n",
                "                logits = model(xb).view(-1)\n",
                "                loss = criterion(logits, yb)\n",
                "\n",
                "                if train_mode:\n",
                "                    loss.backward()\n",
                "                    optimizer.step()\n",
                "\n",
                "                bs = int(xb.shape[0])\n",
                "                loss_sum += float(loss.item()) * bs\n",
                "                n_sum += bs\n",
                "\n",
                "        return float(loss_sum / max(n_sum, 1))\n",
                "\n",
                "    best = {\n",
                "        \"epoch\": 0,\n",
                "        \"val_recall_k\": -1.0,\n",
                "        \"val_pr_auc\": -1.0,\n",
                "        \"state_dict\": None,\n",
                "    }\n",
                "\n",
                "    for epoch in range(1, MAX_EPOCHS + 1):\n",
                "        tr_loss = run_epoch(train_loader, True)\n",
                "        va_loss = run_epoch(val_loader, False)\n",
                "\n",
                "        yv_true, yv_prob = predict_probs(model, val_loader, device)\n",
                "        val_pr_auc = float(average_precision_score(yv_true, yv_prob))\n",
                "        val_recall_k = recall_at_topk(yv_true, yv_prob, BEST_K_PCT)\n",
                "\n",
                "        print(\n",
                "            f\"[lr={lr:g} wd={weight_decay:g} bs={batch_size}] \"\n",
                "            f\"epoch {epoch}/{MAX_EPOCHS} \"\n",
                "            f\"train_loss={tr_loss:.5f} val_loss={va_loss:.5f} \"\n",
                "            f\"val_pr_auc={val_pr_auc:.5f} val_recall_at_{BEST_K_PCT}pct={val_recall_k:.5f}\"\n",
                "        )\n",
                "\n",
                "        better = (val_recall_k > best[\"val_recall_k\"] + 1e-12) or (\n",
                "            abs(val_recall_k - best[\"val_recall_k\"]) <= 1e-12 and val_pr_auc > best[\"val_pr_auc\"]\n",
                "        )\n",
                "        if better:\n",
                "            best[\"epoch\"] = epoch\n",
                "            best[\"val_recall_k\"] = val_recall_k\n",
                "            best[\"val_pr_auc\"] = val_pr_auc\n",
                "            best[\"state_dict\"] = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
                "\n",
                "    model.load_state_dict(best[\"state_dict\"])\n",
                "    return model, scaler, best\n",
                "\n",
                "\n",
                "def retrain_train_val(\n",
                "    X_train: np.ndarray, y_train: np.ndarray,\n",
                "    X_val: np.ndarray, y_val: np.ndarray,\n",
                "    lr: float, weight_decay: float, batch_size: int,\n",
                "    best_epoch: int,\n",
                "    device: str,\n",
                "):\n",
                "    X_tv = np.concatenate([X_train, X_val], axis=0)\n",
                "    y_tv = np.concatenate([y_train, y_val], axis=0)\n",
                "\n",
                "    scaler = StandardScaler()\n",
                "    X_tv_s = scaler.fit_transform(X_tv)\n",
                "\n",
                "    ds = TensorDataset(torch.tensor(X_tv_s, dtype=torch.float32),\n",
                "                       torch.tensor(y_tv, dtype=torch.float32))\n",
                "    loader = DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
                "\n",
                "    input_dim = int(X_tv_s.shape[1])\n",
                "    model = MLP_enhance(input_dim=input_dim).to(device)\n",
                "\n",
                "    criterion = make_criterion(y_tv, device)\n",
                "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
                "\n",
                "    model.train()\n",
                "    for epoch in range(1, best_epoch + 1):\n",
                "        loss_sum, n_sum = 0.0, 0\n",
                "        for xb, yb in loader:\n",
                "            xb = xb.to(device)\n",
                "            yb = yb.to(device).view(-1)\n",
                "\n",
                "            optimizer.zero_grad()\n",
                "            logits = model(xb).view(-1)\n",
                "            loss = criterion(logits, yb)\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "\n",
                "            bs = int(xb.shape[0])\n",
                "            loss_sum += float(loss.item()) * bs\n",
                "            n_sum += bs\n",
                "\n",
                "        print(f\"retrain epoch {epoch}/{best_epoch} loss={loss_sum/max(n_sum,1):.5f}\")\n",
                "\n",
                "    return model, scaler\n",
                "\n",
                "\n",
                "def evaluate_test_and_save(model, scaler, device, X_test, y_test):\n",
                "    X_test_s = scaler.transform(X_test)\n",
                "    test_ds = TensorDataset(torch.tensor(X_test_s, dtype=torch.float32),\n",
                "                            torch.tensor(y_test, dtype=torch.float32))\n",
                "    test_loader = DataLoader(test_ds, batch_size=512, shuffle=False, num_workers=0)\n",
                "\n",
                "    y_true, y_prob = predict_probs(model, test_loader, device)\n",
                "\n",
                "    metrics = evaluate_churn_metrics(y_true, y_prob)\n",
                "    print(\"test_pr_auc:\", float(metrics.get(\"PR-AUC (Average Precision)\", 0.0)))\n",
                "\n",
                "    precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
                "    pr_auc_val = metrics.get(\"PR-AUC (Average Precision)\")\n",
                "    pr_auc_val = float(average_precision_score(y_true, y_prob)) if pr_auc_val is None else float(pr_auc_val)\n",
                "\n",
                "    fig_pr, ax_pr = plt.subplots(figsize=(6, 5))\n",
                "    ax_pr.plot(recall, precision, lw=2, label=f\"PR-AUC={pr_auc_val:.5f}\")\n",
                "    ax_pr.set_xlabel(\"Recall\")\n",
                "    ax_pr.set_ylabel(\"Precision\")\n",
                "    ax_pr.set_title(\"Precision-Recall Curve\")\n",
                "    ax_pr.grid(alpha=0.3)\n",
                "    ax_pr.legend()\n",
                "    fig_pr.tight_layout()\n",
                "\n",
                "    figures = {\"pr_curve\": fig_pr}\n",
                "    for k_pct in REPORT_K_LIST:\n",
                "        figures[f\"confusion_matrix_top{k_pct}\"] = plot_confusion_topk(\n",
                "            y_true=y_true,\n",
                "            y_prob=y_prob,\n",
                "            k_pct=int(k_pct),\n",
                "            labels=(\"non_m2\", \"m2\"),\n",
                "        )\n",
                "\n",
                "    saved = save_model_and_artifacts(\n",
                "        model=model,\n",
                "        model_name=\"mlp_enhance\",\n",
                "        model_type=\"dl\",\n",
                "        model_id=\"dl__mlp_enhance\",\n",
                "        split=\"test\",\n",
                "        metrics=metrics,\n",
                "        y_true=y_true,\n",
                "        y_prob=y_prob,\n",
                "        version=SAVE_VERSION,\n",
                "        scaler=scaler,\n",
                "        figures=figures,\n",
                "        config={\n",
                "            \"model_name\": \"mlp_enhance\",\n",
                "            \"model_type\": \"dl\",\n",
                "            \"version\": SAVE_VERSION,\n",
                "            \"feature_source\": \"features_ml_clean.parquet\",\n",
                "            \"best_selection\": f\"val Recall@{BEST_K_PCT}%\",\n",
                "            \"use_pos_weight\": bool(USE_POS_WEIGHT),\n",
                "            \"note\": \"hp tuned on train/val, test evaluated once\",\n",
                "        },\n",
                "    )\n",
                "\n",
                "    plt.close(fig_pr)\n",
                "    for k_pct in REPORT_K_LIST:\n",
                "        plt.close(figures[f\"confusion_matrix_top{k_pct}\"])\n",
                "\n",
                "    print(\"saved keys:\", list(saved.keys()))\n",
                "    for k, v in saved.items():\n",
                "        print(k, \"->\", v)\n",
                "\n",
                "\n",
                "def main():\n",
                "    seed_everything(SEED)\n",
                "    device = get_device()\n",
                "    print(\"device:\", device)\n",
                "\n",
                "    X_train, y_train, X_val, y_val, X_test, y_test, feature_cols = load_merged_dataset()\n",
                "\n",
                "    best_overall = {\n",
                "        \"score\": (-1.0, -1.0),\n",
                "        \"hp\": None,\n",
                "        \"epoch\": 0,\n",
                "        \"model_state\": None,\n",
                "        \"scaler\": None,\n",
                "    }\n",
                "\n",
                "    print(\"tuning start\")\n",
                "    for lr in LR_LIST:\n",
                "        for wd in WD_LIST:\n",
                "            for bs in BATCH_LIST:\n",
                "                model, scaler, best = train_one_config(\n",
                "                    X_train, y_train, X_val, y_val,\n",
                "                    lr=lr, weight_decay=wd, batch_size=bs,\n",
                "                    device=device,\n",
                "                )\n",
                "\n",
                "                cand_score = (best[\"val_recall_k\"], best[\"val_pr_auc\"])\n",
                "                if (cand_score[0] > best_overall[\"score\"][0] + 1e-12) or (\n",
                "                    abs(cand_score[0] - best_overall[\"score\"][0]) <= 1e-12 and cand_score[1] > best_overall[\"score\"][1]\n",
                "                ):\n",
                "                    best_overall[\"score\"] = cand_score\n",
                "                    best_overall[\"hp\"] = {\"lr\": lr, \"weight_decay\": wd, \"batch_size\": bs}\n",
                "                    best_overall[\"epoch\"] = int(best[\"epoch\"])\n",
                "                    best_overall[\"model_state\"] = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
                "                    best_overall[\"scaler\"] = scaler\n",
                "\n",
                "    print(\"best_score:\", best_overall[\"score\"])\n",
                "    print(\"best_hp:\", best_overall[\"hp\"])\n",
                "    print(\"best_epoch:\", best_overall[\"epoch\"])\n",
                "\n",
                "    hp = best_overall[\"hp\"]\n",
                "    if hp is None:\n",
                "        raise RuntimeError(\"No best hyperparameters found\")\n",
                "\n",
                "    if RETRAIN_ON_TRAIN_VAL:\n",
                "        final_model, final_scaler = retrain_train_val(\n",
                "            X_train, y_train, X_val, y_val,\n",
                "            lr=float(hp[\"lr\"]),\n",
                "            weight_decay=float(hp[\"weight_decay\"]),\n",
                "            batch_size=int(hp[\"batch_size\"]),\n",
                "            best_epoch=int(best_overall[\"epoch\"]),\n",
                "            device=device,\n",
                "        )\n",
                "    else:\n",
                "        input_dim = int(X_train.shape[1])\n",
                "        final_model = MLP_enhance(input_dim=input_dim).to(device)\n",
                "        final_model.load_state_dict(best_overall[\"model_state\"])\n",
                "        final_model.eval()\n",
                "        final_scaler = best_overall[\"scaler\"]\n",
                "\n",
                "    print(\"test_eval_start\")\n",
                "    evaluate_test_and_save(final_model, final_scaler, device, X_test, y_test)\n",
                "    print(\"Done\")\n",
                "\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    main()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "churn_venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.14.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
