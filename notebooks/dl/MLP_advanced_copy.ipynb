{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3e5b9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,\n",
    "    precision_recall_curve,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9129d306",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = Path.cwd().parents[1]\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "def configure_matplotlib_korean():\n",
    "    import platform\n",
    "    import matplotlib as mpl\n",
    "    from matplotlib import font_manager as fm\n",
    "\n",
    "    system = platform.system()\n",
    "    if system == \"Darwin\":\n",
    "        candidates = [\"AppleGothic\", \"Pretendard\", \"Noto Sans CJK KR\", \"NanumGothic\"]\n",
    "    elif system == \"Windows\":\n",
    "        candidates = [\"Malgun Gothic\", \"Pretendard\", \"Noto Sans CJK KR\", \"NanumGothic\"]\n",
    "    else:\n",
    "        candidates = [\"NanumGothic\", \"Noto Sans CJK KR\", \"Pretendard\", \"DejaVu Sans\"]\n",
    "\n",
    "    chosen = None\n",
    "    for name in candidates:\n",
    "        try:\n",
    "            fm.findfont(name, fallback_to_default=False)\n",
    "            chosen = name\n",
    "            break\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    if chosen:\n",
    "        mpl.rcParams[\"font.family\"] = chosen\n",
    "    mpl.rcParams[\"axes.unicode_minus\"] = False\n",
    "    return chosen\n",
    "\n",
    "_ = configure_matplotlib_korean()\n",
    "\n",
    "from app.utils.save import save_model_and_artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2df10c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded shapes: (574092, 14) (137615, 14) (101833, 14)\n",
      "train pos rate: 0.8041254711788355 val: 0.8416524361443156 test: 0.8671255879724648\n"
     ]
    }
   ],
   "source": [
    "SAMPLES_DIR = PROJECT_ROOT / \"outputs\" / \"samples\"\n",
    "if not SAMPLES_DIR.exists():\n",
    "    SAMPLES_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "\n",
    "anchors_path  = SAMPLES_DIR / \"anchors.parquet\"\n",
    "features_path = SAMPLES_DIR / \"features_ml_clean.parquet\"\n",
    "labels_path   = SAMPLES_DIR / \"labels.parquet\"\n",
    "\n",
    "anchors  = pd.read_parquet(anchors_path)\n",
    "features = pd.read_parquet(features_path)\n",
    "labels   = pd.read_parquet(labels_path)\n",
    "\n",
    "anchors[\"user_id\"] = anchors[\"user_id\"].astype(str)\n",
    "features[\"user_id\"] = features[\"user_id\"].astype(str)\n",
    "labels[\"user_id\"] = labels[\"user_id\"].astype(str)\n",
    "\n",
    "if \"split\" not in labels.columns:\n",
    "    raise ValueError(\"labels.parquet에 split 컬럼이 없습니다.\")\n",
    "labels = labels.rename(columns={\"split\": \"split_label\"})\n",
    "\n",
    "data = anchors.merge(features, on=[\"user_id\", \"anchor_time\"], how=\"inner\")\n",
    "data = data.merge(labels, on=[\"user_id\", \"anchor_time\"], how=\"inner\")\n",
    "\n",
    "data[\"target\"] = (data[\"label\"] == \"m2\").astype(int)\n",
    "\n",
    "feature_cols = [c for c in features.columns if c not in [\"user_id\", \"anchor_time\"]]\n",
    "X_all = data[feature_cols].to_numpy()\n",
    "y_all = data[\"target\"].to_numpy().astype(int)\n",
    "\n",
    "train_df = data[data[\"split_label\"] == \"train\"].copy()\n",
    "val_df   = data[data[\"split_label\"] == \"val\"].copy()\n",
    "test_df  = data[data[\"split_label\"] == \"test\"].copy()\n",
    "\n",
    "X_train = train_df[feature_cols].to_numpy()\n",
    "y_train = train_df[\"target\"].to_numpy().astype(int)\n",
    "\n",
    "X_val = val_df[feature_cols].to_numpy()\n",
    "y_val = val_df[\"target\"].to_numpy().astype(int)\n",
    "\n",
    "X_test = test_df[feature_cols].to_numpy()\n",
    "y_test = test_df[\"target\"].to_numpy().astype(int)\n",
    "\n",
    "print(\"loaded shapes:\", X_train.shape, X_val.shape, X_test.shape)\n",
    "print(\"train pos rate:\", y_train.mean(), \"val:\", y_val.mean(), \"test:\", y_test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "989d3ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE 제거: X_train_res, y_train_res 대신 원본 X_train, y_train 사용\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "train_y = torch.tensor(y_train, dtype=torch.float32)\n",
    "val_y   = torch.tensor(y_val, dtype=torch.float32)\n",
    "test_y  = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(torch.tensor(X_train_scaled, dtype=torch.float32), train_y),\n",
    "    batch_size=256, shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    TensorDataset(torch.tensor(X_val_scaled, dtype=torch.float32), val_y),\n",
    "    batch_size=256, shuffle=False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    TensorDataset(torch.tensor(X_test_scaled, dtype=torch.float32), test_y),\n",
    "    batch_size=256, shuffle=False\n",
    ")\n",
    "\n",
    "# loss 쪽에서 pos_weight 적용 (학습 코드에 넣기)\n",
    "n_pos = float((y_train == 1).sum())\n",
    "n_neg = float((y_train == 0).sum())\n",
    "pos_weight = torch.tensor([n_neg / max(n_pos, 1.0)], dtype=torch.float32, device=device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c4786d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"mlp_advanced\"\n",
    "MODEL_ID = \"dl__mlp_advanced\"\n",
    "VERSION = \"baseline\"\n",
    "SPLIT = \"test\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "\n",
    "try:\n",
    "    from models.model_definitions import MLP_advanced\n",
    "    MODEL_IMPORT_OK = True\n",
    "except Exception:\n",
    "    MODEL_IMPORT_OK = False\n",
    "\n",
    "if not MODEL_IMPORT_OK:\n",
    "    class ResidualBlock(nn.Module):\n",
    "        def __init__(self, hidden_dim: int, dropout: float = 0.1):\n",
    "            super().__init__()\n",
    "            self.block = nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "            )\n",
    "            self.relu = nn.ReLU()\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.relu(self.block(x) + x)\n",
    "\n",
    "    class MLP_advanced(nn.Module):\n",
    "        def __init__(self, input_dim: int, hidden_dim: int = 256, num_blocks: int = 2):\n",
    "            super().__init__()\n",
    "            self.input_layer = nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "            self.blocks = nn.ModuleList([ResidualBlock(hidden_dim) for _ in range(num_blocks)])\n",
    "            self.output_layer = nn.Sequential(\n",
    "                nn.Linear(hidden_dim, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(32, 1),\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            out = self.input_layer(x)\n",
    "            for blk in self.blocks:\n",
    "                out = blk(out)\n",
    "            return self.output_layer(out)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        bce = F.binary_cross_entropy_with_logits(logits, targets, reduction=\"none\")\n",
    "        pt = torch.exp(-bce)\n",
    "        loss = self.alpha * (1 - pt) ** self.gamma * bce\n",
    "        return loss.mean()\n",
    "\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "\n",
    "model = MLP_advanced(input_dim=input_dim, hidden_dim=256, num_blocks=2).to(device)\n",
    "criterion = FocalLoss(alpha=0.25, gamma=2.0)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "n_epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "467e49fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 01 | train_loss=0.02969 | val_pr_auc=0.91337 | best=0.91337\n",
      "epoch 02 | train_loss=0.02952 | val_pr_auc=0.91496 | best=0.91496\n",
      "epoch 03 | train_loss=0.02948 | val_pr_auc=0.91532 | best=0.91532\n",
      "epoch 04 | train_loss=0.02945 | val_pr_auc=0.91509 | best=0.91532\n",
      "epoch 05 | train_loss=0.02944 | val_pr_auc=0.91534 | best=0.91534\n",
      "epoch 06 | train_loss=0.02942 | val_pr_auc=0.91526 | best=0.91534\n",
      "epoch 07 | train_loss=0.02941 | val_pr_auc=0.91574 | best=0.91574\n",
      "epoch 08 | train_loss=0.02940 | val_pr_auc=0.91564 | best=0.91574\n",
      "epoch 09 | train_loss=0.02939 | val_pr_auc=0.91565 | best=0.91574\n",
      "epoch 10 | train_loss=0.02938 | val_pr_auc=0.91625 | best=0.91625\n",
      "epoch 11 | train_loss=0.02938 | val_pr_auc=0.91568 | best=0.91625\n",
      "epoch 12 | train_loss=0.02937 | val_pr_auc=0.91565 | best=0.91625\n",
      "epoch 13 | train_loss=0.02937 | val_pr_auc=0.91589 | best=0.91625\n",
      "epoch 14 | train_loss=0.02936 | val_pr_auc=0.91601 | best=0.91625\n",
      "epoch 15 | train_loss=0.02935 | val_pr_auc=0.91607 | best=0.91625\n"
     ]
    }
   ],
   "source": [
    "def predict_proba_from_loader(model, loader, device):\n",
    "    model.eval()\n",
    "    probs = []\n",
    "    trues = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb).view(-1)\n",
    "            prob = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "            probs.append(prob)\n",
    "            trues.append(yb.detach().cpu().numpy())\n",
    "    y_prob = np.concatenate(probs)\n",
    "    y_true = np.concatenate(trues).astype(int)\n",
    "    return y_true, y_prob\n",
    "\n",
    "best_val = -1.0\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device).view(-1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb).view(-1)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    val_true, val_prob = predict_proba_from_loader(model, val_loader, device)\n",
    "    val_pr_auc = float(average_precision_score(val_true, val_prob))\n",
    "\n",
    "    if val_pr_auc > best_val:\n",
    "        best_val = val_pr_auc\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    print(f\"epoch {epoch:02d} | train_loss={np.mean(losses):.5f} | val_pr_auc={val_pr_auc:.5f} | best={best_val:.5f}\")\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6b2f8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_churn_metrics(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_prob = np.asarray(y_prob).astype(float)\n",
    "\n",
    "    pr_auc = float(average_precision_score(y_true, y_prob))\n",
    "    base_rate = float(y_true.mean())\n",
    "\n",
    "    df_res = pd.DataFrame({\"label\": y_true, \"prob\": y_prob}).sort_values(\"prob\", ascending=False)\n",
    "\n",
    "    def at_k(k):\n",
    "        n = len(df_res)\n",
    "        n_sel = max(int(np.floor(n * k / 100)), 1)\n",
    "        top = df_res.iloc[:n_sel]\n",
    "        prec = float(top[\"label\"].mean())\n",
    "        total_pos = float(df_res[\"label\"].sum())\n",
    "        cap_pos = float(top[\"label\"].sum())\n",
    "        rec = float(cap_pos / total_pos) if total_pos > 0 else 0.0\n",
    "        lift = float(prec / base_rate) if base_rate > 0 else 0.0\n",
    "        return prec, rec, lift\n",
    "\n",
    "    ranking_list = []\n",
    "    for k in [5, 10, 15, 20, 25, 30]:\n",
    "        prec, rec, lift = at_k(k)\n",
    "        ranking_list.append({\"Top_K\": f\"{k}%\", \"Precision\": prec, \"Recall\": rec, \"Lift\": lift})\n",
    "\n",
    "    p5, r5, l5 = at_k(5)\n",
    "\n",
    "    return {\n",
    "        \"PR-AUC (Average Precision)\": pr_auc,\n",
    "        \"상위 5% 정밀도 (Precision)\": p5,\n",
    "        \"상위 5% 재현율 (Recall)\": r5,\n",
    "        \"상위 5% 리프트 (Lift)\": l5,\n",
    "        \"ranking\": ranking_list,\n",
    "    }\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title, labels=(\"비이탈(m1)\", \"이탈(m2)\"), cmap=\"Blues\"):\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_pred = np.asarray(y_pred).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    im = ax.imshow(cm, cmap=cmap, interpolation=\"nearest\", aspect=\"equal\")\n",
    "    fig.colorbar(im, ax=ax)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Predicted (예측값)\")\n",
    "    ax.set_ylabel(\"Actual (실제값)\")\n",
    "\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_yticks([0, 1])\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_yticklabels(labels)\n",
    "\n",
    "    thresh = cm.max() / 2.0 if cm.size else 0\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(\n",
    "                j, i, f\"{cm[i, j]}\",\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "                fontsize=12,\n",
    "            )\n",
    "\n",
    "    ax.set_xlim(-0.5, cm.shape[1] - 0.5)\n",
    "    ax.set_ylim(cm.shape[0] - 0.5, -0.5)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def threshold_topk(y_prob, k_pct):\n",
    "    y_prob = np.asarray(y_prob).astype(float)\n",
    "    scores = np.sort(y_prob)[::-1]\n",
    "    n = len(scores)\n",
    "    n_sel = max(int(np.floor(n * k_pct / 100)), 1)\n",
    "    return float(scores[n_sel - 1])\n",
    "\n",
    "# test_true, test_prob\n",
    "test_true, test_prob = predict_proba_from_loader(model, test_loader, device)\n",
    "\n",
    "# metrics (json 저장용)\n",
    "metrics = evaluate_churn_metrics(test_true, test_prob)\n",
    "pr_auc_val = float(metrics[\"PR-AUC (Average Precision)\"])\n",
    "\n",
    "# PR curve figure (저장용)\n",
    "precision, recall, _ = precision_recall_curve(test_true, test_prob)\n",
    "fig_pr, ax_pr = plt.subplots(figsize=(6, 5))\n",
    "ax_pr.plot(recall, precision, lw=2, label=f\"PR-AUC = {pr_auc_val:.5f}\")\n",
    "ax_pr.set_xlabel(\"Recall\")\n",
    "ax_pr.set_ylabel(\"Precision\")\n",
    "ax_pr.set_title(\"Precision-Recall Curve\")\n",
    "ax_pr.legend()\n",
    "ax_pr.grid(alpha=0.3)\n",
    "fig_pr.tight_layout()\n",
    "\n",
    "# Confusion matrices (저장용)\n",
    "k_list = [5, 10, 15, 30]\n",
    "figures = {\"pr_curve\": fig_pr}\n",
    "\n",
    "for k in k_list:\n",
    "    thr = threshold_topk(test_prob, k)\n",
    "    y_pred_k = (test_prob >= thr).astype(int)\n",
    "    fig_cm = plot_confusion_matrix(\n",
    "        test_true,\n",
    "        y_pred_k,\n",
    "        title=f\"Confusion Matrix (Top {k}%, thr={thr:.5f})\",\n",
    "        labels=(\"비이탈(m1)\", \"이탈(m2)\"),\n",
    "        cmap=\"Blues\",\n",
    "    )\n",
    "    figures[f\"confusion_matrix_top{k}\"] = fig_cm\n",
    "\n",
    "# 저장만 (출력 X)\n",
    "_ = save_model_and_artifacts(\n",
    "    model=model,\n",
    "    model_name=\"mlp_enhance\",     # 너 모델명으로\n",
    "    model_type=\"dl\",\n",
    "    model_id=\"dl__mlp_enhance\",   # 너 model_id로\n",
    "    split=\"test\",\n",
    "    metrics=metrics,\n",
    "    y_true=test_true,\n",
    "    y_prob=test_prob,\n",
    "    version=\"baseline\",\n",
    "    scaler=scaler,\n",
    "    figures=figures,\n",
    ")\n",
    "\n",
    "# 메모리 정리 (노트북 출력도 안 뜨게 + 메모리 누수 방지)\n",
    "plt.close(fig_pr)\n",
    "for k in k_list:\n",
    "    plt.close(figures[f\"confusion_matrix_top{k}\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9cfc24d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"PR-AUC (Average Precision)\": 0.9338137050393234,\n",
      "  \"상위 5% 정밀도 (Precision)\": 0.9693576900412493,\n",
      "  \"상위 5% 재현율 (Recall)\": 0.05588774886186043,\n",
      "  \"상위 5% 리프트 (Lift)\": 1.1178976880475022,\n",
      "  \"ranking\": [\n",
      "    {\n",
      "      \"Top_K\": \"5%\",\n",
      "      \"Precision\": 0.9693576900412493,\n",
      "      \"Recall\": 0.05588774886186043,\n",
      "      \"Lift\": 1.1178976880475022\n",
      "    },\n",
      "    {\n",
      "      \"Top_K\": \"10%\",\n",
      "      \"Precision\": 0.9637631346361583,\n",
      "      \"Recall\": 0.11114131050259338,\n",
      "      \"Lift\": 1.1114458482186576\n",
      "    },\n",
      "    {\n",
      "      \"Top_K\": \"15%\",\n",
      "      \"Precision\": 0.9593426738248003,\n",
      "      \"Recall\": 0.16594188127109238,\n",
      "      \"Lift\": 1.1063480159407588\n",
      "    },\n",
      "    {\n",
      "      \"Top_K\": \"20%\",\n",
      "      \"Precision\": 0.9557104978886379,\n",
      "      \"Recall\": 0.22042535842902766,\n",
      "      \"Lift\": 1.1021592617550413\n",
      "    },\n",
      "    {\n",
      "      \"Top_K\": \"25%\",\n",
      "      \"Precision\": 0.9510959226962055,\n",
      "      \"Recall\": 0.27420669973500034,\n",
      "      \"Lift\": 1.0968375698842914\n",
      "    },\n",
      "    {\n",
      "      \"Top_K\": \"30%\",\n",
      "      \"Precision\": 0.9482143441683852,\n",
      "      \"Recall\": 0.3280446649000023,\n",
      "      \"Lift\": 1.093514431266553\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "saved paths:\n",
      "model: /Users/jy/project_2nd/SKN23-2nd-3Team/models/dl/mlp_advanced/baseline/model.pt\n",
      "scaler: /Users/jy/project_2nd/SKN23-2nd-3Team/models/preprocessing/mlp_advanced/baseline/scaler.pkl\n",
      "metrics: /Users/jy/project_2nd/SKN23-2nd-3Team/models/metrics/mlp_advanced/baseline/metrics.json\n",
      "figure_pr_curve: /Users/jy/project_2nd/SKN23-2nd-3Team/assets/training/mlp_advanced/baseline/pr_curve.png\n",
      "figure_confusion_matrix_top5: /Users/jy/project_2nd/SKN23-2nd-3Team/assets/training/mlp_advanced/baseline/confusion_matrix_top5.png\n",
      "figure_confusion_matrix_top10: /Users/jy/project_2nd/SKN23-2nd-3Team/assets/training/mlp_advanced/baseline/confusion_matrix_top10.png\n",
      "figure_confusion_matrix_top15: /Users/jy/project_2nd/SKN23-2nd-3Team/assets/training/mlp_advanced/baseline/confusion_matrix_top15.png\n",
      "figure_confusion_matrix_top30: /Users/jy/project_2nd/SKN23-2nd-3Team/assets/training/mlp_advanced/baseline/confusion_matrix_top30.png\n",
      "config: /Users/jy/project_2nd/SKN23-2nd-3Team/models/configs/mlp_advanced/baseline/config.json\n",
      "eval_dir: /Users/jy/project_2nd/SKN23-2nd-3Team/models/eval/dlmlp_advanced\n"
     ]
    }
   ],
   "source": [
    "saved = save_model_and_artifacts(\n",
    "    model=model,\n",
    "    model_name=MODEL_NAME,\n",
    "    model_type=\"dl\",\n",
    "    model_id=MODEL_ID,\n",
    "    split=SPLIT,\n",
    "    metrics=metrics,\n",
    "    y_true=test_true,\n",
    "    y_prob=test_prob,\n",
    "    version=VERSION,\n",
    "    scaler=scaler,\n",
    "    figures=figures,\n",
    ")\n",
    "\n",
    "print(json.dumps(metrics, indent=2, ensure_ascii=False))\n",
    "\n",
    "print(\"saved paths:\")\n",
    "for k, v in saved.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "plt.close(fig_pr)\n",
    "for k in k_list:\n",
    "    plt.close(figures[f\"confusion_matrix_top{k}\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49f4458b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"model_id\": \"dl__mlp_advanced\",\n",
      "  \"split\": \"test\",\n",
      "  \"percentiles\": [\n",
      "    {\n",
      "      \"pct\": 1,\n",
      "      \"score\": 0.7368711423873902\n",
      "    },\n",
      "    {\n",
      "      \"pct\": 5,\n",
      "      \"score\": 0.7281568169593811\n",
      "    },\n",
      "    {\n",
      "      \"pct\": 10,\n",
      "      \"score\": 0.7191614866256714\n",
      "    },\n",
      "    {\n",
      "      \"pct\": 20,\n",
      "      \"score\": 0.7029853701591492\n",
      "    },\n",
      "    {\n",
      "      \"pct\": 30,\n",
      "      \"score\": 0.6850807428359985\n",
      "    },\n",
      "    {\n",
      "      \"pct\": 50,\n",
      "      \"score\": 0.6561292409896851\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "/Users/jy/project_2nd/SKN23-2nd-3Team/models/metrics/mlp_advanced_score_percentiles.json\n"
     ]
    }
   ],
   "source": [
    "MODEL_ID = \"dl__mlp_advanced\"\n",
    "SPLIT = \"test\"\n",
    "PCTS = [1, 5, 10, 20, 30, 50]\n",
    "\n",
    "scores = np.asarray(test_prob, dtype=float).reshape(-1)\n",
    "\n",
    "percentiles = []\n",
    "for pct in PCTS:\n",
    "    cutoff = float(np.quantile(scores, 1.0 - pct / 100.0))\n",
    "    percentiles.append({\"pct\": int(pct), \"score\": cutoff})\n",
    "\n",
    "payload = {\"model_id\": MODEL_ID, \"split\": SPLIT, \"percentiles\": percentiles}\n",
    "\n",
    "out_path = PROJECT_ROOT / \"models\" / \"metrics\" / \"mlp_advanced_score_percentiles.json\"\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "out_path.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(json.dumps(payload, ensure_ascii=False, indent=2))\n",
    "print(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c658ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "churn_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
