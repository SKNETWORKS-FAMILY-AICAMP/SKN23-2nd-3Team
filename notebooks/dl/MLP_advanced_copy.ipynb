{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae7e6cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ project root added to sys.path: /Users/jy/project_2nd/SKN23-2nd-3Team\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# 현재 파일 위치 기준으로 위로 올라가며 \"app\" 폴더를 찾는다\n",
    "p = Path.cwd().resolve()\n",
    "for _ in range(6):  # 최대 6단계 위까지 탐색\n",
    "    if (p / \"app\").exists() and (p / \"models\").exists():\n",
    "        sys.path.insert(0, str(p))\n",
    "        print(\"✅ project root added to sys.path:\", p)\n",
    "        break\n",
    "    p = p.parent\n",
    "else:\n",
    "    raise RuntimeError(\"❌ 프로젝트 루트를 찾지 못했어요. app/ 와 models/ 가 있는 폴더에서 실행해야 합니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "045a975c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load parquet files\n",
      "rows: 813540 train/val/test: 574092 137615 101833\n",
      "n_features: 14\n",
      "device: mps\n",
      "pos_weight enabled: pos=461642 neg=112450 pos_weight=0.2436\n",
      "Train MLP_advanced (best by val Recall@10%)\n",
      "epoch 1/30 train_loss=0.24806 val_loss=0.22607 val_PR-AUC=0.91440 val_Recall@10%=0.11215\n",
      "epoch 2/30 train_loss=0.24720 val_loss=0.22655 val_PR-AUC=0.91464 val_Recall@10%=0.11221\n",
      "epoch 3/30 train_loss=0.24711 val_loss=0.22512 val_PR-AUC=0.91486 val_Recall@10%=0.11224\n",
      "epoch 4/30 train_loss=0.24695 val_loss=0.22445 val_PR-AUC=0.91533 val_Recall@10%=0.11231\n",
      "epoch 5/30 train_loss=0.24685 val_loss=0.22543 val_PR-AUC=0.91480 val_Recall@10%=0.11223\n",
      "epoch 6/30 train_loss=0.24676 val_loss=0.22582 val_PR-AUC=0.91473 val_Recall@10%=0.11225\n",
      "epoch 7/30 train_loss=0.24677 val_loss=0.22693 val_PR-AUC=0.91545 val_Recall@10%=0.11218\n",
      "epoch 8/30 train_loss=0.24667 val_loss=0.22447 val_PR-AUC=0.91595 val_Recall@10%=0.11251\n",
      "epoch 9/30 train_loss=0.24667 val_loss=0.22619 val_PR-AUC=0.91531 val_Recall@10%=0.11226\n",
      "epoch 10/30 train_loss=0.24661 val_loss=0.22553 val_PR-AUC=0.91580 val_Recall@10%=0.11226\n",
      "epoch 11/30 train_loss=0.24662 val_loss=0.22378 val_PR-AUC=0.91559 val_Recall@10%=0.11246\n",
      "epoch 12/30 train_loss=0.24659 val_loss=0.22523 val_PR-AUC=0.91505 val_Recall@10%=0.11231\n",
      "epoch 13/30 train_loss=0.24654 val_loss=0.22404 val_PR-AUC=0.91587 val_Recall@10%=0.11246\n",
      "epoch 14/30 train_loss=0.24653 val_loss=0.22334 val_PR-AUC=0.91611 val_Recall@10%=0.11232\n",
      "epoch 15/30 train_loss=0.24648 val_loss=0.22573 val_PR-AUC=0.91546 val_Recall@10%=0.11231\n",
      "epoch 16/30 train_loss=0.24645 val_loss=0.22481 val_PR-AUC=0.91579 val_Recall@10%=0.11237\n",
      "epoch 17/30 train_loss=0.24649 val_loss=0.22647 val_PR-AUC=0.91509 val_Recall@10%=0.11218\n",
      "epoch 18/30 train_loss=0.24647 val_loss=0.22428 val_PR-AUC=0.91569 val_Recall@10%=0.11233\n",
      "epoch 19/30 train_loss=0.24646 val_loss=0.22659 val_PR-AUC=0.91597 val_Recall@10%=0.11246\n",
      "epoch 20/30 train_loss=0.24643 val_loss=0.22539 val_PR-AUC=0.91592 val_Recall@10%=0.11232\n",
      "epoch 21/30 train_loss=0.24640 val_loss=0.22417 val_PR-AUC=0.91611 val_Recall@10%=0.11248\n",
      "epoch 22/30 train_loss=0.24639 val_loss=0.22379 val_PR-AUC=0.91561 val_Recall@10%=0.11232\n",
      "epoch 23/30 train_loss=0.24638 val_loss=0.22552 val_PR-AUC=0.91578 val_Recall@10%=0.11226\n",
      "epoch 24/30 train_loss=0.24639 val_loss=0.22507 val_PR-AUC=0.91557 val_Recall@10%=0.11236\n",
      "epoch 25/30 train_loss=0.24637 val_loss=0.22453 val_PR-AUC=0.91598 val_Recall@10%=0.11239\n",
      "epoch 26/30 train_loss=0.24630 val_loss=0.22399 val_PR-AUC=0.91593 val_Recall@10%=0.11227\n",
      "epoch 27/30 train_loss=0.24633 val_loss=0.22499 val_PR-AUC=0.91602 val_Recall@10%=0.11237\n",
      "epoch 28/30 train_loss=0.24626 val_loss=0.22449 val_PR-AUC=0.91604 val_Recall@10%=0.11249\n",
      "epoch 29/30 train_loss=0.24633 val_loss=0.22419 val_PR-AUC=0.91570 val_Recall@10%=0.11225\n",
      "epoch 30/30 train_loss=0.24629 val_loss=0.22376 val_PR-AUC=0.91598 val_Recall@10%=0.11240\n",
      "BEST epoch=8 val_Recall@10%=0.11251 val_PR-AUC=0.91595\n",
      "Evaluate on test (single run)\n",
      "PR-AUC: 0.9336106498459428\n",
      "Save artifacts\n",
      "saved keys: ['model', 'scaler', 'config', 'eval_dir', 'version_dir', 'figure_pr_curve', 'figure_confusion_matrix_top5', 'figure_confusion_matrix_top10', 'figure_confusion_matrix_top15', 'figure_confusion_matrix_top30']\n",
      "model -> /Users/jy/project_2nd/SKN23-2nd-3Team/models/dl/mlp_advanced/baseline/model.pt\n",
      "scaler -> /Users/jy/project_2nd/SKN23-2nd-3Team/models/preprocessing/mlp_advanced/baseline/scaler.pkl\n",
      "config -> /Users/jy/project_2nd/SKN23-2nd-3Team/models/configs/mlp_advanced/baseline/config.json\n",
      "eval_dir -> /Users/jy/project_2nd/SKN23-2nd-3Team/models/eval/dlmlp_advanced\n",
      "version_dir -> baseline\n",
      "figure_pr_curve -> /Users/jy/project_2nd/SKN23-2nd-3Team/assets/training/mlp_advanced/baseline/pr_curve.png\n",
      "figure_confusion_matrix_top5 -> /Users/jy/project_2nd/SKN23-2nd-3Team/assets/training/mlp_advanced/baseline/confusion_matrix_top5.png\n",
      "figure_confusion_matrix_top10 -> /Users/jy/project_2nd/SKN23-2nd-3Team/assets/training/mlp_advanced/baseline/confusion_matrix_top10.png\n",
      "figure_confusion_matrix_top15 -> /Users/jy/project_2nd/SKN23-2nd-3Team/assets/training/mlp_advanced/baseline/confusion_matrix_top15.png\n",
      "figure_confusion_matrix_top30 -> /Users/jy/project_2nd/SKN23-2nd-3Team/assets/training/mlp_advanced/baseline/confusion_matrix_top30.png\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_curve, confusion_matrix, average_precision_score\n",
    "\n",
    "from app.utils.paths import DEFAULT_PATHS as P, ensure_runtime_dirs\n",
    "from app.utils.metrics import evaluate_churn_metrics\n",
    "from app.utils.save import save_model_and_artifacts\n",
    "from models.model_definitions import MLP_advanced\n",
    "\n",
    "try:\n",
    "    from app.utils.plotting import configure_matplotlib_korean\n",
    "    configure_matplotlib_korean()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "# 0) Config\n",
    "\n",
    "ensure_runtime_dirs()\n",
    "\n",
    "SEED = 42\n",
    "BATCH_SIZE = 512\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "N_EPOCHS = 30\n",
    "\n",
    "# 운영/캠페인 기준 K (best 선택 기준)\n",
    "BEST_K_PCT = 10\n",
    "REPORT_K_LIST = (5, 10, 15, 30)\n",
    "\n",
    "# 불균형 보정 (필요하면 True)\n",
    "USE_POS_WEIGHT = True\n",
    "\n",
    "\n",
    "def seed_everything(seed: int = 42) -> None:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "\n",
    "# 1) Utility: Plot / TopK\n",
    "\n",
    "def plot_confusion_matrix_figure(y_true, y_pred, title: str, labels=(\"non_m2\", \"m2\")):\n",
    "    y_true_arr = np.asarray(y_true, dtype=int).reshape(-1)\n",
    "    y_pred_arr = np.asarray(y_pred, dtype=int).reshape(-1)\n",
    "    cm = confusion_matrix(y_true_arr, y_pred_arr)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    im = ax.imshow(cm, interpolation=\"nearest\", aspect=\"equal\", cmap=\"Blues\")\n",
    "    fig.colorbar(im, ax=ax)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"Actual\")\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_yticks([0, 1])\n",
    "    ax.set_xticklabels(list(labels))\n",
    "    ax.set_yticklabels(list(labels))\n",
    "\n",
    "    thresh = float(cm.max()) / 2.0 if cm.size else 0.0\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(\n",
    "                j, i, str(cm[i, j]),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if float(cm[i, j]) > thresh else \"black\",\n",
    "                fontsize=12,\n",
    "            )\n",
    "\n",
    "    ax.set_xlim(-0.5, cm.shape[1] - 0.5)\n",
    "    ax.set_ylim(cm.shape[0] - 0.5, -0.5)\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def topk_threshold(y_prob: np.ndarray, k_pct: int) -> float:\n",
    "    prob = np.asarray(y_prob, dtype=float).reshape(-1)\n",
    "    order = np.argsort(-prob)\n",
    "    n_sel = int(np.floor(len(prob) * (float(k_pct) / 100.0)))\n",
    "    n_sel = max(n_sel, 1)\n",
    "    return float(prob[order[n_sel - 1]])\n",
    "\n",
    "\n",
    "def plot_confusion_topk(y_true, y_prob, k_pct: int, labels=(\"non_m2\", \"m2\")):\n",
    "    thr = topk_threshold(np.asarray(y_prob, dtype=float), int(k_pct))\n",
    "    y_pred = (np.asarray(y_prob, dtype=float) >= thr).astype(int)\n",
    "    return plot_confusion_matrix_figure(\n",
    "        y_true, y_pred,\n",
    "        title=f\"Confusion Matrix (Top {int(k_pct)}%, thr={thr:.5f})\",\n",
    "        labels=labels,\n",
    "    )\n",
    "\n",
    "\n",
    "def recall_at_topk(y_true: np.ndarray, y_prob: np.ndarray, k_pct: int) -> float:\n",
    "    y_true = np.asarray(y_true).astype(int).reshape(-1)\n",
    "    y_prob = np.asarray(y_prob).astype(float).reshape(-1)\n",
    "\n",
    "    order = np.argsort(-y_prob)\n",
    "    n = max(int(np.floor(len(y_true) * (k_pct / 100.0))), 1)\n",
    "    top_idx = order[:n]\n",
    "\n",
    "    return float(y_true[top_idx].sum() / max(y_true.sum(), 1))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_probs(model: nn.Module, loader: DataLoader, device: str):\n",
    "    model.eval()\n",
    "    probs, trues = [], []\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device)\n",
    "        logits = model(xb).view(-1)\n",
    "        prob = torch.sigmoid(logits).detach().cpu().numpy().reshape(-1)\n",
    "        probs.append(prob)\n",
    "        trues.append(yb.detach().cpu().numpy().reshape(-1))\n",
    "    y_true = np.concatenate(trues).astype(int).reshape(-1)\n",
    "    y_prob = np.concatenate(probs).astype(float).reshape(-1)\n",
    "    return y_true, y_prob\n",
    "\n",
    "\n",
    "\n",
    "# 2) Load + Merge (anchors ⨝ features_ml_clean ⨝ labels)\n",
    "\n",
    "def load_merged_dataset():\n",
    "    print(\"Load parquet files\")\n",
    "\n",
    "    anchors = pd.read_parquet(P.parquet_path(\"anchors\"))\n",
    "    features = pd.read_parquet(P.parquet_path(\"features_ml_clean\"))  # 팀 규칙\n",
    "    labels = pd.read_parquet(P.parquet_path(\"labels\"))\n",
    "\n",
    "    for df in (anchors, features, labels):\n",
    "        df[\"user_id\"] = df[\"user_id\"].astype(str)\n",
    "\n",
    "    if \"split\" in anchors.columns:\n",
    "        anchors = anchors.drop(columns=[\"split\"])\n",
    "\n",
    "    need = [c for c in [\"user_id\", \"anchor_time\", \"label\", \"split\"] if c in labels.columns]\n",
    "    if \"split\" not in need:\n",
    "        raise KeyError(f\"labels.parquet에 split 컬럼이 없습니다. labels columns head: {list(labels.columns)[:50]}\")\n",
    "\n",
    "    data = anchors.merge(features, on=[\"user_id\", \"anchor_time\"], how=\"inner\")\n",
    "    data = data.merge(labels[need], on=[\"user_id\", \"anchor_time\"], how=\"inner\")\n",
    "\n",
    "    data[\"target\"] = data[\"label\"].astype(str).eq(\"m2\").astype(int)\n",
    "    split_col = data[\"split\"].astype(str)\n",
    "\n",
    "    feature_cols = [c for c in features.columns if c not in (\"user_id\", \"anchor_time\")]\n",
    "    X_all = data.loc[:, feature_cols].fillna(0.0)\n",
    "    y_all = data[\"target\"].to_numpy(dtype=int)\n",
    "\n",
    "    idx_train = split_col.eq(\"train\").to_numpy()\n",
    "    idx_val = split_col.eq(\"val\").to_numpy()\n",
    "    idx_test = split_col.eq(\"test\").to_numpy()\n",
    "\n",
    "    X_train = X_all.loc[idx_train].to_numpy(dtype=float)\n",
    "    y_train = y_all[idx_train]\n",
    "    X_val = X_all.loc[idx_val].to_numpy(dtype=float)\n",
    "    y_val = y_all[idx_val]\n",
    "    X_test = X_all.loc[idx_test].to_numpy(dtype=float)\n",
    "    y_test = y_all[idx_test]\n",
    "\n",
    "    print(\"rows:\", len(data), \"train/val/test:\", int(idx_train.sum()), int(idx_val.sum()), int(idx_test.sum()))\n",
    "    print(\"n_features:\", int(X_train.shape[1]))\n",
    "\n",
    "    return (X_train, y_train, X_val, y_val, X_test, y_test, feature_cols)\n",
    "\n",
    "\n",
    "\n",
    "# 3) Train (best by val Recall@K, tie-break PR-AUC)\n",
    "\n",
    "def train_with_best_selection(\n",
    "    X_train: np.ndarray, y_train: np.ndarray,\n",
    "    X_val: np.ndarray, y_val: np.ndarray,\n",
    "    input_dim: int,\n",
    "):\n",
    "    def get_device() -> str:\n",
    "        if torch.cuda.is_available():\n",
    "            return \"cuda\"\n",
    "        if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "            return \"mps\"\n",
    "        return \"cpu\"\n",
    "\n",
    "    device = get_device()\n",
    "    print(\"device:\", device)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train)\n",
    "    X_val_s = scaler.transform(X_val)\n",
    "\n",
    "    train_ds = TensorDataset(torch.tensor(X_train_s, dtype=torch.float32),\n",
    "                             torch.tensor(y_train, dtype=torch.float32))\n",
    "    val_ds = TensorDataset(torch.tensor(X_val_s, dtype=torch.float32),\n",
    "                           torch.tensor(y_val, dtype=torch.float32))\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = MLP_advanced(input_dim)\n",
    "    model.to(device)\n",
    "\n",
    "    # --- loss (pos_weight 옵션) ---\n",
    "    if USE_POS_WEIGHT:\n",
    "        pos = int((y_train == 1).sum())\n",
    "        neg = int((y_train == 0).sum())\n",
    "        pw = torch.tensor([neg / max(pos, 1)], dtype=torch.float32).to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pw)\n",
    "        print(f\"pos_weight enabled: pos={pos} neg={neg} pos_weight={float(pw.item()):.4f}\")\n",
    "    else:\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    def run_epoch(loader, train_mode: bool) -> float:\n",
    "        model.train() if train_mode else model.eval()\n",
    "        loss_sum, n_sum = 0.0, 0\n",
    "        with torch.set_grad_enabled(train_mode):\n",
    "            for xb, yb in loader:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device).view(-1)\n",
    "\n",
    "                if train_mode:\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                logits = model(xb).view(-1)\n",
    "                loss = criterion(logits, yb)\n",
    "\n",
    "                if train_mode:\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                bs = int(xb.shape[0])\n",
    "                loss_sum += float(loss.item()) * bs\n",
    "                n_sum += bs\n",
    "\n",
    "        return float(loss_sum / max(n_sum, 1))\n",
    "\n",
    "    best = {\n",
    "        \"epoch\": 0,\n",
    "        \"recall_at_k\": -1.0,\n",
    "        \"pr_auc\": -1.0,\n",
    "        \"state_dict\": None,\n",
    "    }\n",
    "\n",
    "    print(\"Train MLP_advanced (best by val Recall@{}%)\".format(BEST_K_PCT))\n",
    "\n",
    "    for epoch in range(1, N_EPOCHS + 1):\n",
    "        tr_loss = run_epoch(train_loader, True)\n",
    "        va_loss = run_epoch(val_loader, False)\n",
    "\n",
    "        # val metric (prob)\n",
    "        yv_true, yv_prob = predict_probs(model, val_loader, device)\n",
    "        val_pr_auc = float(average_precision_score(yv_true, yv_prob))\n",
    "        val_recall_k = recall_at_topk(yv_true, yv_prob, BEST_K_PCT)\n",
    "\n",
    "        print(\n",
    "            f\"epoch {epoch}/{N_EPOCHS} \"\n",
    "            f\"train_loss={tr_loss:.5f} val_loss={va_loss:.5f} \"\n",
    "            f\"val_PR-AUC={val_pr_auc:.5f} val_Recall@{BEST_K_PCT}%={val_recall_k:.5f}\"\n",
    "        )\n",
    "\n",
    "        # best update: Recall@K, tie-break PR-AUC\n",
    "        better = (val_recall_k > best[\"recall_at_k\"] + 1e-12) or (\n",
    "            abs(val_recall_k - best[\"recall_at_k\"]) <= 1e-12 and val_pr_auc > best[\"pr_auc\"]\n",
    "        )\n",
    "        if better:\n",
    "            best[\"epoch\"] = epoch\n",
    "            best[\"recall_at_k\"] = val_recall_k\n",
    "            best[\"pr_auc\"] = val_pr_auc\n",
    "            best[\"state_dict\"] = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    # restore best\n",
    "    if best[\"state_dict\"] is not None:\n",
    "        model.load_state_dict(best[\"state_dict\"])\n",
    "    print(f\"BEST epoch={best['epoch']} val_Recall@{BEST_K_PCT}%={best['recall_at_k']:.5f} val_PR-AUC={best['pr_auc']:.5f}\")\n",
    "\n",
    "    return model, scaler, device\n",
    "\n",
    "\n",
    "\n",
    "# 4) Test (single run) + Save artifacts\n",
    "\n",
    "def evaluate_and_save(\n",
    "    model: nn.Module,\n",
    "    scaler: StandardScaler,\n",
    "    device: str,\n",
    "    X_test: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "):\n",
    "    print(\"Evaluate on test (single run)\")\n",
    "\n",
    "    X_test_s = scaler.transform(X_test)\n",
    "    test_ds = TensorDataset(torch.tensor(X_test_s, dtype=torch.float32),\n",
    "                            torch.tensor(y_test, dtype=torch.float32))\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    y_true, y_prob = predict_probs(model, test_loader, device)\n",
    "\n",
    "    metrics = evaluate_churn_metrics(y_true, y_prob)\n",
    "    print(\"PR-AUC:\", float(metrics.get(\"PR-AUC (Average Precision)\", 0.0)))\n",
    "\n",
    "    # PR curve fig\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
    "    pr_auc_val = metrics.get(\"PR-AUC (Average Precision)\")\n",
    "    pr_auc_val = float(average_precision_score(y_true, y_prob)) if pr_auc_val is None else float(pr_auc_val)\n",
    "\n",
    "    fig_pr, ax_pr = plt.subplots(figsize=(6, 5))\n",
    "    ax_pr.plot(recall, precision, lw=2, label=f\"PR-AUC={pr_auc_val:.5f}\")\n",
    "    ax_pr.set_xlabel(\"Recall\")\n",
    "    ax_pr.set_ylabel(\"Precision\")\n",
    "    ax_pr.set_title(\"Precision-Recall Curve\")\n",
    "    ax_pr.grid(alpha=0.3)\n",
    "    ax_pr.legend()\n",
    "    fig_pr.tight_layout()\n",
    "\n",
    "    figures = {\"pr_curve\": fig_pr}\n",
    "    for k_pct in REPORT_K_LIST:\n",
    "        figures[f\"confusion_matrix_top{k_pct}\"] = plot_confusion_topk(\n",
    "            y_true=y_true, y_prob=y_prob, k_pct=int(k_pct), labels=(\"non_m2\", \"m2\")\n",
    "        )\n",
    "\n",
    "    print(\"Save artifacts\")\n",
    "\n",
    "    saved = save_model_and_artifacts(\n",
    "        model=model,\n",
    "        model_name=\"mlp_advanced\",\n",
    "        model_type=\"dl\",\n",
    "        model_id=\"dl__mlp_advanced\",\n",
    "        split=\"test\",\n",
    "        metrics=metrics,\n",
    "        y_true=y_true,\n",
    "        y_prob=y_prob,\n",
    "        version=\"baseline\",\n",
    "        scaler=scaler,\n",
    "        figures=figures,\n",
    "        config={\n",
    "            \"model_name\": \"mlp_advanced\",\n",
    "            \"model_type\": \"dl\",\n",
    "            \"version\": \"baseline\",\n",
    "            \"feature_source\": \"features_ml_clean.parquet\",\n",
    "            \"best_selection\": f\"val Recall@{BEST_K_PCT}%\",\n",
    "            \"use_pos_weight\": bool(USE_POS_WEIGHT),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # close figs\n",
    "    plt.close(fig_pr)\n",
    "    for k_pct in REPORT_K_LIST:\n",
    "        plt.close(figures[f\"confusion_matrix_top{k_pct}\"])\n",
    "\n",
    "    print(\"saved keys:\", list(saved.keys()))\n",
    "    for k, v in saved.items():\n",
    "        print(k, \"->\", v)\n",
    "\n",
    "    print(\"Done\")\n",
    "\n",
    "\n",
    "# 5) Main\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    seed_everything(SEED)\n",
    "\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, feature_cols = load_merged_dataset()\n",
    "    input_dim = int(X_train.shape[1])\n",
    "\n",
    "    model, scaler, device = train_with_best_selection(\n",
    "        X_train=X_train, y_train=y_train,\n",
    "        X_val=X_val, y_val=y_val,\n",
    "        input_dim=input_dim,\n",
    "    )\n",
    "\n",
    "    evaluate_and_save(\n",
    "        model=model,\n",
    "        scaler=scaler,\n",
    "        device=device,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868642bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "churn_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
