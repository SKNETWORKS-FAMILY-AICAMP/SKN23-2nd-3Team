{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12441486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 461642, number of negative: 112450\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013629 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2495\n",
      "[LightGBM] [Info] Number of data points in the train set: 574092, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.804125 -> initscore=1.412281\n",
      "[LightGBM] [Info] Start training from score 1.412281\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[40]\tvalid_0's binary_logloss: 0.430834\n",
      "Evaluated only: binary_logloss\n",
      "[trial 01] val_pr_auc=0.914389 | best=0.914389\n",
      "[LightGBM] [Info] Number of positive: 461642, number of negative: 112450\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013519 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2495\n",
      "[LightGBM] [Info] Number of data points in the train set: 574092, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.804125 -> initscore=1.412281\n",
      "[LightGBM] [Info] Start training from score 1.412281\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid_0's binary_logloss: 0.431687\n",
      "Evaluated only: binary_logloss\n",
      "[trial 02] val_pr_auc=0.909475 | best=0.914389\n",
      "[LightGBM] [Info] Number of positive: 461642, number of negative: 112450\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027588 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2495\n",
      "[LightGBM] [Info] Number of data points in the train set: 574092, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.804125 -> initscore=1.412281\n",
      "[LightGBM] [Info] Start training from score 1.412281\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.440188\n",
      "Evaluated only: binary_logloss\n",
      "[trial 03] val_pr_auc=0.909640 | best=0.914389\n",
      "[LightGBM] [Info] Number of positive: 461642, number of negative: 112450\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014162 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2495\n",
      "[LightGBM] [Info] Number of data points in the train set: 574092, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.804125 -> initscore=1.412281\n",
      "[LightGBM] [Info] Start training from score 1.412281\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[35]\tvalid_0's binary_logloss: 0.403678\n",
      "Evaluated only: binary_logloss\n",
      "[trial 04] val_pr_auc=0.916031 | best=0.916031\n",
      "[LightGBM] [Info] Number of positive: 461642, number of negative: 112450\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016000 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2495\n",
      "[LightGBM] [Info] Number of data points in the train set: 574092, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.804125 -> initscore=1.412281\n",
      "[LightGBM] [Info] Start training from score 1.412281\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[40]\tvalid_0's binary_logloss: 0.431043\n",
      "Evaluated only: binary_logloss\n",
      "[trial 05] val_pr_auc=0.912303 | best=0.916031\n",
      "[LightGBM] [Info] Number of positive: 461642, number of negative: 112450\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016769 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2495\n",
      "[LightGBM] [Info] Number of data points in the train set: 574092, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.804125 -> initscore=1.412281\n",
      "[LightGBM] [Info] Start training from score 1.412281\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's binary_logloss: 0.440225\n",
      "Evaluated only: binary_logloss\n",
      "[trial 06] val_pr_auc=0.915335 | best=0.916031\n",
      "[LightGBM] [Info] Number of positive: 461642, number of negative: 112450\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013329 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2495\n",
      "[LightGBM] [Info] Number of data points in the train set: 574092, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.804125 -> initscore=1.412281\n",
      "[LightGBM] [Info] Start training from score 1.412281\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[97]\tvalid_0's binary_logloss: 0.395995\n",
      "Evaluated only: binary_logloss\n",
      "[trial 07] val_pr_auc=0.916279 | best=0.916279\n",
      "[LightGBM] [Info] Number of positive: 461642, number of negative: 112450\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013437 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2495\n",
      "[LightGBM] [Info] Number of data points in the train set: 574092, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.804125 -> initscore=1.412281\n",
      "[LightGBM] [Info] Start training from score 1.412281\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.440188\n",
      "Evaluated only: binary_logloss\n",
      "[trial 08] val_pr_auc=0.909640 | best=0.916279\n",
      "[LightGBM] [Info] Number of positive: 461642, number of negative: 112450\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015101 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2495\n",
      "[LightGBM] [Info] Number of data points in the train set: 574092, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.804125 -> initscore=1.412281\n",
      "[LightGBM] [Info] Start training from score 1.412281\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[255]\tvalid_0's binary_logloss: 0.395552\n",
      "Evaluated only: binary_logloss\n",
      "[trial 09] val_pr_auc=0.916886 | best=0.916886\n",
      "[LightGBM] [Info] Number of positive: 461642, number of negative: 112450\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016450 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2495\n",
      "[LightGBM] [Info] Number of data points in the train set: 574092, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.804125 -> initscore=1.412281\n",
      "[LightGBM] [Info] Start training from score 1.412281\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's binary_logloss: 0.431818\n",
      "Evaluated only: binary_logloss\n",
      "[trial 10] val_pr_auc=0.909856 | best=0.916886\n",
      "[LightGBM] [Info] Number of positive: 461642, number of negative: 112450\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015655 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2495\n",
      "[LightGBM] [Info] Number of data points in the train set: 574092, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.804125 -> initscore=1.412281\n",
      "[LightGBM] [Info] Start training from score 1.412281\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[23]\tvalid_0's binary_logloss: 0.404058\n",
      "Evaluated only: binary_logloss\n",
      "[trial 11] val_pr_auc=0.916180 | best=0.916886\n",
      "[LightGBM] [Info] Number of positive: 461642, number of negative: 112450\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014648 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2495\n",
      "[LightGBM] [Info] Number of data points in the train set: 574092, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.804125 -> initscore=1.412281\n",
      "[LightGBM] [Info] Start training from score 1.412281\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[32]\tvalid_0's binary_logloss: 0.403422\n",
      "Evaluated only: binary_logloss\n",
      "[trial 12] val_pr_auc=0.915672 | best=0.916886\n",
      "[LightGBM] [Info] Number of positive: 461642, number of negative: 112450\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014479 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2495\n",
      "[LightGBM] [Info] Number of data points in the train set: 574092, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.804125 -> initscore=1.412281\n",
      "[LightGBM] [Info] Start training from score 1.412281\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[257]\tvalid_0's binary_logloss: 0.395974\n",
      "Evaluated only: binary_logloss\n",
      "[trial 13] val_pr_auc=0.916320 | best=0.916886\n",
      "[LightGBM] [Info] Number of positive: 461642, number of negative: 112450\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014967 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2495\n",
      "[LightGBM] [Info] Number of data points in the train set: 574092, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.804125 -> initscore=1.412281\n",
      "[LightGBM] [Info] Start training from score 1.412281\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's binary_logloss: 0.430871\n",
      "Evaluated only: binary_logloss\n",
      "[trial 14] val_pr_auc=0.913460 | best=0.916886\n",
      "[LightGBM] [Info] Number of positive: 461642, number of negative: 112450\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012990 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2495\n",
      "[LightGBM] [Info] Number of data points in the train set: 574092, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.804125 -> initscore=1.412281\n",
      "[LightGBM] [Info] Start training from score 1.412281\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[243]\tvalid_0's binary_logloss: 0.395746\n",
      "Evaluated only: binary_logloss\n",
      "[trial 15] val_pr_auc=0.916819 | best=0.916886\n",
      "[LightGBM] [Info] Number of positive: 461642, number of negative: 112450\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.086559 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2495\n",
      "[LightGBM] [Info] Number of data points in the train set: 574092, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.804125 -> initscore=1.412281\n",
      "[LightGBM] [Info] Start training from score 1.412281\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's binary_logloss: 0.430684\n",
      "Evaluated only: binary_logloss\n",
      "[trial 16] val_pr_auc=0.911467 | best=0.916886\n",
      "[LightGBM] [Info] Number of positive: 461642, number of negative: 112450\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013199 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2495\n",
      "[LightGBM] [Info] Number of data points in the train set: 574092, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.804125 -> initscore=1.412281\n",
      "[LightGBM] [Info] Start training from score 1.412281\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[61]\tvalid_0's binary_logloss: 0.404018\n",
      "Evaluated only: binary_logloss\n",
      "[trial 17] val_pr_auc=0.915856 | best=0.916886\n",
      "[LightGBM] [Info] Number of positive: 461642, number of negative: 112450\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013519 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2495\n",
      "[LightGBM] [Info] Number of data points in the train set: 574092, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.804125 -> initscore=1.412281\n",
      "[LightGBM] [Info] Start training from score 1.412281\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's binary_logloss: 0.425337\n",
      "Evaluated only: binary_logloss\n",
      "[trial 18] val_pr_auc=0.909347 | best=0.916886\n",
      "[LightGBM] [Info] Number of positive: 461642, number of negative: 112450\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012921 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2495\n",
      "[LightGBM] [Info] Number of data points in the train set: 574092, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.804125 -> initscore=1.412281\n",
      "[LightGBM] [Info] Start training from score 1.412281\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[23]\tvalid_0's binary_logloss: 0.403476\n",
      "Evaluated only: binary_logloss\n",
      "[trial 19] val_pr_auc=0.915646 | best=0.916886\n",
      "[LightGBM] [Info] Number of positive: 461642, number of negative: 112450\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013418 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2495\n",
      "[LightGBM] [Info] Number of data points in the train set: 574092, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.804125 -> initscore=1.412281\n",
      "[LightGBM] [Info] Start training from score 1.412281\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.431515\n",
      "Evaluated only: binary_logloss\n",
      "[trial 20] val_pr_auc=0.912272 | best=0.916886\n",
      "[LightGBM] [Info] Number of positive: 461642, number of negative: 112450\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013738 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2495\n",
      "[LightGBM] [Info] Number of data points in the train set: 574092, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.804125 -> initscore=1.412281\n",
      "[LightGBM] [Info] Start training from score 1.412281\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[76]\tvalid_0's binary_logloss: 0.395995\n",
      "Evaluated only: binary_logloss\n",
      "[trial 21] val_pr_auc=0.916226 | best=0.916886\n",
      "[LightGBM] [Info] Number of positive: 461642, number of negative: 112450\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013068 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2495\n",
      "[LightGBM] [Info] Number of data points in the train set: 574092, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.804125 -> initscore=1.412281\n",
      "[LightGBM] [Info] Start training from score 1.412281\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[30]\tvalid_0's binary_logloss: 0.425298\n",
      "Evaluated only: binary_logloss\n",
      "[trial 22] val_pr_auc=0.909436 | best=0.916886\n",
      "[LightGBM] [Info] Number of positive: 461642, number of negative: 112450\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013994 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2495\n",
      "[LightGBM] [Info] Number of data points in the train set: 574092, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.804125 -> initscore=1.412281\n",
      "[LightGBM] [Info] Start training from score 1.412281\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's binary_logloss: 0.431654\n",
      "Evaluated only: binary_logloss\n",
      "[trial 23] val_pr_auc=0.907143 | best=0.916886\n",
      "[LightGBM] [Info] Number of positive: 461642, number of negative: 112450\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013975 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2495\n",
      "[LightGBM] [Info] Number of data points in the train set: 574092, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.804125 -> initscore=1.412281\n",
      "[LightGBM] [Info] Start training from score 1.412281\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's binary_logloss: 0.431108\n",
      "Evaluated only: binary_logloss\n",
      "[trial 24] val_pr_auc=0.913383 | best=0.916886\n",
      "[LightGBM] [Info] Number of positive: 461642, number of negative: 112450\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014034 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2495\n",
      "[LightGBM] [Info] Number of data points in the train set: 574092, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.804125 -> initscore=1.412281\n",
      "[LightGBM] [Info] Start training from score 1.412281\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.439931\n",
      "Evaluated only: binary_logloss\n",
      "[trial 25] val_pr_auc=0.912007 | best=0.916886\n",
      "[LightGBM] [Info] Number of positive: 461642, number of negative: 112450\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013848 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2495\n",
      "[LightGBM] [Info] Number of data points in the train set: 574092, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.804125 -> initscore=1.412281\n",
      "[LightGBM] [Info] Start training from score 1.412281\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.43987\n",
      "Evaluated only: binary_logloss\n",
      "[trial 26] val_pr_auc=0.914553 | best=0.916886\n",
      "[LightGBM] [Info] Number of positive: 461642, number of negative: 112450\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012819 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2495\n",
      "[LightGBM] [Info] Number of data points in the train set: 574092, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.804125 -> initscore=1.412281\n",
      "[LightGBM] [Info] Start training from score 1.412281\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[87]\tvalid_0's binary_logloss: 0.403577\n",
      "Evaluated only: binary_logloss\n",
      "[trial 27] val_pr_auc=0.915849 | best=0.916886\n",
      "[LightGBM] [Info] Number of positive: 461642, number of negative: 112450\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014920 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2495\n",
      "[LightGBM] [Info] Number of data points in the train set: 574092, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.804125 -> initscore=1.412281\n",
      "[LightGBM] [Info] Start training from score 1.412281\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[190]\tvalid_0's binary_logloss: 0.405756\n",
      "Evaluated only: binary_logloss\n",
      "[trial 28] val_pr_auc=0.914973 | best=0.916886\n",
      "[LightGBM] [Info] Number of positive: 461642, number of negative: 112450\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015245 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2495\n",
      "[LightGBM] [Info] Number of data points in the train set: 574092, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.804125 -> initscore=1.412281\n",
      "[LightGBM] [Info] Start training from score 1.412281\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.439893\n",
      "Evaluated only: binary_logloss\n",
      "[trial 29] val_pr_auc=0.913453 | best=0.916886\n",
      "[LightGBM] [Info] Number of positive: 461642, number of negative: 112450\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015831 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2495\n",
      "[LightGBM] [Info] Number of data points in the train set: 574092, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.804125 -> initscore=1.412281\n",
      "[LightGBM] [Info] Start training from score 1.412281\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[40]\tvalid_0's binary_logloss: 0.430599\n",
      "Evaluated only: binary_logloss\n",
      "[trial 30] val_pr_auc=0.913242 | best=0.916886\n",
      "BEST: {'val_pr_auc': 0.916886450548324, 'params': {'learning_rate': 0.03, 'num_leaves': 31, 'min_child_samples': 100, 'subsample': 0.8, 'colsample_bytree': 0.7, 'reg_alpha': 0.0001, 'reg_lambda': 0.01, 'scale_pos_weight': 1.0}, 'best_iter': 255}\n",
      "[LightGBM] [Info] Number of positive: 577466, number of negative: 134241\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017851 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2493\n",
      "[LightGBM] [Info] Number of data points in the train set: 711707, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.811382 -> initscore=1.459013\n",
      "[LightGBM] [Info] Start training from score 1.459013\n",
      "\n",
      "=== metrics ===\n",
      "{\n",
      "  \"PR-AUC (Average Precision)\": 0.9334148767041387,\n",
      "  \"base_rate\": 0.8671255879724648,\n",
      "  \"n_total\": 101833,\n",
      "  \"ranking\": [\n",
      "    {\n",
      "      \"Top_K\": \"5%\",\n",
      "      \"n_selected\": 5091,\n",
      "      \"Precision\": 0.9673934394028678,\n",
      "      \"Recall\": 0.055774501143801954,\n",
      "      \"Lift\": 1.1156324445053594\n",
      "    },\n",
      "    {\n",
      "      \"Top_K\": \"10%\",\n",
      "      \"n_selected\": 10183,\n",
      "      \"Precision\": 0.9630757144260041,\n",
      "      \"Recall\": 0.11106203709995244,\n",
      "      \"Lift\": 1.1106530908376173\n",
      "    },\n",
      "    {\n",
      "      \"Top_K\": \"15%\",\n",
      "      \"n_selected\": 15274,\n",
      "      \"Precision\": 0.9589498494173104,\n",
      "      \"Recall\": 0.1658739326402573,\n",
      "      \"Lift\": 1.105894996893762\n",
      "    },\n",
      "    {\n",
      "      \"Top_K\": \"30%\",\n",
      "      \"n_selected\": 30549,\n",
      "      \"Precision\": 0.9472323152967363,\n",
      "      \"Recall\": 0.3277049217458268,\n",
      "      \"Lift\": 1.0923819207222096\n",
      "    }\n",
      "  ],\n",
      "  \"score_for_selection\": 0.6431672945415667\n",
      "}\n",
      "\n",
      "=== saved paths ===\n",
      "model: /Users/jy/project_2nd/SKN23-2nd-3Team/models/ml/lgbm/v1_tuned/model.pkl\n",
      "metrics: /Users/jy/project_2nd/SKN23-2nd-3Team/models/metrics/lgbm/v1_tuned/metrics.json\n",
      "figure_pr_curve: /Users/jy/project_2nd/SKN23-2nd-3Team/assets/training/lgbm/v1_tuned/pr_curve.png\n",
      "figure_confusion_matrix_top5: /Users/jy/project_2nd/SKN23-2nd-3Team/assets/training/lgbm/v1_tuned/confusion_matrix_top5.png\n",
      "figure_confusion_matrix_top10: /Users/jy/project_2nd/SKN23-2nd-3Team/assets/training/lgbm/v1_tuned/confusion_matrix_top10.png\n",
      "figure_confusion_matrix_top15: /Users/jy/project_2nd/SKN23-2nd-3Team/assets/training/lgbm/v1_tuned/confusion_matrix_top15.png\n",
      "figure_confusion_matrix_top30: /Users/jy/project_2nd/SKN23-2nd-3Team/assets/training/lgbm/v1_tuned/confusion_matrix_top30.png\n",
      "config: /Users/jy/project_2nd/SKN23-2nd-3Team/models/configs/lgbm/v1_tuned/config.json\n",
      "eval_dir: /Users/jy/project_2nd/SKN23-2nd-3Team/models/eval/mllgbm\n",
      "\n",
      "streamlit percentiles: /Users/jy/project_2nd/SKN23-2nd-3Team/models/metrics/lgbm_score_percentiles.json\n",
      "tuning: /Users/jy/project_2nd/SKN23-2nd-3Team/models/metrics/lgbm/v1_tuned/tuning.json\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve,\n",
    "    average_precision_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "PROJECT_ROOT = Path(\"/Users/jy/project_2nd/SKN23-2nd-3Team\")\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from app.utils.save import save_model_and_artifacts\n",
    "from app.utils.paths import PATHS\n",
    "\n",
    "# -------------------------\n",
    "# (선택) 한글 폰트\n",
    "# -------------------------\n",
    "try:\n",
    "    from app.utils.plotting import configure_matplotlib_korean\n",
    "    configure_matplotlib_korean()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "# JSON 저장 안전 변환 (np.int64 등 -> python int/float/bool)\n",
    "\n",
    "def to_py(obj):\n",
    "    if isinstance(obj, (np.integer,)):\n",
    "        return int(obj)\n",
    "    if isinstance(obj, (np.floating,)):\n",
    "        return float(obj)\n",
    "    if isinstance(obj, (np.bool_,)):\n",
    "        return bool(obj)\n",
    "    return obj\n",
    "\n",
    "def dict_to_py(d: dict) -> dict:\n",
    "    return {str(k): to_py(v) for k, v in d.items()}\n",
    "\n",
    "\n",
    "\n",
    "# Streamlit percentile 파일 규칙 (app 코드 기준)\n",
    "# models/metrics/{model_name}_score_percentiles.json\n",
    "\n",
    "def write_streamlit_score_percentiles(model_name: str, payload: dict) -> str:\n",
    "    out = PROJECT_ROOT / \"models\" / \"metrics\" / f\"{model_name}_score_percentiles.json\"\n",
    "    out.parent.mkdir(parents=True, exist_ok=True)\n",
    "    out.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    return str(out)\n",
    "\n",
    "def score_percentiles_payload(model_id: str, split: str, y_prob, pcts=(1, 5, 10, 20, 30, 50)):\n",
    "    y_prob = np.asarray(y_prob, dtype=float).reshape(-1)\n",
    "    percentiles = [{\"pct\": int(p), \"score\": float(np.quantile(y_prob, 1.0 - p / 100.0))} for p in pcts]\n",
    "    return {\"model_id\": model_id, \"split\": split, \"percentiles\": percentiles}\n",
    "\n",
    "\n",
    "\n",
    "# Ranking metrics + selection score (빠짐없이)\n",
    "\n",
    "def build_ranking_metrics(y_true, y_prob, k_list=(5, 10, 15, 30)):\n",
    "    y_true = np.asarray(y_true).astype(int).reshape(-1)\n",
    "    y_prob = np.asarray(y_prob).astype(float).reshape(-1)\n",
    "\n",
    "    pr_auc = float(average_precision_score(y_true, y_prob))\n",
    "\n",
    "    df_rank = pd.DataFrame({\"y\": y_true, \"score\": y_prob}).sort_values(\"score\", ascending=False)\n",
    "    base_rate = float(df_rank[\"y\"].mean())\n",
    "    total_pos = float(df_rank[\"y\"].sum())\n",
    "    n_total = int(len(df_rank))\n",
    "\n",
    "    ranking = []\n",
    "    for k in k_list:\n",
    "        n_sel = max(int(np.floor(n_total * k / 100)), 1)\n",
    "        selected = df_rank.iloc[:n_sel]\n",
    "\n",
    "        precision_k = float(selected[\"y\"].mean())\n",
    "        recall_k = float(selected[\"y\"].sum() / (total_pos + 1e-12))\n",
    "        lift_k = float(precision_k / base_rate) if base_rate > 0 else 0.0\n",
    "\n",
    "        ranking.append({\n",
    "            \"Top_K\": f\"{k}%\",\n",
    "            \"n_selected\": int(n_sel),\n",
    "            \"Precision\": precision_k,\n",
    "            \"Recall\": recall_k,\n",
    "            \"Lift\": lift_k,\n",
    "        })\n",
    "\n",
    "    # selection score (네가 쓰던 가중치 그대로)\n",
    "    def _get(k_pct: int):\n",
    "        target = f\"{k_pct}%\"\n",
    "        for row in ranking:\n",
    "            if row[\"Top_K\"] == target:\n",
    "                return row\n",
    "        return {\"Recall\": 0.0, \"Lift\": 0.0}\n",
    "\n",
    "    r5  = _get(5)\n",
    "    r10 = _get(10)\n",
    "    r30 = _get(30)\n",
    "\n",
    "    selection_score = (\n",
    "        0.55 * pr_auc\n",
    "        + 0.20 * float(r10[\"Recall\"])\n",
    "        + 0.15 * float(r30[\"Recall\"])\n",
    "        + 0.05 * float(r5[\"Recall\"])\n",
    "        + 0.03 * float(r10[\"Lift\"])\n",
    "        + 0.02 * float(r5[\"Lift\"])\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"PR-AUC (Average Precision)\": pr_auc,\n",
    "        \"base_rate\": base_rate,\n",
    "        \"n_total\": n_total,\n",
    "        \"ranking\": ranking,\n",
    "        \"score_for_selection\": float(selection_score),\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# Figures (PR curve + confusion matrices by TopK)\n",
    "\n",
    "def threshold_topk(y_prob, k_pct: int) -> float:\n",
    "    y_prob = np.asarray(y_prob, dtype=float).reshape(-1)\n",
    "    order = np.argsort(-y_prob)\n",
    "    n_sel = max(int(np.floor(len(y_prob) * k_pct / 100)), 1)\n",
    "    return float(y_prob[order[n_sel - 1]])\n",
    "def plot_confusion_matrix(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    title,\n",
    "    labels=(\"비이탈(m1)\", \"이탈(m2)\"),\n",
    "    cmap=\"Blues\",  # ✅ 기본값 Blues로 고정\n",
    "):\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_pred = np.asarray(y_pred).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    im = ax.imshow(cm, cmap=cmap, interpolation=\"nearest\", aspect=\"equal\")  # ✅ cmap 적용\n",
    "    fig.colorbar(im, ax=ax)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Predicted (예측값)\")\n",
    "    ax.set_ylabel(\"Actual (실제값)\")\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_yticks([0, 1])\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_yticklabels(labels)\n",
    "\n",
    "    thresh = cm.max() / 2.0 if cm.size else 0\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(\n",
    "                j, i, f\"{cm[i, j]}\",\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\"\n",
    "            )\n",
    "\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def make_figures(test_true, test_prob, k_list=(5, 10, 15, 30), cm_cmap=\"Blues\"):\n",
    "    precision, recall, _ = precision_recall_curve(test_true, test_prob)\n",
    "    pr_auc_val = float(average_precision_score(test_true, test_prob))\n",
    "\n",
    "    fig_pr, ax_pr = plt.subplots(figsize=(6, 5))\n",
    "    ax_pr.plot(recall, precision, lw=2, label=f\"PR-AUC = {pr_auc_val:.5f}\")\n",
    "    ax_pr.set_xlabel(\"Recall\")\n",
    "    ax_pr.set_ylabel(\"Precision\")\n",
    "    ax_pr.set_title(\"Precision-Recall Curve\")\n",
    "    ax_pr.legend()\n",
    "    ax_pr.grid(alpha=0.3)\n",
    "    fig_pr.tight_layout()\n",
    "\n",
    "    figures = {\"pr_curve\": fig_pr}\n",
    "    for k in k_list:\n",
    "        thr = threshold_topk(test_prob, k)\n",
    "        y_pred_k = (np.asarray(test_prob) >= thr).astype(int)\n",
    "\n",
    "        figures[f\"confusion_matrix_top{k}\"] = plot_confusion_matrix(\n",
    "            test_true,\n",
    "            y_pred_k,\n",
    "            title=f\"Confusion Matrix (Top {k}%, thr={thr:.5f})\",\n",
    "            cmap=cm_cmap,  # ✅ 여기서도 Blues 고정\n",
    "        )\n",
    "\n",
    "    return figures\n",
    "\n",
    "\n",
    "\n",
    "# 데이터 로드/병합 (팀 규칙: labels.parquet split 그대로)\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "features = pd.read_parquet(DATA_DIR / \"features_ml_clean.parquet\")\n",
    "labels   = pd.read_parquet(DATA_DIR / \"labels.parquet\")\n",
    "\n",
    "features[\"user_id\"] = features[\"user_id\"].astype(str)\n",
    "labels[\"user_id\"]   = labels[\"user_id\"].astype(str)\n",
    "\n",
    "df = features.merge(\n",
    "    labels[[\"user_id\", \"anchor_time\", \"label\", \"split\"]],\n",
    "    on=[\"user_id\", \"anchor_time\"],\n",
    "    how=\"inner\",\n",
    "    validate=\"one_to_one\",\n",
    ")\n",
    "df[\"y\"] = (df[\"label\"] == \"m2\").astype(int)\n",
    "\n",
    "feature_cols = [c for c in df.columns if c not in [\"user_id\", \"anchor_time\", \"label\", \"split\", \"y\"]]\n",
    "\n",
    "train_df = df[df[\"split\"] == \"train\"]\n",
    "val_df   = df[df[\"split\"] == \"val\"]\n",
    "test_df  = df[df[\"split\"] == \"test\"]\n",
    "\n",
    "X_train, y_train = train_df[feature_cols], train_df[\"y\"].to_numpy()\n",
    "X_val,   y_val   = val_df[feature_cols],   val_df[\"y\"].to_numpy()\n",
    "X_test,  y_test  = test_df[feature_cols],  test_df[\"y\"].to_numpy()\n",
    "\n",
    "X_tv = pd.concat([X_train, X_val], axis=0)\n",
    "y_tv = np.concatenate([y_train, y_val])\n",
    "\n",
    "neg = float((y_train == 0).sum())\n",
    "pos = float((y_train == 1).sum())\n",
    "ratio = neg / (pos + 1e-12)\n",
    "\n",
    "\n",
    "\n",
    "# LightGBM 튜닝 (VAL PR-AUC) + early stopping\n",
    "\n",
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "\n",
    "def tune_lgbm(n_trials=30, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    space = {\n",
    "        \"learning_rate\":      [0.01, 0.02, 0.03, 0.05, 0.07],\n",
    "        \"num_leaves\":         [15, 31, 63, 127, 255],\n",
    "        \"min_child_samples\":  [10, 20, 50, 100],\n",
    "        \"subsample\":          [0.7, 0.8, 0.9, 1.0],\n",
    "        \"colsample_bytree\":   [0.7, 0.8, 0.9, 1.0],\n",
    "        \"reg_alpha\":          [0.0, 1e-4, 1e-3, 1e-2],\n",
    "        \"reg_lambda\":         [0.0, 1e-4, 1e-3, 1e-2],\n",
    "        \"scale_pos_weight\":   [1.0, 2.0, 5.0, 10.0, ratio],  # 불균형 보정\n",
    "    }\n",
    "\n",
    "    best = {\"val_pr_auc\": -1.0, \"params\": None, \"best_iter\": None}\n",
    "\n",
    "    for t in range(n_trials):\n",
    "        params = {k: rng.choice(v) for k, v in space.items()}\n",
    "\n",
    "        model = LGBMClassifier(\n",
    "            objective=\"binary\",\n",
    "            boosting_type=\"gbdt\",\n",
    "            n_estimators=5000,     # 크게 두고 early stopping으로 best_iter 결정\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            **params,\n",
    "        )\n",
    "\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_metric=\"aucpr\",\n",
    "            callbacks=[\n",
    "                early_stopping(stopping_rounds=100, first_metric_only=True),\n",
    "                log_evaluation(period=0),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        val_prob = model.predict_proba(X_val)[:, 1]\n",
    "        val_pr_auc = float(average_precision_score(y_val, val_prob))\n",
    "\n",
    "        if val_pr_auc > best[\"val_pr_auc\"]:\n",
    "            best[\"val_pr_auc\"] = val_pr_auc\n",
    "            best[\"params\"] = dict(params)\n",
    "            best[\"best_iter\"] = int(getattr(model, \"best_iteration_\", model.n_estimators))\n",
    "\n",
    "        print(f\"[trial {t+1:02d}] val_pr_auc={val_pr_auc:.6f} | best={best['val_pr_auc']:.6f}\")\n",
    "\n",
    "    return best\n",
    "\n",
    "BEST = tune_lgbm(n_trials=30, seed=42)\n",
    "print(\"BEST:\", {**BEST, \"params\": dict_to_py(BEST[\"params\"])})\n",
    "\n",
    "\n",
    "\n",
    "# 최종 학습(TRAIN+VAL) -> TEST 1회 평가 + 저장(공통 함수)\n",
    "\n",
    "MODEL_NAME = \"lgbm\"\n",
    "MODEL_ID   = \"ml__lgbm\"\n",
    "VERSION    = \"v1_tuned\"   # 폴더명/버전명\n",
    "\n",
    "final = LGBMClassifier(\n",
    "    objective=\"binary\",\n",
    "    boosting_type=\"gbdt\",\n",
    "    n_estimators=int(BEST[\"best_iter\"]),\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    **BEST[\"params\"],\n",
    ")\n",
    "final.fit(X_tv, y_tv)\n",
    "\n",
    "test_prob = final.predict_proba(X_test)[:, 1]\n",
    "test_true = np.asarray(y_test).astype(int)\n",
    "\n",
    "metrics_payload = build_ranking_metrics(test_true, test_prob)\n",
    "figures = make_figures(test_true, test_prob, k_list=(5, 10, 15, 30))\n",
    "\n",
    "saved = save_model_and_artifacts(\n",
    "    model=final,\n",
    "    model_name=MODEL_NAME,\n",
    "    model_type=\"ml\",\n",
    "    model_id=MODEL_ID,\n",
    "    split=\"test\",\n",
    "    metrics=metrics_payload,\n",
    "    y_true=test_true,\n",
    "    y_prob=np.asarray(test_prob).astype(float),\n",
    "    version=VERSION,\n",
    "    scaler=None,\n",
    "    figures=figures,\n",
    "    # feature_cols 없어도 된다 했으니 최소 config만(필요하면 여기에 더 추가)\n",
    "    config={\n",
    "        \"model_name\": MODEL_NAME,\n",
    "        \"model_type\": \"ml\",\n",
    "        \"version\": VERSION,\n",
    "        \"feature_source\": \"features_ml_clean.parquet\",\n",
    "        \"n_features\": int(len(feature_cols)),\n",
    "        \"best_iter\": int(BEST[\"best_iter\"]),\n",
    "    },\n",
    ")\n",
    "\n",
    "# figures close\n",
    "plt.close(figures[\"pr_curve\"])\n",
    "for k in (5, 10, 15, 30):\n",
    "    plt.close(figures[f\"confusion_matrix_top{k}\"])\n",
    "\n",
    "# Streamlit percentile 저장 (app이 읽는 규칙)\n",
    "sp = score_percentiles_payload(MODEL_ID, \"test\", test_prob)\n",
    "p_streamlit = write_streamlit_score_percentiles(MODEL_NAME, sp)\n",
    "\n",
    "# 튜닝 결과 저장 (PATHS 기준 폴더)\n",
    "tuning_payload = {\n",
    "    \"model_id\": MODEL_ID,\n",
    "    \"version\": VERSION,\n",
    "    \"best_val_pr_auc\": float(BEST[\"val_pr_auc\"]),\n",
    "    \"best_params\": dict_to_py(BEST[\"params\"]),\n",
    "    \"best_iter\": int(BEST[\"best_iter\"]),\n",
    "}\n",
    "tuning_path = Path(PATHS[\"models_metrics\"]) / MODEL_NAME / VERSION / \"tuning.json\"\n",
    "tuning_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "tuning_path.write_text(json.dumps(tuning_payload, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"\\n=== metrics ===\")\n",
    "print(json.dumps(metrics_payload, ensure_ascii=False, indent=2))\n",
    "\n",
    "print(\"\\n=== saved paths ===\")\n",
    "for k, v in saved.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "print(\"\\nstreamlit percentiles:\", p_streamlit)\n",
    "print(\"tuning:\", str(tuning_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768a0ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "churn_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
