{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65a6c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# 현재 파일 위치 기준으로 위로 올라가며 \"app\" 폴더를 찾는다\n",
    "p = Path.cwd().resolve()\n",
    "for _ in range(6):  # 최대 6단계 위까지 탐색\n",
    "    if (p / \"app\").exists() and (p / \"models\").exists():\n",
    "        sys.path.insert(0, str(p))\n",
    "        print(\"✅ project root added to sys.path:\", p)\n",
    "        break\n",
    "    p = p.parent\n",
    "else:\n",
    "    raise RuntimeError(\"❌ 프로젝트 루트를 찾지 못했어요. app/ 와 models/ 가 있는 폴더에서 실행해야 합니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12441486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load parquet files\n",
      "rows: 813540 train/val/test: 574092 137615 101833\n",
      "n_features: 14\n",
      "train positives/negatives: 461642 112450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jy/project_2nd/SKN23-2nd-3Team/churn_venv/lib/python3.14/site-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/jy/project_2nd/SKN23-2nd-3Team/churn_venv/lib/python3.14/site-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/jy/project_2nd/SKN23-2nd-3Team/churn_venv/lib/python3.14/site-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/jy/project_2nd/SKN23-2nd-3Team/churn_venv/lib/python3.14/site-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/jy/project_2nd/SKN23-2nd-3Team/churn_venv/lib/python3.14/site-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/jy/project_2nd/SKN23-2nd-3Team/churn_venv/lib/python3.14/site-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial 6 / 30 best_score= 0.33197340491099986 best_val_topk_recall= 0.06499516507804945 best_val_topk_lift= 1.1329081244098511 best_val_pr_auc= 0.9168106443120612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jy/project_2nd/SKN23-2nd-3Team/churn_venv/lib/python3.14/site-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/jy/project_2nd/SKN23-2nd-3Team/churn_venv/lib/python3.14/site-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/jy/project_2nd/SKN23-2nd-3Team/churn_venv/lib/python3.14/site-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/jy/project_2nd/SKN23-2nd-3Team/churn_venv/lib/python3.14/site-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/jy/project_2nd/SKN23-2nd-3Team/churn_venv/lib/python3.14/site-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/jy/project_2nd/SKN23-2nd-3Team/churn_venv/lib/python3.14/site-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial 12 / 30 best_score= 0.332996643951732 best_val_topk_recall= 0.06633340240364691 best_val_topk_lift= 1.1329863685959873 best_val_pr_auc= 0.9165917967219137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jy/project_2nd/SKN23-2nd-3Team/churn_venv/lib/python3.14/site-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/jy/project_2nd/SKN23-2nd-3Team/churn_venv/lib/python3.14/site-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/jy/project_2nd/SKN23-2nd-3Team/churn_venv/lib/python3.14/site-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/jy/project_2nd/SKN23-2nd-3Team/churn_venv/lib/python3.14/site-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/jy/project_2nd/SKN23-2nd-3Team/churn_venv/lib/python3.14/site-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/jy/project_2nd/SKN23-2nd-3Team/churn_venv/lib/python3.14/site-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial 18 / 30 best_score= 0.332996643951732 best_val_topk_recall= 0.06633340240364691 best_val_topk_lift= 1.1329863685959873 best_val_pr_auc= 0.9165917967219137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jy/project_2nd/SKN23-2nd-3Team/churn_venv/lib/python3.14/site-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/jy/project_2nd/SKN23-2nd-3Team/churn_venv/lib/python3.14/site-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/jy/project_2nd/SKN23-2nd-3Team/churn_venv/lib/python3.14/site-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/jy/project_2nd/SKN23-2nd-3Team/churn_venv/lib/python3.14/site-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/jy/project_2nd/SKN23-2nd-3Team/churn_venv/lib/python3.14/site-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/jy/project_2nd/SKN23-2nd-3Team/churn_venv/lib/python3.14/site-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial 24 / 30 best_score= 0.332996643951732 best_val_topk_recall= 0.06633340240364691 best_val_topk_lift= 1.1329863685959873 best_val_pr_auc= 0.9165917967219137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jy/project_2nd/SKN23-2nd-3Team/churn_venv/lib/python3.14/site-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/jy/project_2nd/SKN23-2nd-3Team/churn_venv/lib/python3.14/site-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/jy/project_2nd/SKN23-2nd-3Team/churn_venv/lib/python3.14/site-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/jy/project_2nd/SKN23-2nd-3Team/churn_venv/lib/python3.14/site-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/jy/project_2nd/SKN23-2nd-3Team/churn_venv/lib/python3.14/site-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/jy/project_2nd/SKN23-2nd-3Team/churn_venv/lib/python3.14/site-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial 30 / 30 best_score= 0.332996643951732 best_val_topk_recall= 0.06633340240364691 best_val_topk_lift= 1.1329863685959873 best_val_pr_auc= 0.9165917967219137\n",
      "best_val_score: 0.332996643951732\n",
      "best_val_topk: {\n",
      "  \"k_pct\": 5,\n",
      "  \"thr\": 0.7833491720416923,\n",
      "  \"recall\": 0.06633340240364691,\n",
      "  \"precision\": 0.9535807372471142,\n",
      "  \"lift\": 1.1329863685959873,\n",
      "  \"base_rate\": 0.8416524361443156,\n",
      "  \"n_sel\": 8057\n",
      "}\n",
      "best_val_pr_auc: 0.9165917967219137\n",
      "best_iter: 163\n",
      "best_params: {\n",
      "  \"objective\": \"binary\",\n",
      "  \"random_state\": 42,\n",
      "  \"n_estimators\": 10000,\n",
      "  \"n_jobs\": -1,\n",
      "  \"class_weight\": \"balanced\",\n",
      "  \"verbose\": -1,\n",
      "  \"force_row_wise\": true,\n",
      "  \"learning_rate\": 0.03,\n",
      "  \"num_leaves\": 127,\n",
      "  \"max_depth\": -1,\n",
      "  \"min_child_samples\": 10,\n",
      "  \"subsample\": 1.0,\n",
      "  \"subsample_freq\": 5,\n",
      "  \"colsample_bytree\": 0.8,\n",
      "  \"reg_alpha\": 0.1,\n",
      "  \"reg_lambda\": 1.0,\n",
      "  \"min_split_gain\": 0.0001,\n",
      "  \"min_child_weight\": 10.0,\n",
      "  \"max_bin\": 63,\n",
      "  \"min_data_in_bin\": 1,\n",
      "  \"feature_fraction_bynode\": 1.0\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jy/project_2nd/SKN23-2nd-3Team/churn_venv/lib/python3.14/site-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PR-AUC: 0.9334303518442961\n",
      "saved keys: ['model', 'config', 'eval_dir', 'version_dir', 'figure_pr_curve', 'figure_confusion_matrix_top5', 'figure_confusion_matrix_top10', 'figure_confusion_matrix_top15', 'figure_confusion_matrix_top30']\n",
      "model -> /Users/jy/project_2nd/SKN23-2nd-3Team/models/ml/lgbm/tuned_on_val/model.pkl\n",
      "config -> /Users/jy/project_2nd/SKN23-2nd-3Team/models/configs/lgbm/tuned_on_val/config.json\n",
      "eval_dir -> /Users/jy/project_2nd/SKN23-2nd-3Team/models/eval/mllgbm\n",
      "version_dir -> tuned_on_val\n",
      "figure_pr_curve -> /Users/jy/project_2nd/SKN23-2nd-3Team/assets/training/lgbm/tuned_on_val/pr_curve.png\n",
      "figure_confusion_matrix_top5 -> /Users/jy/project_2nd/SKN23-2nd-3Team/assets/training/lgbm/tuned_on_val/confusion_matrix_top5.png\n",
      "figure_confusion_matrix_top10 -> /Users/jy/project_2nd/SKN23-2nd-3Team/assets/training/lgbm/tuned_on_val/confusion_matrix_top10.png\n",
      "figure_confusion_matrix_top15 -> /Users/jy/project_2nd/SKN23-2nd-3Team/assets/training/lgbm/tuned_on_val/confusion_matrix_top15.png\n",
      "figure_confusion_matrix_top30 -> /Users/jy/project_2nd/SKN23-2nd-3Team/assets/training/lgbm/tuned_on_val/confusion_matrix_top30.png\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, confusion_matrix, average_precision_score\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "here = Path.cwd().resolve()\n",
    "project_root = next(\n",
    "    (\n",
    "        p\n",
    "        for p in [here, *here.parents]\n",
    "        if (p / \"app\" / \"utils\" / \"paths.py\").is_file()\n",
    "        and (p / \"models\").is_dir()\n",
    "        and (p / \"data\").is_dir()\n",
    "    ),\n",
    "    None,\n",
    ")\n",
    "if project_root is None:\n",
    "    raise FileNotFoundError(f\"프로젝트 루트를 못 찾았어. 현재 위치={here}\")\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "os.chdir(project_root)\n",
    "\n",
    "import importlib\n",
    "\n",
    "importlib.invalidate_caches()\n",
    "for k in list(sys.modules.keys()):\n",
    "    if k == \"app\" or k.startswith(\"app.\"):\n",
    "        del sys.modules[k]\n",
    "\n",
    "from app.utils.paths import DEFAULT_PATHS as P, ensure_runtime_dirs\n",
    "from app.utils.metrics import evaluate_churn_metrics\n",
    "from app.utils.save import save_model_and_artifacts\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "except Exception as e:\n",
    "    raise ImportError(\"lightgbm이 설치되어 있어야 해. 예: pip install lightgbm\") from e\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "try:\n",
    "    from app.utils.plotting import configure_matplotlib_korean\n",
    "\n",
    "    configure_matplotlib_korean()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "ensure_runtime_dirs()\n",
    "\n",
    "MODEL_NAME = \"lgbm\"\n",
    "MODEL_ID = \"ml__lgbm\"\n",
    "VERSION = \"tuned_on_val\"\n",
    "N_TRIALS = 30\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "TUNE_K = 5\n",
    "ALPHA = 0.75\n",
    "BETA = 0.25\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "def plot_confusion_matrix_figure(y_true, y_pred, title: str, labels=(\"non_m2\", \"m2\")):\n",
    "    y_true_arr = np.asarray(y_true, dtype=int).reshape(-1)\n",
    "    y_pred_arr = np.asarray(y_pred, dtype=int).reshape(-1)\n",
    "    cm = confusion_matrix(y_true_arr, y_pred_arr)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    im = ax.imshow(cm, interpolation=\"nearest\", aspect=\"equal\", cmap=\"Blues\")\n",
    "    fig.colorbar(im, ax=ax)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"Actual\")\n",
    "\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_yticks([0, 1])\n",
    "    ax.set_xticklabels(list(labels))\n",
    "    ax.set_yticklabels(list(labels))\n",
    "\n",
    "    thresh = float(cm.max()) / 2.0 if cm.size else 0.0\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(\n",
    "                j,\n",
    "                i,\n",
    "                str(cm[i, j]),\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                color=\"white\" if float(cm[i, j]) > thresh else \"black\",\n",
    "                fontsize=12,\n",
    "            )\n",
    "\n",
    "    ax.set_xlim(-0.5, cm.shape[1] - 0.5)\n",
    "    ax.set_ylim(cm.shape[0] - 0.5, -0.5)\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def topk_threshold(y_prob: np.ndarray, k_pct: int) -> float:\n",
    "    prob = np.asarray(y_prob, dtype=float).reshape(-1)\n",
    "    order = np.argsort(-prob)\n",
    "    n_sel = int(np.floor(len(prob) * (float(k_pct) / 100.0)))\n",
    "    n_sel = max(n_sel, 1)\n",
    "    return float(prob[order[n_sel - 1]])\n",
    "\n",
    "\n",
    "def plot_confusion_topk(y_true, y_prob, k_pct: int, labels=(\"non_m2\", \"m2\")):\n",
    "    thr = topk_threshold(np.asarray(y_prob, dtype=float), int(k_pct))\n",
    "    y_pred = (np.asarray(y_prob, dtype=float) >= thr).astype(int)\n",
    "    return plot_confusion_matrix_figure(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        title=f\"Confusion Matrix (Top {int(k_pct)}%, thr={thr:.5f})\",\n",
    "        labels=labels,\n",
    "    )\n",
    "\n",
    "\n",
    "def topk_stats(y_true: np.ndarray, y_prob: np.ndarray, k_pct: int) -> dict:\n",
    "    y_true_arr = np.asarray(y_true, dtype=int).reshape(-1)\n",
    "    y_prob_arr = np.asarray(y_prob, dtype=float).reshape(-1)\n",
    "\n",
    "    thr = topk_threshold(y_prob_arr, int(k_pct))\n",
    "    sel = (y_prob_arr >= thr).astype(int)\n",
    "\n",
    "    base_rate = float(y_true_arr.mean())\n",
    "    total_pos = float(y_true_arr.sum())\n",
    "    tp = float(np.sum((sel == 1) & (y_true_arr == 1)))\n",
    "    n_sel = float(np.sum(sel))\n",
    "\n",
    "    recall_k = float(tp / (total_pos + 1e-12))\n",
    "    precision_k = float(tp / (n_sel + 1e-12))\n",
    "    lift_k = float(precision_k / base_rate) if base_rate > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        \"k_pct\": int(k_pct),\n",
    "        \"thr\": float(thr),\n",
    "        \"recall\": float(recall_k),\n",
    "        \"precision\": float(precision_k),\n",
    "        \"lift\": float(lift_k),\n",
    "        \"base_rate\": float(base_rate),\n",
    "        \"n_sel\": int(n_sel),\n",
    "    }\n",
    "\n",
    "\n",
    "def tune_lgbm_on_val(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    n_trials: int = 30,\n",
    "    seed: int = 42,\n",
    "    k_pct: int = 5,\n",
    "    alpha: float = 0.75,\n",
    "    beta: float = 0.25,\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    best = {\n",
    "        \"score\": -1e18,\n",
    "        \"params\": None,\n",
    "        \"best_iteration\": None,\n",
    "        \"val_topk\": None,\n",
    "        \"val_pr_auc\": None,\n",
    "    }\n",
    "\n",
    "    base_params = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"random_state\": seed,\n",
    "        \"n_estimators\": 10000,\n",
    "        \"n_jobs\": -1,\n",
    "        \"class_weight\": \"balanced\",\n",
    "        \"verbose\": -1,\n",
    "        \"force_row_wise\": True,\n",
    "    }\n",
    "\n",
    "    for t in range(int(n_trials)):\n",
    "        params = dict(base_params)\n",
    "\n",
    "        params[\"learning_rate\"] = float(rng.choice([0.01, 0.02, 0.03, 0.05]))\n",
    "        params[\"num_leaves\"] = int(rng.choice([31, 63, 127, 255]))\n",
    "        params[\"max_depth\"] = int(rng.choice([-1, 4, 6, 8, 10, 12]))\n",
    "        params[\"min_child_samples\"] = int(rng.choice([5, 10, 20, 30, 50, 100]))\n",
    "        params[\"subsample\"] = float(rng.choice([0.7, 0.8, 0.9, 1.0]))\n",
    "        params[\"subsample_freq\"] = int(rng.choice([0, 1, 5]))\n",
    "        params[\"colsample_bytree\"] = float(rng.choice([0.7, 0.8, 0.9, 1.0]))\n",
    "        params[\"reg_alpha\"] = float(rng.choice([0.0, 1e-4, 1e-3, 1e-2, 1e-1, 1.0]))\n",
    "        params[\"reg_lambda\"] = float(rng.choice([0.0, 1e-4, 1e-3, 1e-2, 1e-1, 1.0, 10.0]))\n",
    "        params[\"min_split_gain\"] = float(rng.choice([0.0, 1e-4, 1e-3, 1e-2, 5e-2]))\n",
    "        params[\"min_child_weight\"] = float(rng.choice([1e-3, 1e-2, 1e-1, 1.0, 10.0]))\n",
    "        params[\"max_bin\"] = int(rng.choice([63, 127, 255]))\n",
    "\n",
    "        params[\"min_data_in_bin\"] = int(rng.choice([1, 3, 5, 10]))\n",
    "        params[\"feature_fraction_bynode\"] = float(rng.choice([0.6, 0.8, 1.0]))\n",
    "\n",
    "        m = LGBMClassifier(**params)\n",
    "        m.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_metric=\"average_precision\",\n",
    "            callbacks=[early_stopping(stopping_rounds=80, verbose=False), log_evaluation(0)],\n",
    "        )\n",
    "\n",
    "        val_prob = m.predict_proba(X_val)[:, 1].astype(float).reshape(-1)\n",
    "        topk = topk_stats(y_val, val_prob, k_pct=int(k_pct))\n",
    "        pr_auc = float(average_precision_score(np.asarray(y_val, dtype=int), val_prob))\n",
    "\n",
    "        score = float(alpha * topk[\"recall\"] + beta * topk[\"lift\"] + (1.0 - alpha - beta) * pr_auc)\n",
    "\n",
    "        if score > best[\"score\"]:\n",
    "            best[\"score\"] = score\n",
    "            best[\"params\"] = dict(params)\n",
    "            best[\"val_topk\"] = dict(topk)\n",
    "            best[\"val_pr_auc\"] = float(pr_auc)\n",
    "            try:\n",
    "                best[\"best_iteration\"] = int(getattr(m, \"best_iteration_\", 0) or 0)\n",
    "            except Exception:\n",
    "                best[\"best_iteration\"] = None\n",
    "\n",
    "        if (t + 1) % max(1, int(n_trials // 5)) == 0:\n",
    "            print(\n",
    "                \"trial\",\n",
    "                t + 1,\n",
    "                \"/\",\n",
    "                n_trials,\n",
    "                \"best_score=\",\n",
    "                float(best[\"score\"]),\n",
    "                \"best_val_topk_recall=\",\n",
    "                float(best[\"val_topk\"][\"recall\"]) if best[\"val_topk\"] else None,\n",
    "                \"best_val_topk_lift=\",\n",
    "                float(best[\"val_topk\"][\"lift\"]) if best[\"val_topk\"] else None,\n",
    "                \"best_val_pr_auc=\",\n",
    "                float(best[\"val_pr_auc\"]) if best[\"val_pr_auc\"] is not None else None,\n",
    "            )\n",
    "\n",
    "    return best\n",
    "\n",
    "\n",
    "print(\"Load parquet files\")\n",
    "\n",
    "anchors = pd.read_parquet(P.parquet_path(\"anchors\"))\n",
    "features = pd.read_parquet(P.parquet_path(\"features_ml_clean\"))\n",
    "labels = pd.read_parquet(P.parquet_path(\"labels\"))\n",
    "\n",
    "for df in (anchors, features, labels):\n",
    "    df[\"user_id\"] = df[\"user_id\"].astype(str)\n",
    "\n",
    "if \"split\" in anchors.columns:\n",
    "    anchors = anchors.drop(columns=[\"split\"])\n",
    "\n",
    "if \"split\" not in labels.columns:\n",
    "    raise KeyError(f\"labels.parquet에 split 컬럼이 없습니다. labels columns head: {list(labels.columns)[:50]}\")\n",
    "\n",
    "data = anchors.merge(features, on=[\"user_id\", \"anchor_time\"], how=\"inner\")\n",
    "data = data.merge(labels[[\"user_id\", \"anchor_time\", \"label\", \"split\"]], on=[\"user_id\", \"anchor_time\"], how=\"inner\")\n",
    "\n",
    "data[\"target\"] = data[\"label\"].astype(str).eq(\"m2\").astype(int)\n",
    "split_col = data[\"split\"].astype(str)\n",
    "\n",
    "feature_cols = [c for c in features.columns if c not in (\"user_id\", \"anchor_time\")]\n",
    "X_all = data.loc[:, feature_cols].copy()\n",
    "X_all = X_all.replace([np.inf, -np.inf], np.nan)\n",
    "X_all = X_all.fillna(0.0)\n",
    "y_all = data[\"target\"].to_numpy(dtype=int)\n",
    "\n",
    "idx_train = split_col.eq(\"train\").to_numpy()\n",
    "idx_val = split_col.eq(\"val\").to_numpy()\n",
    "idx_test = split_col.eq(\"test\").to_numpy()\n",
    "\n",
    "X_train = X_all.loc[idx_train].to_numpy(dtype=float)\n",
    "y_train = y_all[idx_train]\n",
    "X_val = X_all.loc[idx_val].to_numpy(dtype=float)\n",
    "y_val = y_all[idx_val]\n",
    "X_test = X_all.loc[idx_test].to_numpy(dtype=float)\n",
    "y_test = y_all[idx_test]\n",
    "\n",
    "print(\"rows:\", len(data), \"train/val/test:\", int(idx_train.sum()), int(idx_val.sum()), int(idx_test.sum()))\n",
    "print(\"n_features:\", int(X_train.shape[1]))\n",
    "print(\"train positives/negatives:\", int(y_train.sum()), int((1 - y_train).sum()))\n",
    "\n",
    "tune = tune_lgbm_on_val(\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    n_trials=int(N_TRIALS),\n",
    "    seed=int(RANDOM_SEED),\n",
    "    k_pct=int(TUNE_K),\n",
    "    alpha=float(ALPHA),\n",
    "    beta=float(BETA),\n",
    ")\n",
    "\n",
    "best_params = tune[\"params\"]\n",
    "best_iter = tune[\"best_iteration\"]\n",
    "best_topk = tune[\"val_topk\"]\n",
    "best_pr_auc = tune[\"val_pr_auc\"]\n",
    "\n",
    "print(\"best_val_score:\", float(tune[\"score\"]))\n",
    "print(\"best_val_topk:\", json.dumps(best_topk, ensure_ascii=False, indent=2) if best_topk else None)\n",
    "print(\"best_val_pr_auc:\", float(best_pr_auc) if best_pr_auc is not None else None)\n",
    "print(\"best_iter:\", int(best_iter) if best_iter else None)\n",
    "print(\"best_params:\", json.dumps(best_params, ensure_ascii=False, indent=2) if best_params else None)\n",
    "\n",
    "final_params = dict(best_params)\n",
    "if best_iter and int(best_iter) > 0:\n",
    "    final_params[\"n_estimators\"] = int(best_iter)\n",
    "\n",
    "X_trval = np.vstack([X_train, X_val])\n",
    "y_trval = np.concatenate([y_train, y_val])\n",
    "\n",
    "final_model = LGBMClassifier(**final_params)\n",
    "final_model.fit(X_trval, y_trval)\n",
    "\n",
    "y_prob = final_model.predict_proba(X_test)[:, 1].astype(float).reshape(-1)\n",
    "y_true = np.asarray(y_test, dtype=int).reshape(-1)\n",
    "\n",
    "metrics = evaluate_churn_metrics(y_true, y_prob)\n",
    "print(\"Test PR-AUC:\", float(metrics.get(\"PR-AUC (Average Precision)\", 0.0)))\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
    "pr_auc_val = metrics.get(\"PR-AUC (Average Precision)\")\n",
    "pr_auc_val = float(average_precision_score(y_true, y_prob)) if pr_auc_val is None else float(pr_auc_val)\n",
    "\n",
    "fig_pr, ax_pr = plt.subplots(figsize=(6, 5))\n",
    "ax_pr.plot(recall, precision, lw=2, label=f\"PR-AUC={pr_auc_val:.5f}\")\n",
    "ax_pr.set_xlabel(\"Recall\")\n",
    "ax_pr.set_ylabel(\"Precision\")\n",
    "ax_pr.set_title(\"Precision-Recall Curve\")\n",
    "ax_pr.grid(alpha=0.3)\n",
    "ax_pr.legend()\n",
    "fig_pr.tight_layout()\n",
    "\n",
    "figures = {\"pr_curve\": fig_pr}\n",
    "for k_pct in (5, 10, 15, 30):\n",
    "    figures[f\"confusion_matrix_top{k_pct}\"] = plot_confusion_topk(\n",
    "        y_true=y_true, y_prob=y_prob, k_pct=int(k_pct), labels=(\"non_m2\", \"m2\")\n",
    "    )\n",
    "\n",
    "saved = save_model_and_artifacts(\n",
    "    model=final_model,\n",
    "    model_name=MODEL_NAME,\n",
    "    model_type=\"ml\",\n",
    "    model_id=MODEL_ID,\n",
    "    split=\"test\",\n",
    "    metrics=metrics,\n",
    "    y_true=y_true,\n",
    "    y_prob=y_prob,\n",
    "    version=VERSION,\n",
    "    scaler=None,\n",
    "    figures=figures,\n",
    "    config={\n",
    "        \"model_name\": MODEL_NAME,\n",
    "        \"model_type\": \"ml\",\n",
    "        \"version\": VERSION,\n",
    "        \"feature_source\": \"features_ml_clean.parquet\",\n",
    "        \"tuning\": {\n",
    "            \"n_trials\": int(N_TRIALS),\n",
    "            \"tune_k\": int(TUNE_K),\n",
    "            \"alpha\": float(ALPHA),\n",
    "            \"beta\": float(BETA),\n",
    "            \"best_val_score\": float(tune[\"score\"]),\n",
    "            \"best_val_topk\": best_topk,\n",
    "            \"best_val_pr_auc\": float(best_pr_auc) if best_pr_auc is not None else None,\n",
    "            \"best_iteration\": int(best_iter) if best_iter else None,\n",
    "            \"best_params\": {k: final_params[k] for k in sorted(final_params.keys()) if k not in [\"n_jobs\"]},\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"saved keys:\", list(saved.keys()))\n",
    "for k, v in saved.items():\n",
    "    print(k, \"->\", v)\n",
    "\n",
    "plt.close(fig_pr)\n",
    "for k_pct in (5, 10, 15, 30):\n",
    "    plt.close(figures[f\"confusion_matrix_top{k_pct}\"])\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702673bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "churn_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
